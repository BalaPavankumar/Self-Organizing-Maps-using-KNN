{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run as writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use scipy logsumexp().\n"
     ]
    }
   ],
   "source": [
    "%%writefile_run bayeschangept.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing \n",
    "from matplotlib.pylab import rcParams\n",
    "import datetime as dt\n",
    "import cProfile\n",
    "import bayesian_changepoint_detection.offline_changepoint_detection as offcd\n",
    "import bayesian_changepoint_detection.online_changepoint_detection as oncd\n",
    "from functools import partial\n",
    "import matplotlib.cm as cm\n",
    "import argparse\n",
    "import time\n",
    "# Importing reader and checker python files as modules\n",
    "import reader_writer.reader as reader\n",
    "import reader_writer.checker as checker\n",
    "import reader_writer.reader_configs as read_args\n",
    "import reader_writer.writer_configs as write_args\n",
    "import psycopg2\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 9\n",
    "rcParams[ 'axes.grid']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assetno = ['1']\n",
    "con = '52.173.76.89:4242'\n",
    "src_type     =  'opentsdb'\n",
    "param = ['FE-001.DRIVEENERGY']\n",
    "from_timestamp = 1520402214\n",
    "to_timestamp = 1520407294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "def read(reader_kwargs):\n",
    "    response_dict=reader.reader_api(**reader_kwargs)\n",
    "#     print(response_dict)\n",
    "    print(\"Getting the dataset from the reader....\\n\")\n",
    "    entire_data = parse_dict_to_dataframe(response_dict)\n",
    "    print(entire_data.head())\n",
    "#     if(data[data.columns].values)\n",
    "#     data[data.columns] = data[data.columns].values.astype(np.float64)\n",
    "    print(entire_data.dtypes)\n",
    "    print(entire_data.shape)\n",
    "#     entire_data = entire_data[np.isfinite(entire_data[entire_data.columns].values)]\n",
    "    return entire_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dict_to_dataframe(response_dict):\n",
    "#     print(response_dict)\n",
    "    entire_data_set = []\n",
    "    \n",
    "    for data_per_asset in response_dict['body']:\n",
    "        assetno = data_per_asset['assetno']\n",
    "        for data_per_metric in data_per_asset['readings']:\n",
    "            data = pd.DataFrame(data_per_metric['datapoints'],columns=['timestamp',data_per_metric['name']])\n",
    "            data.index = data['timestamp']\n",
    "            del data['timestamp']\n",
    "            data['assetno']=assetno\n",
    "            entire_data_set.append(data)\n",
    "    \n",
    "    return pd.concat(entire_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(assetno,from_timestamp,to_timestamp,con,para_list,source_type='opentsdb',table_name='',\n",
    "        qry_str='',impute_fill_method='forward',down_sampling_method=None,down_sampling_window=None,freq=None,\n",
    "        resample_fill_method=None,to_resample=None,to_impute=None,thres_prob=0.5,samples_to_wait=10,expected_run_length=100):\n",
    "\n",
    "        reader_kwargs={\n",
    "            'assetno':assetno,\n",
    "            'from_timestamp':from_timestamp,\n",
    "            'to_timestamp':to_timestamp,\n",
    "            'con':con,\n",
    "            'para_list':para_list,\n",
    "            'source_type':source_type,\n",
    "            'table_name':table_name,\n",
    "            'qry_str':qry_str,\n",
    "            'impute_fill_method':impute_fill_method,\n",
    "            'down_sampling_method':down_sampling_method,\n",
    "            'down_sampling_window':down_sampling_window,\n",
    "            'freq':freq,\n",
    "            'resample_fill_method':resample_fill_method,\n",
    "            'to_resample':to_resample,\n",
    "            'to_impute':to_impute\n",
    "        }\n",
    "\n",
    "        algo_kwargs={\n",
    "            'thres_prob':thres_prob,\n",
    "            'samples_to_wait':samples_to_wait,\n",
    "            'expected_run_length':expected_run_length\n",
    "        }\n",
    "        \n",
    "        entire_data = read(reader_kwargs)\n",
    "#         print(original_data.head())\n",
    "#         anom_indexes = anomaly_detector(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the dataset from the reader....\n",
      "\n",
      "               FE-001.DRIVEENERGY assetno\n",
      "timestamp                                \n",
      "1520402214990          177.208099       1\n",
      "1520402224990          443.687561       1\n",
      "1520402234990          127.826195       1\n",
      "1520402244990          167.014084       1\n",
      "1520402254990          418.113342       1\n",
      "FE-001.DRIVEENERGY    float64\n",
      "assetno                object\n",
      "dtype: object\n",
      "(508, 2)\n",
      "               FE-001.DRIVEENERGY assetno\n",
      "timestamp                                \n",
      "1520402214990          177.208099       1\n",
      "1520402224990          443.687561       1\n",
      "1520402234990          127.826195       1\n",
      "1520402244990          167.014084       1\n",
      "1520402254990          418.113342       1\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "            'assetno':assetno,\n",
    "            'from_timestamp':from_timestamp,\n",
    "            'to_timestamp':to_timestamp,\n",
    "            'con':con,\n",
    "            'para_list':param,\n",
    "            'source_type':src_type,\n",
    "            'table_name':'',\n",
    "            'qry_str':'',\n",
    "            'impute_fill_method':'forward',\n",
    "            'down_sampling_method':None,\n",
    "            'down_sampling_window':None,\n",
    "            'freq':None,\n",
    "            'resample_fill_method':None,\n",
    "            'to_resample':None,\n",
    "            'to_impute':None,\n",
    "            'thres_prob':0.5,\n",
    "            'samples_to_wait':10,\n",
    "            'expected_run_length':100\n",
    "        }\n",
    "\n",
    "call(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "def ts_to_unix(t):\n",
    "    return int((t - dt.datetime(1970, 1, 1)).total_seconds()*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "def readata_from_opentsb(reader_kwargs):\n",
    "    (checker.check_global_args(**reader_kwargs))\n",
    "    data=reader.reader_api(**reader_kwargs)\n",
    "    print(\"Downloading the dataset from the opentsdb....\")\n",
    "    data= data.iloc[:, ::-1]\n",
    "    data.index = data['timestamp']\n",
    "    assetno = pd.unique(data['assetno'])\n",
    "    del data['timestamp']\n",
    "    del data['assetno']\n",
    "    data[data.columns] = data[data.columns].values.astype(np.float64)\n",
    "    print(data.dtypes)\n",
    "    data = data[np.isfinite(data[data.columns].values)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_json = '''{\n",
    "\"header\":{\n",
    "\"parameter_count\":12,\n",
    "\"asset_count\":7,\n",
    "\"data_count\":14000\n",
    "},\n",
    "\"body\":[\n",
    "{ \"asset\": \"1\",\"readings\":[{\"name\":\"<TagName>\",\"datapoints\":[ {\"<EpochInMs>\":1.34}]}]}]}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_epochs = set().union(*(d.keys() for d in data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = set().union(*(d.values() for d in data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Timestamp':time_epochs,metric_name:values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df['Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(d['body'][0]['readings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "def write_to_db(table_name,col_names,col_vals):\n",
    "    \n",
    "#     print(\"\\n Changepoint info : \\n {}\".format(col_vals))\n",
    "    \n",
    "    conn = psycopg2.connect(**reader.db_props.db_connection)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    col_vals1 = [[str(val) if(type(val)!=str) else \"'{}'\".format(val) for val in row] for row in col_vals]\n",
    "    \n",
    "    joined_col_vals = [\"({})\".format(','.join(map(str,val))) for val in col_vals1]\n",
    "    fmt_col_vals = (','.join(joined_col_vals))\n",
    "    insert_query = \"\"\" INSERT INTO {} ({}) VALUES{};\"\"\".format(table_name,col_names,fmt_col_vals)\n",
    "#     print(insert_query)\n",
    "    cur.execute(insert_query)\n",
    "    \n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "def processanomalies(original_data,anom_indexes,sql_query_args,is_csv=False,window=10):\n",
    "    \n",
    "    col_names_list = list(sql_query_args.keys())\n",
    "    col_names = ','.join(col_names_list)\n",
    "    col_vals = []\n",
    "    table_name = write_args.table_name\n",
    "    \n",
    "    for i in anom_indexes:\n",
    "        \n",
    "        sql_query_args['parameter_list'] = '[{}]'.format(original_data.columns[0])\n",
    "        sql_query_args['event_name'] = original_data.columns[0]+write_args.anomaly_code+'_anomaly'\n",
    "\n",
    "        if(is_csv):\n",
    "            time_series = original_data.index[i-window:i+window]\n",
    "            sql_query_args['event_timestamp'] =  str(original_data.index[i])\n",
    "            sql_query_args['event_timestamp_epoch'] = str(ts_to_unix(original_data.index[i]))\n",
    "        else:\n",
    "            time_series = (pd.to_datetime(original_data.index[i-window:i+window],unit='ms',utc=True))\n",
    "            sql_query_args['event_timestamp'] =  str(pd.to_datetime(original_data.index[i],unit='ms',utc=True))\n",
    "            sql_query_args['event_timestamp_epoch'] = str((original_data.index[i]))\n",
    "\n",
    "        time_around_anoms = [\"''{}''\".format((t)) for t in time_series]\n",
    "        data_around_anoms = {'timestamp':time_around_anoms,\n",
    "                            'value':(list(original_data.iloc[i-window:i+window,0].values))}\n",
    "        \n",
    "        pts_around_anoms = ''\n",
    "        \n",
    "        for key,val in data_around_anoms.items():\n",
    "            pts_around_anoms += \"{}:{},\".format(key,val)\n",
    "        \n",
    "        sql_query_args['event_context_info'] = \"{}\".format(\"{\"+pts_around_anoms.strip(',')+\"}\")\n",
    "        \n",
    "        col_vals.append(list(sql_query_args.values()))\n",
    "    \n",
    "    write_to_db(table_name,col_names,col_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "def analyse_data(file,date_col,time_format='%Y-%m',isweek=False):\n",
    "    if(isweek!=True):\n",
    "            dateparse = lambda dates: pd.to_datetime(dates,infer_datetime_format=True)\n",
    "    else:\n",
    "        dateparse = lambda dates: dt.datetime.strptime(dates+'-0', time_format)\n",
    "    data = pd.read_csv(file, parse_dates=[date_col], index_col=date_col,date_parser=dateparse)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "def normalise_standardise(data):    \n",
    "    # Create a minimum and maximum processor object\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    # Create an object to transform the data to fit minmax processor\n",
    "    data_norm = pd.DataFrame(min_max_scaler.fit_transform(data.values),columns=data.columns,index=data.index)\n",
    "    data_standardised = (data_norm - data_norm.mean())/(data_norm.std())\n",
    "    return data_standardised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "def findonchangepoint(data,mean_runlength=100):\n",
    "    R, maxes = oncd.online_changepoint_detection(data, partial(oncd.constant_hazard, mean_runlength),\n",
    "                                                 oncd.StudentT(0.1, .01, 1, 0))\n",
    "    return R,maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "def findthreshold(data):\n",
    "    mu = np.mean(data)\n",
    "    sigma = np.mean(data)\n",
    "    inv_pt = []\n",
    "    for i in range(len(data)-1):\n",
    "        if((data[i+1]>mu and data[i]<=mu) or (data[i+1]<mu and data[i]>=mu)):\n",
    "            inv_pt.append(i)\n",
    "\n",
    "    return inv_pt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "def plotonchangepoints(data,R,maxes,nrow=None,ncol=0,Nw=10,pthres=0.5):\n",
    "    fig,(ax1,ax2,ax3) = plt.subplots(3,figsize=[18, 16])\n",
    "    ltext = 'Column : '+str(ncol+1)+' data with threshold probab = '+ str(pthres)\n",
    "    \n",
    "    ax1.set_title(data.columns[ncol])\n",
    "    \n",
    "    cp_probs = np.array(R[Nw,Nw:-1][1:-2])\n",
    "    \n",
    "    inversion_pts = findthreshold(cp_probs)\n",
    "    \n",
    "    max_indexes = []\n",
    "    for i in range(len(inversion_pts)-1):\n",
    "        max_indexes.append(inversion_pts[i]+np.argmax(cp_probs[inversion_pts[i]:inversion_pts[i+1]+1]))\n",
    "    \n",
    "    cp_mapped_probs = pd.Series(cp_probs[max_indexes],index=max_indexes)\n",
    "    anom_indexes = cp_mapped_probs.index[(np.where(cp_mapped_probs.values>pthres)[0])]\n",
    "\n",
    "    if(nrow==None):\n",
    "        ax1.plot(data.values[:,ncol],label=ltext)\n",
    "    else:\n",
    "        ax1.plot(data.values[:nrow,ncol],label=ltext)\n",
    "        \n",
    "    ax1.legend()\n",
    "\n",
    "    \n",
    "    for a in anom_indexes:\n",
    "        if(a):\n",
    "            ax1.axvline(x=a,color='r')\n",
    "        \n",
    "    sparsity = 5  # only plot every fifth data for faster display\n",
    "    ax2.pcolor(np.array(range(0, len(R[:,0]), sparsity)), \n",
    "              np.array(range(0, len(R[:,0]), sparsity)), \n",
    "              -np.log(R[0:-1:sparsity, 0:-1:sparsity]), \n",
    "              cmap=cm.Greys, vmin=0, vmax=30,label=\"Distribution of Run length\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    ax3.plot(cp_probs)\n",
    "   \n",
    "    ax3.set_title('Change points with Probability')\n",
    "\n",
    "    plt.show()\n",
    "    print(\"\\n No of Anomalies detected = %g\"%(len(anom_indexes)))\n",
    "    \n",
    "    return anom_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "def analyse_detectchangepts(src_type,filepath,date_col,time_format='%Y-%m',ncol=0,\n",
    "                            weekly_data=False,pthres=0.5,mean_runlen = 100,Nw=10):\n",
    "    \n",
    "#     orig_data = pd.DataFrame()\n",
    "#     if(src_type==0 or src_type=='csv'):\n",
    "#         orig_data = analyse_data(filepath,date_col=date_col,time_format=time_format,isweek=weekly_data)\n",
    "#     else:\n",
    "#         orig_data = readata_from_opentsb(read_args.reader_kwargs)\n",
    "    \n",
    "    \n",
    "    data = normalise_standardise(orig_data)\n",
    "    print(\"Shape of the dataset : \")\n",
    "    print(data.shape)\n",
    "    print(\"Overview of first five rows of dataset : \")\n",
    "    print(data.head())\n",
    "    \n",
    "    ax = data[data.columns[ncol]].plot.hist(figsize=(9,7),bins=100)\n",
    "    ax.set_title(\"Histogram of Dataset\")\n",
    "    \n",
    "    R,maxes = findonchangepoint(data[data.columns[ncol]].values,mean_runlength=mean_runlen)\n",
    "    anom_indexes = plotonchangepoints(data,R,maxes,Nw=Nw,pthres=pthres,ncol=ncol)\n",
    "    sql_query_args = write_args.writer_kwargs\n",
    "    if(anom_indexes.shape[0]!=0):\n",
    "        is_csv = src_type==0 or src_type=='csv'\n",
    "        processanomalies(orig_data,anom_indexes,sql_query_args,is_csv)\n",
    "    return orig_data,anom_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "def add_args(parser,arg_name,arg_help,arg_optional=False,arg_def=None):\n",
    "    if(arg_optional):\n",
    "        arg_name = '--'+arg_name\n",
    "    parser.add_argument(arg_name,help=arg_help,type=type(arg_def),default=arg_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "arg_name_list = ['src_type','filepath','date_column_name','time_format','weekly_data','Threshold_probability',\n",
    "                'samples_to_wait','expected_run_length','data_column']\n",
    "\n",
    "arg_help_list = [\n",
    "                'Enter 0 to read from csv and 1 to read from opentsdb',\n",
    "                'Enter the full path of input file with extension .csv',\n",
    "                'Enter the name of the date column',\n",
    "                'Enter the time format of the date column',\n",
    "                'Enter whether data sampled weekly or not. Give True or False',\n",
    "                'Enter the threshold probability',\n",
    "                'Enter no of samples to wait to detect a change point',\n",
    "                'Enter the expected gap or no of samples between changepoints',\n",
    "                'Enter the index of column which you want to analyse with starting index excluding date column']\n",
    "\n",
    "arg_opt_list = [False,True,True,True,True,True,True,True,True]\n",
    "\n",
    "arg_def_list = [1,'','Month','%Y-%m',False,0.5,10,100.0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "args_list = {\n",
    "    'arg_name':arg_name_list,\n",
    "    'arg_help':arg_help_list,\n",
    "    'arg_optional':arg_opt_list,\n",
    "    'arg_def':arg_def_list,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "arg = {\n",
    "    'arg_name':'',\n",
    "    'arg_help':'',\n",
    "    'arg_optional':False,\n",
    "    'arg_def':'',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "for i in range(len(args_list['arg_name'])):\n",
    "    arg['arg_name'] = args_list['arg_name'][i]\n",
    "    arg['arg_help'] = args_list['arg_help'][i]\n",
    "    arg['arg_optional'] = args_list['arg_optional'][i]\n",
    "    arg['arg_def'] = args_list['arg_def'][i]\n",
    "    add_args(parser,**arg)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "if(args.src_type!=None):\n",
    "    src_type = (args.src_type)\n",
    "if(args.filepath!=None):\n",
    "    filepath = (args.filepath)\n",
    "if(args.outputfile_dir!=None):\n",
    "    outputfile_dir = (args.outputfile_dir)\n",
    "if(args.Threshold_probability!=None):\n",
    "    pthres = (args.Threshold_probability)\n",
    "if(args.date_column_name!=None):\n",
    "    date_col = (args.date_column_name)\n",
    "if(args.time_format!=None):\n",
    "    time_format = (args.time_format)\n",
    "if(args.weekly_data!=None):\n",
    "    weekly_data = (args.weekly_data==True)\n",
    "if(args.samples_to_wait!=None):\n",
    "    Nw = int(args.samples_to_wait) \n",
    "if(args.expected_run_length!=None):\n",
    "    mean_runlen = (args.expected_run_length)\n",
    "if(args.data_column!=None):\n",
    "    ncol = int(args.data_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_type      = 'csv'\n",
    "filepath      = ''\n",
    "date_col      = ''\n",
    "pthres        = 0.01\n",
    "time_format   = '%Y-%m'\n",
    "weekly_data   = False\n",
    "mean_runlen   = 100\n",
    "Nw            = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "input_kwargs = {\n",
    "    'src_type'       :src_type,\n",
    "    'filepath'      :filepath,\n",
    "    'date_col'      :date_col,\n",
    "    'pthres'        :pthres,\n",
    "    'time_format'   :time_format,\n",
    "    'weekly_data'   :weekly_data,\n",
    "    'mean_runlen'   :mean_runlen,\n",
    "    'Nw'            :Nw\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%writefile_run bayeschangept.py -a\n",
    "\n",
    "input_kwargs['src_type'] = 1\n",
    "original_data,anom_indexes = analyse_detectchangepts(**input_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "original_data,anom_indexes = analyse_detectchangepts(src_type=0,filepath=\"./alcohol-demand-log-spirits-consu.csv\",date_col='Month',\n",
    "                                                     pthres=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp_data,anom_indexes = analyse_detectchangepts(src_type=0,filepath=\"./average-annual-temperature-centr.csv\",date_col='Year'\n",
    "                                         ,pthres=0.4,time_format='%Y')\n",
    "\n",
    "plt.plot(temp_data[0:200])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fisher_tempdata = analyse_detectchangepts(src_type=0,filepath=\"./mean-daily-temperature-fisher-ri.csv\",\n",
    "                                         date_col='Date',time_format='%d-%m-%Y',pthres=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "female_unemp_data = analyse_detectchangepts(src_type=0,filepath=\"./monthly-us-female-20-years-and-o.csv\",\n",
    "                                         date_col='Month',time_format='%Y-%m',pthres=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weekly_close_data = analyse_detectchangepts(src_type=0,filepath=\"./weekly-closings-of-the-dowjones-.csv\",\n",
    "                                         date_col='Week',time_format='%Y-W%W-%w',weekly_data=True,pthres=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weekly_close_data = analyse_detectchangepts(src_type=0,filepath=\"methane-input-into-gas-furnace-c.csv\",ncol=0,\n",
    "                                         date_col='Time',pthres=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weekly_close_data2 = analyse_detectchangepts(src_type=0,filepath=\"methane-input-into-gas-furnace-c.csv\",ncol=1,\n",
    "                                         date_col='Time',pthres=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_tempdata = analyse_detectchangepts(src_type=0,filepath=\"./mean-monthly-temperature-1907-19.csv\",\n",
    "                                        date_col='Month',time_format='%Y-%m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "* Hence we observe that **Bayesian Changepoint Detection** works well only on level shifts or variational shift datasets over outlier or surge,sag datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
