{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohithram/anaconda3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reader_writer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7bb50f4df678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mreader_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreader_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecker\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mchecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'reader_writer'"
     ]
    }
   ],
   "source": [
    "%%writefile_run data_handler.py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import json\n",
    "# from \n",
    "import reader_writer.reader as reader\n",
    "import reader_writer.checker as checker\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run data_handler.py -a\n",
    "\n",
    "\n",
    "class Data_Writer():\n",
    "    def __init__(self,anomaly_detector):\n",
    "        self.anomaly_detector = anomaly_detector\n",
    "               \n",
    "        \n",
    "    def write(self):\n",
    "        raise NotImplementedError    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run data_handler.py -a\n",
    "\n",
    "\n",
    "class Anomaly_Detector():\n",
    "    def __init__(self,algo_name,data,anom_indexes,is_train=False,):\n",
    "        self.algo_name = algo_name\n",
    "        self.istrainable = is_train\n",
    "        self.data = data\n",
    "        self.anom_indexes = anom_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([{'a':2},{'a':1}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run data_handler.py -a\n",
    "\n",
    "\n",
    "    \n",
    "class Postgres_Writer():\n",
    "    def __init__(self,anomaly_detectors,db_credentials,sql_query_args,table_name,window_size=10):\n",
    "        \n",
    "#         super(Postgres_Writer,self).__init__(anomaly_detector)\n",
    "        self.anomaly_detectors = anomaly_detectors\n",
    "        self.db_credentials = db_credentials\n",
    "        self.sql_query_args = sql_query_args\n",
    "        self.table_name = table_name\n",
    "        self.window_size = window_size\n",
    "        print(\"Postgres writer initialised \\n\")\n",
    "\n",
    "        \n",
    "    def write_to_db(self,col_names,col_vals):\n",
    "\n",
    "    #     print(\"\\n Changepoint info : \\n {}\".format(col_vals))\n",
    "        col_vals1 = [[str(val) if(type(val)!=str) else \"'{}'\".format(val) for val in row] for row in col_vals]\n",
    "        joined_col_vals = [\"({})\".format(','.join(map(str,val))) for val in col_vals1]\n",
    "        fmt_col_vals = (','.join(joined_col_vals))\n",
    "        insert_query = \"\"\" INSERT INTO {} ({}) VALUES{};\"\"\".format(self.table_name,col_names,fmt_col_vals)\n",
    "        \n",
    "        status = 0\n",
    "        try:\n",
    "            conn = psycopg2.connect(**self.db_credentials)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(insert_query)\n",
    "            conn.commit()\n",
    "            print('\\n Successfully written into database\\n')\n",
    "\n",
    "        except psycopg2.DatabaseError as error:\n",
    "            status = 1\n",
    "            print(\"Database error : {}\".format(error))\n",
    "            return status\n",
    "        finally:\n",
    "            try:\n",
    "                if cur is not None:\n",
    "                    cur.close()\n",
    "                if conn is not None:\n",
    "                    conn.close()\n",
    "            except:\n",
    "                return status\n",
    "\n",
    "        return status\n",
    "        \n",
    "    def ts_to_unix(self,t):\n",
    "        return int((t - dt.datetime(1970, 1, 1)).total_seconds()*1000)\n",
    "    \n",
    "    def map_outputs_and_write(self):\n",
    "         \n",
    "        sql_query_args = self.sql_query_args\n",
    "        col_names_list = list(sql_query_args.keys())\n",
    "        col_names = ','.join(col_names_list)\n",
    "        col_vals = []\n",
    "        table_name = self.table_name\n",
    "        if(self.anomaly_detectors[0].algo_type=='univariate'):\n",
    "            \n",
    "            for anomaly_detector in self.anomaly_detectors:\n",
    "                print(\"Assetno : {} \\n\".format(anomaly_detector.assetno))\n",
    "                queries = self.make_query_args(anomaly_detector,sql_query_args)\n",
    "                [col_vals.append(list(query.values())) for query in queries]\n",
    "                print('sql_query: \\n{}\\n'.format(queries))\n",
    "        else:\n",
    "            \n",
    "            assetlist = [anomaly_detector.assetno for anomaly_detector in self.anomaly_detectors]\n",
    "            df_assets = pd.DataFrame(np.array(assetlist),columns=['assetno'])\n",
    "            asset_groups = df_assets.groupby('assetno')\n",
    "\n",
    "            for assetno,metrics_per_asset in asset_groups:\n",
    "#                 print(\"Assetno : {}\\n\".format(assetno))\n",
    "                query_per_metric = []\n",
    "                data_per_asset = {\"asset\":assetno,\"readings\":[]}\n",
    "                event_ctxt_info =  {\"body\":[data_per_asset]}\n",
    "                sql_query_args = self.sql_query_args\n",
    "                for index in list(metrics_per_asset.index):\n",
    "                    \n",
    "                    queries = (self.make_query_args(self.anomaly_detectors[index],sql_query_args))\n",
    "                    for query in queries:\n",
    "                        event_ctxt_info_exact = json.loads(query['event_context_info'])\n",
    "                        sql_query_args.update(query)\n",
    "                        event_ctxt_info['body'][0]['readings'].append(event_ctxt_info_exact['body'][0]['readings'][0])\n",
    "                        sql_query_args['parameter_list'] = '{}'.format(list(self.anomaly_detectors[index].data.columns[1:]))\n",
    "                        sql_query_args['event_context_info'] = json.dumps(event_ctxt_info)\n",
    "\n",
    "                col_vals.append(list(sql_query_args.values()))\n",
    "                print('Assetno : {} \\n sql_query: \\n{} \\n'.format(assetno,sql_query_args))\n",
    "        self.write_to_db(col_names,col_vals)\n",
    "        \n",
    "    def make_query_args(self,anomaly_detector,sql_query_args):\n",
    "\n",
    "        if(anomaly_detector.anom_indexes is not None):\n",
    "                anom_indexes = anomaly_detector.anom_indexes\n",
    "                original_data = anomaly_detector.data\n",
    "                col_index = anomaly_detector.data_col_index\n",
    "                metric_name = original_data.columns[anomaly_detector.data_col_index]\n",
    "                assetno = anomaly_detector.assetno\n",
    "                window = self.window_size\n",
    "                sql_query_args['event_name'] = '{}_'.format(original_data.columns[col_index])+anomaly_detector.algo_code+'_anomaly'\n",
    "                sql_query_args['event_source'] = anomaly_detector.algo_name\n",
    "                sql_query_args['operating_unit_serial_number'] = assetno\n",
    "                sql_query_args['parameter_list'] = '[{}]'.format(original_data.columns[anomaly_detector.data_col_index])\n",
    "                sql_queries = []\n",
    "                for i in anom_indexes:\n",
    "                    event_ctxt_info =  {\"body\":[]}\n",
    "                    data_per_asset = {\"asset\": '',\"readings\":[]}\n",
    "                    data_per_metric = {\"name\":'',\"datapoints\":''}\n",
    "\n",
    "                    time_series = (pd.to_datetime(original_data.index[i-window:i+window],unit='ms',utc=True))\n",
    "                    sql_query_args['event_timestamp'] =  str(pd.to_datetime(original_data.index[i],unit='ms',utc=True))\n",
    "                    sql_query_args['event_timestamp_epoch'] = str((original_data.index[i]))\n",
    "\n",
    "                    time_around_anoms = [\"''{}''\".format((t)) for t in time_series]                    \n",
    "\n",
    "                    data_per_metric['name']=metric_name\n",
    "                    datapoints = (list(zip(time_around_anoms,list(original_data.iloc[i-window:i+window,col_index].values))))\n",
    "                    data_per_metric['datapoints'] = datapoints\n",
    "                    data_per_asset['asset'] = assetno\n",
    "                    data_per_asset['readings'].append(data_per_metric)\n",
    "                    event_ctxt_info['body'].append(data_per_asset)\n",
    "\n",
    "                    sql_query_args['event_context_info'] = json.dumps(event_ctxt_info)\n",
    "                    sql_queries.append(sql_query_args)\n",
    "#                     print(\"event_name: \\n {} \\n event context info: \\n {} \\n\".format(sql_query_args['event_name'],sql_query_args['event_context_info']))\n",
    "\n",
    "        return (sql_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run data_handler.py -a\n",
    "\n",
    "class Data_reader():\n",
    "    \n",
    "    def __init__(self,reader_kwargs):\n",
    "        self.reader_kwargs = reader_kwargs\n",
    "        print(\"Data reader initialised \\n\")\n",
    "        \n",
    "    def read(self):\n",
    "        \n",
    "        response_dict=reader.reader_api(**self.reader_kwargs)\n",
    "#         print(response_dict)\n",
    "        print(\"Getting the dataset from the reader....\\n\")\n",
    "        entire_data = self.parse_dict_to_dataframe(response_dict)\n",
    "\n",
    "        try:\n",
    "            entire_data.index = entire_data['timestamp'].astype(np.int64)\n",
    "            del entire_data['timestamp']\n",
    "        except:\n",
    "            \n",
    "            pass\n",
    "        return entire_data\n",
    "    \n",
    "    def parse_dict_to_dataframe(self,response_dict):\n",
    "        entire_data_set = []\n",
    "        for data_per_asset in response_dict['body']:\n",
    "            dataframe_per_asset = []\n",
    "            assetno = data_per_asset['assetno']\n",
    "            for data_per_metric in data_per_asset['readings']:\n",
    "                data = pd.DataFrame(data_per_metric['datapoints'],columns=['timestamp',data_per_metric['name']])\n",
    "                # making index of dataframe as timestamp and deleting that column\n",
    "                data.index = data['timestamp']\n",
    "                del data['timestamp']\n",
    "                data['assetno']=assetno\n",
    "                dataframe_per_asset.append(data)\n",
    "            dataframe = pd.concat(dataframe_per_asset,axis=1)\n",
    "            dataframe = dataframe.T.drop_duplicates().T\n",
    "            cols = list(dataframe.columns)\n",
    "            cols.insert(0, cols.pop(cols.index('assetno')))\n",
    "            dataframe = dataframe[cols]\n",
    "            print('Asset no : {} \\n {} \\n'.format(assetno,dataframe.head()))\n",
    "            entire_data_set.append(dataframe)        \n",
    "\n",
    "        return entire_data_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
