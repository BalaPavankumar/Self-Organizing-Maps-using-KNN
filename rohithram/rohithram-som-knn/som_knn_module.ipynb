{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Self Organizing Maps using KNN for anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run as writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_module.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#torch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from skimage import io, transform\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing \n",
    "from matplotlib.pylab import rcParams\n",
    "import datetime as dt\n",
    "import argparse\n",
    "import time\n",
    "from seasonal import fit_seasons,adjust_seasons\n",
    "from scipy.signal import detrend\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#Importing reader and checker python files as modules\n",
    "import reader as reader\n",
    "import checker as checker\n",
    "import writer_configs as write_args\n",
    "import psycopg2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 9\n",
    "rcParams[ 'axes.grid']=True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_module.py -a\n",
    "\n",
    "class Som_net(nn.Module):\n",
    "    \n",
    "    def __init__(self,som_shape,input_feature_size,time_constant,n_iterations,\n",
    "                 minNumPerBmu=1,no_of_neighbors=3,initial_radius=1,initial_learning_rate=0.4,diff_order=1):\n",
    "        super(Som_net, self).__init__()\n",
    "        \n",
    "        self.shape = som_shape\n",
    "        self.weight_dim = self.shape.__len__()\n",
    "        self.feature_size = input_feature_size\n",
    "        self.time_constant = time_constant\n",
    "        self.initial_radius = initial_radius\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.weights = torch.rand((*self.shape,self.feature_size),dtype=torch.float64)\n",
    "        self.bmu_counts = torch.zeros((*self.shape),dtype=torch.int32)\n",
    "        self.n_iterations = n_iterations\n",
    "        self.neuron_locations = self.neuron_locations(*self.shape)\n",
    "        self.minNumPerBmu = minNumPerBmu\n",
    "        self.no_of_neighbors = no_of_neighbors\n",
    "        self.diff_order = diff_order\n",
    "#         self.allowed_nodes = torch.from_numpy(np.array())\n",
    "        \n",
    "    def findBMU(self,x_batch):\n",
    "        \"\"\"\n",
    "         Find the best matching unit for a specific batch of samples\n",
    "        :param x_batch: The data points for which the best matching unit should be found.\n",
    "        :type x_batch: numpy.ndarray\n",
    "        :return: numpy.ndarray with index\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = len(x_batch)\n",
    "        bmu_indexes = np.zeros((batch_size,2),dtype=int)\n",
    "#         print(bmu_indexes[0].shape)\n",
    "        for i in range(batch_size):\n",
    "#             print(np.square(self.weights.numpy()-x_batch[i].numpy()).sum(axis=-1))\n",
    "            bmu_dists = np.square(self.weights.numpy()-x_batch[i].numpy()).sum(axis=-1)\n",
    "            arg_min_ind = np.argmin(bmu_dists)\n",
    "#             print(arg_min_ind)\n",
    "            bmu_indexes[i] = np.unravel_index(arg_min_ind,bmu_dists.shape)\n",
    "#             print(bmu_indexes[i])\n",
    "#             print(self.bmu_counts[])\n",
    "            \n",
    "            self.bmu_counts[np.unravel_index(arg_min_ind,bmu_dists.shape)] +=1\n",
    "\n",
    "                           \n",
    "#         print(\"Bmu indexes shape {}\".format(bmu_indexes.shape))\n",
    "        return bmu_indexes\n",
    "    \n",
    "    \n",
    "    def fit(self,train_batch,curr_batch_iter):\n",
    "        \n",
    "        \"\"\"Train the SOM to a specific dataset.\n",
    "        :param train_batch: The complete training dataset\n",
    "        :type train_batch: 2d ndarray\n",
    "        :param num_iterations: The number of iterations used for training\n",
    "        :type num_iterations: int\n",
    "        :return: a reference to the object\n",
    "        \"\"\"\n",
    "        \n",
    "        bmu_indexes = self.findBMU(train_batch)\n",
    "        \n",
    "        curr_iter = np.array([curr_batch_iter+i for i in range(len(train_batch))])\n",
    "        # Update the parameters to let them decay to 0\n",
    "               \n",
    "        r_batch = self.decay_radius((curr_iter))\n",
    "        l_batch = self.decay_learning_rate(curr_iter)\n",
    "        \n",
    "    \n",
    "        self.update_weights(train_batch, bmu_indexes, r_batch, l_batch)\n",
    "        self.allowed_nodes = self.weights[self.bmu_counts >= self.minNumPerBmu]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def evaluate(self,evaluationData):\n",
    "        \"\"\"\n",
    "        This function maps the evaluation data to the previously fitted network. It calculates the anomaly measure\n",
    "        based on the distance between the observation and the K-NN nodes of this observation.\n",
    "        :param evaluationData: Numpy array of the data to be evaluated\n",
    "        :return: 1D-array with for each observation an anomaly measure\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.allowed_nodes\n",
    "            assert self.allowed_nodes.shape[0] > 1\n",
    "        except NameError:\n",
    "            raise Exception(\"Make sure the method fit is called before evaluating data.\")\n",
    "        except AssertionError:\n",
    "            raise Exception(\"There are no nodes satisfying the minimum criterium, algorithm cannot proceed.\")\n",
    "        else:\n",
    "            classifier = NearestNeighbors(n_neighbors=self.no_of_neighbors)\n",
    "            classifier.fit(self.allowed_nodes)\n",
    "            dist, _ = classifier.kneighbors(evaluationData)\n",
    "        return dist.mean(axis=1)\n",
    "    \n",
    "    \n",
    "    def neuron_locations(self,m,n):\n",
    "        r0 = np.arange(m) # Or r0,r1 = np.ogrid[:m,:n], out[:,:,0] = r0\n",
    "        r1 = np.arange(n)\n",
    "        out = np.empty((m,n,2),dtype=int)\n",
    "        out[:,:,0] = r0[:,None]\n",
    "        out[:,:,1] = r1\n",
    "        return out\n",
    "\n",
    "    def decay_radius(self,i):\n",
    "        return self.initial_radius * np.exp(-(1*i)/self.time_constant)\n",
    "\n",
    "    \n",
    "    def decay_learning_rate(self, i):\n",
    "        return self.initial_learning_rate * np.exp(-(1*i)/self.n_iterations)\n",
    "\n",
    "    \n",
    "    def calculate_influence(self,distance, radius,index):\n",
    "        return np.exp(-distance / (2* (radius**2)))\n",
    "\n",
    "    \n",
    "    def update_weights(self,train_batch, bmu_indexes,radius,learning_speed):\n",
    "        \n",
    "        '''\n",
    "        # now we know the BMU, update its weight vector to move closer to input\n",
    "        # and move its neighbours in 2-D space closer\n",
    "        # by a factor proportional to their 2-D distance from the BMU\n",
    "        '''\n",
    "        \n",
    "        batch_size = len(train_batch)\n",
    "        \n",
    "        #print(\"BMU Index for batch {}\".format(bmu_idx))\n",
    "        network_indexes = self.neuron_locations\n",
    "        network_indexes = np.stack([network_indexes for i in range(batch_size)],axis=0)\n",
    "#         print(\"Network shape {}\".format(network_indexes.shape))\n",
    "\n",
    "        bmu_indexes = bmu_indexes.reshape(batch_size,1,1,2)\n",
    "        learning_speed = learning_speed.reshape(batch_size,1,1,1)\n",
    "        \n",
    "        w_dists_coordinates = (np.square(network_indexes - bmu_indexes))\n",
    "        \n",
    "        w_dists = (w_dists_coordinates[:,:,:,0]+w_dists_coordinates[:,:,:,1])\n",
    "\n",
    "#         print(\"Shape of distance of neurons from bmu {}\".format(w_dists.shape))\n",
    "\n",
    "        bool_index = np.array([(w_dists[i]<=r2) for i,r2 in enumerate(radius**2)])\n",
    "        \n",
    "        influence = np.array([self.calculate_influence(w_dists[i][bool_index[i]],radius[i],bool_index[i]) \n",
    "                              for i in range((batch_size))])\n",
    "        \n",
    "        influence_neurons = np.zeros(w_dists.shape)\n",
    "#         print(influence.shape)\n",
    "        influence = influence.reshape(-1,)\n",
    "#         print(influence)\n",
    "        try:\n",
    "            influence_neurons[bool_index] = influence\n",
    "            influential_neurons = np.stack([influence_neurons for i in range(self.feature_size)],axis=-1)\n",
    "\n",
    "    #         print(\"Influtential neurons shape {}\".format(influential_neurons.shape))\n",
    "\n",
    "            learningMatrix = np.array([-torch.add(self.weights,-1*(train_batch[k])).numpy() for k in range(batch_size)])\n",
    "\n",
    "    #         print(\"Learning matrix shape {}\".format(learningMatrix.shape))\n",
    "\n",
    "    #         scaledLearningMatrix = np.zeros((batch_size,*weights.shape))\n",
    "            scaledLearningMatrix = learning_speed * (influential_neurons * learningMatrix)\n",
    "\n",
    "    #         print(\"Scaled lmatrix {}\".format(scaledLearningMatrix.shape))\n",
    "\n",
    "            [torch.add(self.weights,torch.from_numpy(scaledLearningMatrix)[k],out=self.weights) for k in range(batch_size)]\n",
    "        except:\n",
    "            return\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
