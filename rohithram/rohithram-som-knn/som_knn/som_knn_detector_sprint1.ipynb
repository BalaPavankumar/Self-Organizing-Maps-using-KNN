{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_detector.py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#torch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "#importing sklearn libraries\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import pickle \n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#importing error_codes\n",
    "import error_codes as error_codes\n",
    "import som_knn_module\n",
    "import traceback\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 9\n",
    "rcParams[ 'axes.grid']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_detector.py -a\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model,metric_names,filename='som_trained_model',target_dir=r\"Anomaly Detection Models/Machine Learning Models\"):\n",
    "    \n",
    "    try:\n",
    "        time_now = str(pd.to_datetime(dt.datetime.now(),utc=True))\n",
    "        filename = filename+'_{}_{}'.format('_'.join(metric_names),time_now)\n",
    "        filepath = r\"{}.pkl\".format(filename)\n",
    "#         print(filepath)\n",
    "#         filepath = os.path.join(target_dir,filename)\n",
    "        \n",
    "        filehandler = open('som_model_1.pkl', 'wb')\n",
    "        pickle.dump(model, filehandler)\n",
    "        print(\"\\nSaved model : {} in {},\\nLast Checkpointed at: {}\\n\".format(filename,target_dir,time_now))\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        pass\n",
    "#         error_codes.error_codes['unknown']['message'] = e\n",
    "#         return error_codes.error_codes['unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_detector.py -a\n",
    "\n",
    "def load_model():\n",
    "    filehandler = open('som_model_1.pkl', 'rb')\n",
    "    return pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_detector.py -a\n",
    "\n",
    "\n",
    "class Som_Detector():\n",
    "    def __init__(self,data,assetno,metric_names,model_input_args,training_args,anom_thres=7):\n",
    "        \n",
    "        '''\n",
    "        Class which is used to find Changepoints in the dataset with given algorithm parameters.\n",
    "        It has all methods related to finding anomalies to plotting those anomalies and returns the\n",
    "        data being analysed and anomaly indexes.\n",
    "        Arguments :\n",
    "        data -> dataframe which has one or two more metric columnwise\n",
    "        assetno -> assetno of the dataset\n",
    "        is_train -> By Default is False , as no training required for this algo\n",
    "        data_col_index -> column index of the metric to find changepoints on\n",
    "        pthres -> Default value :0.5 , (float) it is threshold after which a changepoint is flagged as on anomaly\n",
    "        mean_runlen -> (int) By default 100, It is the average gap between two changepoints , this comes from \n",
    "                       nitty gritty math of exponential distributions\n",
    "        Nw (samples to wait) -> (int) By default 10 is being used for optimal performance. It is the samples after which\n",
    "                                we start assigning probailities for it to be a changepoint.\n",
    "        to_plot -> True if you want to plot anomalies\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        self.algo_name = 'Self Organizing Map AD'\n",
    "        self.algo_code = 'som'\n",
    "        self.algo_type = 'multivariate'\n",
    "        self.istrain = training_args['is_train']\n",
    "        self.data = data\n",
    "        self.metric_name = metric_names\n",
    "        self.assetno = assetno\n",
    "        self.anom_indexes = None\n",
    "        self.data_col_index = None\n",
    "        self.model_input_args = model_input_args\n",
    "        self.training_args = training_args\n",
    "        self.anom_thres = anom_thres\n",
    "\n",
    "    def detect_anomalies(self):\n",
    "        \n",
    "        '''\n",
    "        Detects anomalies and returns data and anomaly indexes\n",
    "        '''\n",
    "        data = self.data\n",
    "        anom_thres = self.anom_thres\n",
    "        diff_order = self.model_input_args['diff_order']\n",
    "        \n",
    "        print(\"Shape of the dataset : \")\n",
    "        print(data.shape)\n",
    "        if(self.istrain):\n",
    "            net,data_set,train_data,test_data = train_som(torch.from_numpy(data[data.columns[1:]].values),\n",
    "                                                          self.model_input_args,self.training_args)\n",
    "            res = save_model(net,metric_names = self.metric_name)\n",
    "            if(res!=None):\n",
    "                return res\n",
    "            \n",
    "            anom_indexes = test(net,data_set[:,].numpy(),anom_thres=anom_thres,diff_order=diff_order)\n",
    "        else:\n",
    "            anom_indexes = test(net,test_data,anom_thres=anom_thres,diff_order=diff_order)\n",
    "\n",
    "        self.anom_indexes = anom_indexes\n",
    "          \n",
    "        print(\"\\n No of Anomalies detected = %g\"%(len(anom_indexes)))\n",
    "\n",
    "        return anom_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_detector.py -a\n",
    "\n",
    "\n",
    "\n",
    "class TimeSeries_Dataset(Dataset):\n",
    "    \"\"\"Time series dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,data,data_col_index=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: input data after all preprocessing done\n",
    "            Trains the model on this Tensor object data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "#         print(\"Overview of dataset : \\n {} \\n\".format(self.data.head()))\n",
    "            \n",
    "        print(self.data.dtype)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]        \n",
    "        \n",
    "        return (sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_detector.py -a\n",
    "\n",
    "\n",
    "def split_the_data(data,test_frac=0.1):\n",
    "    '''\n",
    "    Splitting the data into train and test with default ratio = 0.1\n",
    "    Splits the data in orderly manner not random\n",
    "    '''\n",
    "    train_data = data[0:int(np.ceil((1-test_frac)*data[:,].shape[0])),:]\n",
    "    test_data = data[-int(np.ceil(test_frac*data[:,].shape[0])):]\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_detector.py -a\n",
    "\n",
    "\n",
    "def plot_dataset(data):\n",
    "    fig = plt.figure()\n",
    "    print(\"Dataset has {} rows {} columns\".format(data[:,].shape[0],data[:,].shape[1]))\n",
    "\n",
    "    for i in range(data[:,].shape[-1]):\n",
    "        plt.plot(data[:,i])\n",
    "        plt.title(\"Plot of the dataset column wise\")\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_detector.py -a\n",
    "\n",
    "\n",
    "def process_data(data,test_frac,to_plot=True):\n",
    "    data_set = TimeSeries_Dataset(data)\n",
    "    train_data,test_data = split_the_data(data_set,test_frac=test_frac)\n",
    "    if(to_plot):\n",
    "        plot_dataset(train_data.numpy())\n",
    "    return data_set,train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_detector.py -a\n",
    "\n",
    "\n",
    "def network_dimensions(train_data,N=100):\n",
    "    \n",
    "    approx_network_size = 5*np.sqrt(N)\n",
    "    train_df = pd.DataFrame(train_data.numpy())\n",
    "    cov_train_df = train_df.cov()\n",
    "    cov_data = cov_train_df.values\n",
    "    \n",
    "    if(train_data.shape[-1]<=1):\n",
    "        x = sp.linalg.eigh(cov_data,eigvals_only=True)[-1:]\n",
    "        ratio = np.ceil(x)    \n",
    "    else:\n",
    "        x,y = sp.linalg.eigh(cov_data,eigvals_only=True)[-2:]\n",
    "        ratio = np.ceil(x)/np.ceil(y)\n",
    "        \n",
    "    row_dim = int(np.ceil(np.sqrt(approx_network_size*ratio)))\n",
    "    col_dim = int(np.ceil(approx_network_size/row_dim))\n",
    "    return row_dim,col_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_detector.py -a\n",
    "\n",
    "\n",
    "def test(net,evaluateData,anom_thres=7,diff_order=1,to_plot=True):\n",
    "        \n",
    "        original_data = evaluateData\n",
    "        no_cols = original_data.shape[-1]\n",
    "        print(\"Input data's shape: {}\".format(original_data.shape))\n",
    "        res_evaluateData = np.diff(evaluateData,n=diff_order,axis=0).reshape(-1,no_cols)\n",
    "        print(\"Differenced data shape {}\".format(res_evaluateData.shape))\n",
    "        \n",
    "#         res_evaluateData.reshape(-1,original_data.shape[-1])\n",
    "        # Fit the anomaly detector and apply to the evaluateData data\n",
    "        anomaly_metrics = net.evaluate(res_evaluateData) # Evaluate on the evaluateData data\n",
    "        print(anomaly_metrics.shape)\n",
    "        anomaly_metrics = anomaly_metrics/np.linalg.norm(anomaly_metrics)\n",
    "#         k=anom_thres\n",
    "        thres = anom_thres*(1/np.sqrt(len(anomaly_metrics)))\n",
    "#         thres = np.mean(anomaly_metrics)+k*np.std(anomaly_metrics)\n",
    "        selector = anomaly_metrics > thres\n",
    "        anom_indexes = np.arange(len(res_evaluateData))[selector]\n",
    "#         anom_indexes = anom_indexes+diff_order\n",
    "#         anom_indexes = np.arange(original_data.shape[0]-diff_order)[selector]\n",
    "        \n",
    "        \n",
    "        if(to_plot):\n",
    "            '''\n",
    "            # We make a density plot and a histogram showing the distrbution\n",
    "            # of the number of points mapped to a BMU\n",
    "            '''\n",
    "            figa = plt.figure(figsize=(20,10))\n",
    "            plt.subplot(121)\n",
    "            density = gaussian_kde(anomaly_metrics)\n",
    "            xs = np.linspace(0,5,200)\n",
    "            plt.plot(xs,density(xs))\n",
    "            plt.title(\"Distribution of dataset\")\n",
    "\n",
    "            plt.subplot(122)\n",
    "            plt.hist(net.bmu_counts)\n",
    "            plt.title(\"Histogram of Bmu counts\")\n",
    "            plt.show();\n",
    "            \n",
    "            fig = plt.figure(figsize=(20,10))\n",
    "            plt.plot(anomaly_metrics)\n",
    "            plt.title(\"Anomaly score\")\n",
    "            plt.axhline(y=thres,color='r',label=\"Threshold\")\n",
    "            plt.legend()\n",
    "            plt.show();\n",
    "\n",
    "            if(diff_order!=0):\n",
    "                fig2 = plt.figure(figsize=(20,10))\n",
    "                plt.plot(res_evaluateData[:,])\n",
    "                plt.title(\"Dataset after differencing marked with anomalies\")\n",
    "#                 plt.scatter(x=anom_indexes,y=res_evaluateData[anom_indexes,0],color='r')\n",
    "                [plt.axvline(x=ind,color='r') for ind in anom_indexes]\n",
    "                plt.show();\n",
    "\n",
    "                fig3 = plt.figure(figsize=(20,10))\n",
    "                plt.plot(original_data)\n",
    "                plt.title(\"Exact Dataset with detectedanomalies\")\n",
    "#                 plt.scatter(x=anom_indexes,y=original_data[anom_indexes,0],color='r')\n",
    "                [plt.axvline(x=ind,color='r') for ind in anom_indexes]\n",
    "\n",
    "                plt.show();\n",
    "\n",
    "            else:\n",
    "                fig3 = plt.figure(figsize=(20,10))\n",
    "                plt.plot(original_data)\n",
    "                plt.title(\"Exact Dataset with detectedanomalies\")\n",
    "#                 plt.scatter(x=anom_indexes,y=original_data[anom_indexes,0],color='r')\n",
    "                [plt.axvline(x=ind,color='r') for ind in anom_indexes]\n",
    "\n",
    "                plt.show();\n",
    "        \n",
    "        \n",
    "#         print(\"Anomaly indexes : {}\".format(anom_indexes))\n",
    "\n",
    "        no_anoms_detected = (list(selector).count(True))\n",
    "        print(\"No of anomalies detected : {}, Fraction of data detected as anomaly : {}\".\n",
    "              format(no_anoms_detected,no_anoms_detected/(evaluateData.shape[0])))\n",
    "        return anom_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_detector.py -a\n",
    "\n",
    "\n",
    "\n",
    "def train(net,train_loader,epochs):\n",
    "    curr_batch_iter = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch : {} completed \\n Max Bmu index : {}\".\n",
    "              format(epoch,np.unravel_index(torch.argmax(net.bmu_counts),net.bmu_counts.shape)))\n",
    "\n",
    "    for i,x_batch in enumerate(train_loader):\n",
    "            curr_batch_iter += 1\n",
    "            net = net.fit(x_batch,curr_batch_iter)\n",
    "    \n",
    "    print(\"\\n Training successfully completed \\n\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_detector.py -a\n",
    "\n",
    "\n",
    "def train_som(data,model_input_args,training_args):\n",
    "    \n",
    "    data_set,train_data,test_data = process_data(data=data,test_frac=training_args['test_frac'])\n",
    "    \n",
    "    if(model_input_args['som_shape']!=None):\n",
    "        row_dim,col_dim = network_dimensions(train_data,model_input_args['N'])\n",
    "    else:\n",
    "        row_dim,col_dim = model_input_args['som_shape']\n",
    "        \n",
    "    actual_network_size = row_dim*col_dim\n",
    "    \n",
    "    epochs = training_args['epochs']\n",
    "    batch_size = training_args['batch_size']\n",
    "    \n",
    "    # initial neighbourhood radius\n",
    "    if(model_input_args['initial_radius']==None):\n",
    "        init_radius = max(row_dim,col_dim)/2\n",
    "    else:\n",
    "        init_radius = model_input_args['initial_radius']\n",
    "        \n",
    "    # initial learning rate\n",
    "    init_learning_rate = model_input_args['initial_learning_rate']\n",
    "    # radius decay parameter\n",
    "    n_iterations = int(epochs*(len(train_data)/batch_size))\n",
    "    time_constant = n_iterations/np.log(init_radius)\n",
    "    \n",
    "    model_input_args['som_shape'] = (row_dim,col_dim)\n",
    "    model_input_args['input_feature_size'] = train_data.shape[-1]\n",
    "    model_input_args['initial_radius'] = init_radius\n",
    "    model_input_args['initial_learning_rate'] = init_learning_rate\n",
    "    model_input_args['time_constant'] = time_constant\n",
    "    model_input_args['n_iterations'] = n_iterations\n",
    "    \n",
    "    \n",
    "    print(\"Network dimensions are {} x {} \\n\".format(row_dim,col_dim))\n",
    "    diff_order = model_input_args['diff_order']\n",
    "    del model_input_args['diff_order']\n",
    "    del model_input_args['N']\n",
    "    net = som_knn_module.Som_net(**model_input_args)\n",
    "\n",
    "    \n",
    "    res_series = (np.diff(train_data.numpy().reshape(-1),n=diff_order))\n",
    "    \n",
    "    train_data_diff = torch.from_numpy(res_series)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data_diff, batch_size=batch_size,shuffle=True)\n",
    "    \n",
    "#     if(training_args['freeze']==False):\n",
    "    net = train(net=net,epochs=epochs,train_loader=train_loader)\n",
    "    \n",
    "    return net,data_set,train_data,test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
