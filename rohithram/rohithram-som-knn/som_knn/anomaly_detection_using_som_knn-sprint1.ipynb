{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run as writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_wrapper_sprint1.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import pickle\n",
    "\n",
    "#torch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "#importing sklearn libraries\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# Importing db properties and writer args python files as modules\n",
    "import db_properties as db_props\n",
    "import writer_configs as write_args\n",
    "import csv_prep_for_reader as csv_reader\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "from preprocessors import *\n",
    "from data_handler import *\n",
    "import som_knn_detector as som_detector\n",
    "import som_knn_module as som_model\n",
    "\n",
    "import error_codes as error_codes\n",
    "import type_checker as type_checker\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 9\n",
    "rcParams[ 'axes.grid']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "from IPython import display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_wrapper_sprint1.py -a\n",
    "\n",
    "\n",
    "\n",
    "ideal_train_kwargs_type  = {\n",
    "            'som_shape':tuple,\n",
    "            'input_feature_size':int,\n",
    "            'time_constant':float,\n",
    "            'minNumPerBmu':int,\n",
    "            'no_of_neighbors':int,\n",
    "            'initial_radius':float,\n",
    "            'initial_learning_rate':float,\n",
    "            'n_iterations':int,\n",
    "            'N':int,    \n",
    "            'diff_order':int,\n",
    "            'is_train':bool,\n",
    "            'epochs':int,\n",
    "            'batch_size':int,\n",
    "            'to_plot':bool,\n",
    "            'test_frac':float\n",
    "        }\n",
    "\n",
    "ideal_eval_kwargs_type = {\n",
    "            'model_path':str,\n",
    "            'to_plot':bool,\n",
    "            'anom_thres':int\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_wrapper_sprint1.py -a\n",
    "\n",
    "\n",
    "\n",
    "def train(assetno,from_timestamp,to_timestamp,con,para_list,source_type='opentsdb',table_name='',\n",
    "        qry_str='',impute_fill_method='forward',down_sampling_method=None,down_sampling_window=None,freq=None,\n",
    "        resample_fill_method=None,to_resample=None,to_impute=None,\n",
    "        network_shape=None,input_feature_size=None,time_constant=None,minNumPerBmu=2,no_of_neighbours=10,init_radius=0.4,\n",
    "        init_learning_rate=0.01,N=100,diff_order=1,is_train=True,epochs=4,batch_size=4,to_plot=True,test_frac=0.5):\n",
    "\n",
    "        '''\n",
    "        Wrapper function which should be called inorder to run the anomaly detection, it has four parts :\n",
    "        *reader           - Class Data_reader defined in data_handler.py which takes in reader args and parses json \n",
    "                            and gives dataframes\n",
    "        *preprocessor     - preprocessors are defined in preprocessors.py, which takes in data and gives out processed \n",
    "                            data\n",
    "        *anomaly detector - Class Bayesian_Changept_Detector defined in bayesian_changept_detector.py, which takes in\n",
    "                            data and algorithm parameters as argument and returns anomaly indexes and data.        \n",
    "        *writer           - Class Postgres_Writer defined in data_handler.py which takes in anomaly detector object and\n",
    "                            and sql_queries , db_properties and table name as args and gives out response code.\n",
    "        \n",
    "        Arguments :\n",
    "        It takes reader args as of now to get the dataset and algo related arguments\n",
    "        Note:\n",
    "        To run this, import this python file as module and call this function with required args and it will detect\n",
    "        anomalies and writes to the local database.\n",
    "        This algorithm is univariate, so each metric per asset is processed individually\n",
    "        '''\n",
    "        \n",
    "        #reader arguments\n",
    "        reader_kwargs={\n",
    "            'assetno':assetno,\n",
    "            'from_timestamp':from_timestamp,\n",
    "            'to_timestamp':to_timestamp,\n",
    "            'con':con,\n",
    "            'para_list':para_list,\n",
    "            'source_type':source_type,\n",
    "            'table_name':table_name,\n",
    "            'qry_str':qry_str,\n",
    "            'impute_fill_method':impute_fill_method,\n",
    "            'down_sampling_method':down_sampling_method,\n",
    "            'down_sampling_window':down_sampling_window,\n",
    "            'freq':freq,\n",
    "            'resample_fill_method':resample_fill_method,\n",
    "            'to_resample':to_resample,\n",
    "            'to_impute':to_impute\n",
    "        }\n",
    "        \n",
    "        #algorithm arguments\n",
    "\n",
    "        model_input_args = {\n",
    "            'som_shape':network_shape,\n",
    "            'input_feature_size':None,\n",
    "            'time_constant':None,\n",
    "            'minNumPerBmu':minNumPerBmu,\n",
    "            'no_of_neighbors':no_of_neighbours,\n",
    "            'initial_radius':init_radius,\n",
    "            'initial_learning_rate':init_learning_rate,\n",
    "            'n_iterations':None,\n",
    "            'N':N,    \n",
    "            'diff_order':diff_order\n",
    "        }\n",
    "        \n",
    "        #Training arguments\n",
    "        training_args = {\n",
    "            'is_train':True,\n",
    "            'epochs':epochs,\n",
    "            'batch_size':batch_size,\n",
    "            'to_plot':to_plot,\n",
    "            'test_frac':test_frac\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "        #merging all algo arguments for params checking\n",
    "        algo_kwargs = {**model_input_args,**training_args}\n",
    "        \n",
    "                    \n",
    "        try: \n",
    "            '''\n",
    "            #reseting the error_codes to avoid overwritting\n",
    "            #error_codes is a python file imported as error_codes which has error_codes dictionary mapping \n",
    "            #for different kinds errors and reset function to reset them.\n",
    "            '''\n",
    "            \n",
    "            error_codes.reset()\n",
    "            # type_checker is python file which has Type_checker class which checks given parameter types\n",
    "            checker = type_checker.Type_checker(kwargs=algo_kwargs,ideal_args_type=ideal_train_kwargs_type)\n",
    "            # res is None when no error raised, otherwise it stores the appropriate error message\n",
    "            res = checker.params_checker()\n",
    "            if(res!=None):\n",
    "                return res\n",
    "            \n",
    "            # instanstiating the reader class with reader arguments\n",
    "            data_reader = Data_reader(reader_kwargs=reader_kwargs)\n",
    "            #getting list of dataframes per asset if not empty\n",
    "            #otherwise gives string 'Empty Dataframe'\n",
    "            entire_data = data_reader.read()\n",
    "            \n",
    "            writer_data = []\n",
    "            anomaly_detectors = []\n",
    "            \n",
    "            if((len(entire_data)!=0 and entire_data!=None and type(entire_data)!=dict)):\n",
    "\n",
    "                '''\n",
    "                looping over the data per assets and inside that looping over metrics per asset\n",
    "                * Instantiates anomaly detector class with algo args and metric index to detect on\n",
    "                * Stores the anomaly indexes and anomaly detector object to bulk write to db at once\n",
    "                '''\n",
    "\n",
    "                for i,data_per_asset in enumerate(entire_data):\n",
    "                    assetno = reader_kwargs['assetno'][i]\n",
    "                    data_per_asset[data_per_asset.columns[1:]] = normalise_standardise(data_per_asset[data_per_asset.columns[1:]]\n",
    "                                                                 )\n",
    "                    \n",
    "                    \n",
    "                    print(\"Data of Asset no: {} \\n {}\\n\".format(assetno,data_per_asset.head()))\n",
    "                    cols = list(data_per_asset.columns[1:])\n",
    "                    \n",
    "                    anomaly_detector = som_detector.Som_Detector(data = data_per_asset,                                                            assetno=assetno,model_input_args=model_input_args,\n",
    "                                                                 training_args=training_args,metric_names=cols,\n",
    "                                                                eval_args=None)\n",
    "                    \n",
    "                    anom_indexes = anomaly_detector.detect_anomalies()\n",
    "                    anomaly_detectors.append(anomaly_detector)\n",
    "                                       \n",
    "                    \n",
    "                \n",
    "                '''\n",
    "                Instantiates writer class to write into local database with arguments given below\n",
    "                Used for Bulk writing\n",
    "                '''\n",
    "                sql_query_args = write_args.writer_kwargs\n",
    "                table_name = write_args.table_name\n",
    "                window_size = 10\n",
    "                    \n",
    "                writer = Postgres_Writer(anomaly_detectors,db_credentials=db_props.db_connection,sql_query_args=sql_query_args,\n",
    "                                        table_name=table_name,window_size=window_size)\n",
    "\n",
    "                #called for mapping args before writing into db\n",
    "                res = writer.map_outputs_and_write()\n",
    "                return res\n",
    "            else:\n",
    "                '''\n",
    "                Data empty error\n",
    "                '''\n",
    "                return error_codes.error_codes['data_missing']\n",
    "        except Exception as e:\n",
    "            '''\n",
    "            unknown exceptions are caught here and traceback used to know the source of the error\n",
    "            '''\n",
    "            traceback.print_exc()\n",
    "            error_codes.error_codes['unknown']['message']=e\n",
    "            return error_codes.error_codes['unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_wrapper_sprint1.py -a\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(assetno,from_timestamp,to_timestamp,con,para_list,model_path,source_type='opentsdb',table_name='',\n",
    "        qry_str='',impute_fill_method='forward',down_sampling_method=None,down_sampling_window=None,freq=None,\n",
    "        resample_fill_method=None,to_resample=None,to_impute=None,to_plot=True,anom_thres=3):\n",
    "\n",
    "    \n",
    "        '''\n",
    "        Wrapper function which should be called inorder to run the anomaly detection, it has four parts :\n",
    "        *reader           - Class Data_reader defined in data_handler.py which takes in reader args and parses json \n",
    "                            and gives dataframes\n",
    "        *preprocessor     - preprocessors are defined in preprocessors.py, which takes in data and gives out processed \n",
    "                            data\n",
    "        *anomaly detector - Class Bayesian_Changept_Detector defined in bayesian_changept_detector.py, which takes in\n",
    "                            data and algorithm parameters as argument and returns anomaly indexes and data.        \n",
    "        *writer           - Class Postgres_Writer defined in data_handler.py which takes in anomaly detector object and\n",
    "                            and sql_queries , db_properties and table name as args and gives out response code.\n",
    "        \n",
    "        Arguments :\n",
    "        It takes reader args as of now to get the dataset and algo related arguments\n",
    "        Note:\n",
    "        To run this, import this python file as module and call this function with required args and it will detect\n",
    "        anomalies and writes to the local database.\n",
    "        This algorithm is univariate, so each metric per asset is processed individually\n",
    "        '''\n",
    "        \n",
    "        #reader arguments\n",
    "        reader_kwargs={\n",
    "            'assetno':assetno,\n",
    "            'from_timestamp':from_timestamp,\n",
    "            'to_timestamp':to_timestamp,\n",
    "            'con':con,\n",
    "            'para_list':para_list,\n",
    "            'source_type':source_type,\n",
    "            'table_name':table_name,\n",
    "            'qry_str':qry_str,\n",
    "            'impute_fill_method':impute_fill_method,\n",
    "            'down_sampling_method':down_sampling_method,\n",
    "            'down_sampling_window':down_sampling_window,\n",
    "            'freq':freq,\n",
    "            'resample_fill_method':resample_fill_method,\n",
    "            'to_resample':to_resample,\n",
    "            'to_impute':to_impute\n",
    "        }\n",
    "        \n",
    "        eval_args = {\n",
    "            'model_path':model_path,\n",
    "            'to_plot':to_plot,\n",
    "            'anom_thres':anom_thres\n",
    "        }\n",
    "                \n",
    "                    \n",
    "        try: \n",
    "            '''\n",
    "            #reseting the error_codes to avoid overwritting\n",
    "            #error_codes is a python file imported as error_codes which has error_codes dictionary mapping \n",
    "            #for different kinds errors and reset function to reset them.\n",
    "            '''\n",
    "            \n",
    "            error_codes.reset()\n",
    "            # type_checker is python file which has Type_checker class which checks given parameter types\n",
    "            checker = type_checker.Type_checker(kwargs=eval_args,ideal_args_type=ideal_eval_kwargs_type)\n",
    "            # res is None when no error raised, otherwise it stores the appropriate error message\n",
    "            res = checker.params_checker()\n",
    "            if(res!=None):\n",
    "                return res\n",
    "            \n",
    "            # instanstiating the reader class with reader arguments\n",
    "            data_reader = Data_reader(reader_kwargs=reader_kwargs)\n",
    "            #getting list of dataframes per asset if not empty\n",
    "            #otherwise gives string 'Empty Dataframe'\n",
    "            entire_data = data_reader.read()\n",
    "            \n",
    "            writer_data = []\n",
    "            anomaly_detectors = []\n",
    "            \n",
    "            if((len(entire_data)!=0 and entire_data!=None and type(entire_data)!=dict)):\n",
    "\n",
    "                '''\n",
    "                looping over the data per assets and inside that looping over metrics per asset\n",
    "                * Instantiates anomaly detector class with algo args and metric index to detect on\n",
    "                * Stores the anomaly indexes and anomaly detector object to bulk write to db at once\n",
    "                '''\n",
    "\n",
    "                for i,data_per_asset in enumerate(entire_data):\n",
    "                    assetno = reader_kwargs['assetno'][i]\n",
    "                    data_per_asset[data_per_asset.columns[1:]] = normalise_standardise(data_per_asset[data_per_asset.columns[1:]]\n",
    "                                                                 )\n",
    "                    \n",
    "                    print(\"Data of Asset no: {} \\n {}\\n\".format(assetno,data_per_asset.head()))\n",
    "                    cols = list(data_per_asset.columns[1:])\n",
    "                    \n",
    "                    anomaly_detector = som_detector.Som_Detector(data = data_per_asset,                                                            assetno=assetno,model_input_args=model_input_args,\n",
    "                                                                 training_args=None,metric_names=cols,eval_args=eval_args)\n",
    "                    \n",
    "                    anom_indexes = anomaly_detector.detect_anomalies()\n",
    "                    anomaly_detectors.append(anomaly_detector)\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                '''\n",
    "                Instantiates writer class to write into local database with arguments given below\n",
    "                Used for Bulk writing\n",
    "                '''\n",
    "                sql_query_args = write_args.writer_kwargs\n",
    "                table_name = write_args.table_name\n",
    "                window_size = 10\n",
    "                    \n",
    "                writer = Postgres_Writer(anomaly_detectors,db_credentials=db_props.db_connection,sql_query_args=sql_query_args,\n",
    "                                        table_name=table_name,window_size=window_size)\n",
    "\n",
    "                #called for mapping args before writing into db\n",
    "                res = writer.map_outputs_and_write()\n",
    "                return res\n",
    "            else:\n",
    "                '''\n",
    "                Data empty error\n",
    "                '''\n",
    "                return error_codes.error_codes['data_missing']\n",
    "        except Exception as e:\n",
    "            '''\n",
    "            unknown exceptions are caught here and traceback used to know the source of the error\n",
    "            '''\n",
    "            traceback.print_exc()\n",
    "            error_codes.error_codes['unknown']['message']=e\n",
    "            return error_codes.error_codes['unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run som_knn_wrapper_sprint1.py -a\n",
    "\n",
    "\n",
    "\n",
    "reader_kwargs= lambda:{\n",
    "            'assetno':['1'],\n",
    "            'from_timestamp':'',\n",
    "            'to_timestamp':'',\n",
    "            'con':'',\n",
    "            'para_list':'',\n",
    "            'source_type':'',\n",
    "            'table_name':'',\n",
    "            'qry_str':'',\n",
    "            'impute_fill_method':'forward',\n",
    "            'down_sampling_method':None,\n",
    "            'down_sampling_window':None,\n",
    "            'freq':None,\n",
    "            'resample_fill_method':None,\n",
    "            'to_resample':None,\n",
    "            'to_impute':True,\n",
    "}\n",
    "\n",
    "model_input_args = lambda :{\n",
    "    'network_shape':(8,8),\n",
    "    'input_feature_size':None,\n",
    "    'time_constant':None,\n",
    "    'minNumPerBmu':2,\n",
    "    'no_of_neighbours':3,\n",
    "    'init_radius':0.4,\n",
    "    'init_learning_rate':0.01,\n",
    "    'N':100,    \n",
    "    'diff_order':1\n",
    "}\n",
    "\n",
    "training_args = lambda:{\n",
    "            'is_train':True,\n",
    "            'epochs':5,\n",
    "            'batch_size':4,\n",
    "            'to_plot':True,\n",
    "            'test_frac':0.7\n",
    "        }\n",
    "\n",
    "\n",
    "        \n",
    "eval_args = lambda: {\n",
    "    'model_path':'',\n",
    "    'to_plot':True,\n",
    "    'anom_thres':3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile_run bayeschangept_sprint1.py -a\n",
    "\n",
    "'''\n",
    "Arguments for reader module to get data from opentsdb\n",
    "This is included for now just for testing, later the main function will take json as direct input\n",
    "'''\n",
    "\n",
    "assetno = ['TSFAD_A1']\n",
    "con = '192.168.2.5:4242'\n",
    "src_type =  'opentsdb'\n",
    "param = ['FE-001.TEMPERATURE']\n",
    "# from_timestamp=1516147200000\n",
    "# to_timestamp=1528109111000\n",
    "\n",
    "from_timestamp = 1520402214\n",
    "to_timestamp = 1520407294"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on data from opentsdb and saving it in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reader initialised \n",
      "\n",
      "{'code': '404', 'status': 'Not Found', 'message': '(<class \\'requests.exceptions.InvalidURL\\'>, InvalidURL(\"Invalid URL \\'http:///api/query?start=1520402214&end=1520407294&ms=true&m=max:none:FE-001.TEMPERATURE{AssetNo=TSFAD_A1}\\': No host supplied\",), <traceback object at 0x000001D979EB3DC8>)'}\n"
     ]
    }
   ],
   "source": [
    "# %%writefile_run bayeschangept_sprint1.py -a\n",
    "\n",
    "'''\n",
    "Dictionary of arguments given to wrapper function which executes this whole program for detecting changepoints and writing\n",
    "to database\n",
    "'''\n",
    "\n",
    "reader_kwargs1 = reader_kwargs()\n",
    "reader_kwargs1['assetno'] = assetno\n",
    "reader_kwargs1['source_type']=src_type\n",
    "reader_kwargs1['from_timestamp'] = from_timestamp\n",
    "reader_kwargs1['to_timestamp'] = to_timestamp\n",
    "reader_kwargs1['para_list'] = param\n",
    "model_input_args1 = model_input_args()\n",
    "training_args1 = training_args()\n",
    "kwargs1 = {**reader_kwargs1,**model_input_args1,**training_args1}\n",
    "res = train(**kwargs1)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing saved model on data from opentsdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reader initialised \n",
      "\n",
      "{'code': '404', 'status': 'Not Found', 'message': '(<class \\'requests.exceptions.InvalidURL\\'>, InvalidURL(\"Invalid URL \\'http:///api/query?start=1520402214&end=1520407294&ms=true&m=max:none:FE-001.TEMPERATURE{AssetNo=TSFAD_A1}\\': No host supplied\",), <traceback object at 0x000001D979EB36C8>)'}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Dictionary of arguments given to wrapper function which executes this whole program for detecting changepoints with \n",
    "a trained model and writing anomalies to database\n",
    "'''\n",
    "\n",
    "reader_kwargs1 = reader_kwargs()\n",
    "reader_kwargs1['assetno'] = assetno\n",
    "reader_kwargs1['source_type']=src_type\n",
    "reader_kwargs1['from_timestamp'] = from_timestamp\n",
    "reader_kwargs1['to_timestamp'] = to_timestamp\n",
    "reader_kwargs1['para_list'] = param\n",
    "eval_args1 = eval_args()\n",
    "eval_args1['anom_thres'] = 3\n",
    "eval_args1['model_path'] = './Anomaly_Detection_Models/Machine_Learning_Models/som_trained_model_AlcoholdemandlogspiritsconsumptionperheadUK18701938_1529931394207'\n",
    "kwargs1 = {**reader_kwargs1,**eval_args1}\n",
    "res = evaluate(**kwargs1)\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on data from a list of sample csv datasets and saving them in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detecting anomalies for alcohol-demand-log-spirits-consu.csv\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: 1 \n",
      "                assetno  \\\n",
      "timestamp                \n",
      "-3.147898e+12      1.0   \n",
      "-3.137357e+12      1.0   \n",
      "-3.126816e+12      1.0   \n",
      "-3.116362e+12      1.0   \n",
      "-3.105821e+12      1.0   \n",
      "\n",
      "               Alcohol demand (log spirits consumption per head), UK, 1870-1938  \n",
      "timestamp                                                                        \n",
      "-3.147898e+12                                           0.026580                 \n",
      "-3.137357e+12                                           0.114869                 \n",
      "-3.126816e+12                                           0.247302                 \n",
      "-3.116362e+12                                           0.379735                 \n",
      "-3.105821e+12                                           0.423880                 \n",
      "\n",
      "Shape of the Entire dataset : torch.Size([207, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([63, 1]) and Test dataset :torch.Size([145, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (62, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_AlcoholdemandlogspiritsconsumptionperheadUK18701938_1529940390881 in Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1529940390881\n",
      "\n",
      "Input data's shape: (207, 1)\n",
      "Differenced data shape (206, 1)\n",
      "(206,)\n",
      "No of anomalies detected : 4, Fraction of data detected as anomaly : 0.01932367149758454\n",
      "Postgres writer initialised \n",
      "\n",
      "\n",
      "Multivariate writer initialised\n",
      "\n",
      " Successfully written into database\n",
      "\n",
      "{'code': '200', 'status': 'OK'}\n",
      "\n",
      "Detecting anomalies for average-annual-temperature-centr.csv\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: 1 \n",
      "            assetno  Average annual temperature, central England, 1723 ? 1970\n",
      "timestamp                                                                   \n",
      "0.0            1.0                                           0.920853       \n",
      "0.0            1.0                                           0.089020       \n",
      "0.0            1.0                                          -0.925817       \n",
      "0.0            1.0                                           0.205476       \n",
      "0.0            1.0                                           1.203676       \n",
      "\n",
      "Shape of the Entire dataset : torch.Size([248, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([75, 1]) and Test dataset :torch.Size([174, 1])\n",
      "\n",
      "Network dimensions are 10 x 5 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (75, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_AverageannualtemperaturecentralEngland17231970_1529940391048 in Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1529940391048\n",
      "\n",
      "Input data's shape: (248, 1)\n",
      "Differenced data shape (248, 1)\n",
      "(248,)\n",
      "No of anomalies detected : 3, Fraction of data detected as anomaly : 0.012096774193548387\n",
      "Postgres writer initialised \n",
      "\n",
      "\n",
      "Multivariate writer initialised\n",
      "\n",
      " Successfully written into database\n",
      "\n",
      "{'code': '200', 'status': 'OK'}\n",
      "\n",
      "Detecting anomalies for mean-monthly-temperature-1907-19.csv\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: 1 \n",
      "                assetno  Mean monthly temperature, 1907 ? 1972\n",
      "timestamp                                                    \n",
      "-1.988150e+12      1.0                              -1.280625\n",
      "-1.985472e+12      1.0                              -0.477613\n",
      "-1.983053e+12      1.0                              -0.667301\n",
      "-1.980374e+12      1.0                               0.091451\n",
      "-1.977782e+12      1.0                              -0.110883\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([792, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([238, 1]) and Test dataset :torch.Size([555, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (238, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_Meanmonthlytemperature19071972_1529940391351 in Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1529940391351\n",
      "\n",
      "Input data's shape: (792, 1)\n",
      "Differenced data shape (792, 1)\n",
      "(792,)\n",
      "No of anomalies detected : 3, Fraction of data detected as anomaly : 0.003787878787878788\n",
      "Postgres writer initialised \n",
      "\n",
      "\n",
      "Multivariate writer initialised\n",
      "\n",
      " Successfully written into database\n",
      "\n",
      "{'code': '200', 'status': 'OK'}\n",
      "\n",
      "Detecting anomalies for methane-input-into-gas-furnace-c.csv\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: 1 \n",
      "               assetno       CO2   Methane\n",
      "timestamp                                \n",
      "7.808400e+12      1.0  0.090839 -1.170356\n",
      "7.808940e+12      1.0  0.028381 -1.340206\n",
      "7.809480e+12      1.0 -0.002849 -1.062837\n",
      "7.810020e+12      1.0 -0.002849 -0.811959\n",
      "7.810560e+12      1.0 -0.034078 -0.758978\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([296, 2])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([89, 2]) and Test dataset :torch.Size([208, 2])\n",
      "\n",
      "Network dimensions are 5 x 10 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (87, 2)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_CO2_Methane_1529940391563 in Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1529940391563\n",
      "\n",
      "Input data's shape: (296, 2)\n",
      "Differenced data shape (294, 2)\n",
      "(294,)\n",
      "No of anomalies detected : 3, Fraction of data detected as anomaly : 0.010135135135135136\n",
      "Postgres writer initialised \n",
      "\n",
      "\n",
      "Multivariate writer initialised\n",
      "\n",
      " Successfully written into database\n",
      "\n",
      "{'code': '200', 'status': 'OK'}\n",
      "\n",
      "Detecting anomalies for monthly-us-female-20-years-and-o.csv\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: 1 \n",
      "                assetno  \\\n",
      "timestamp                \n",
      "-6.943104e+11      1.0   \n",
      "-6.916320e+11      1.0   \n",
      "-6.891264e+11      1.0   \n",
      "-6.864480e+11      1.0   \n",
      "-6.838560e+11      1.0   \n",
      "\n",
      "               Monthly U.S. female (20 years and over) unemployment figures (10**3) 1948-1981  \n",
      "timestamp                                                                                      \n",
      "-6.943104e+11                                          -1.351533                               \n",
      "-6.916320e+11                                          -1.054180                               \n",
      "-6.891264e+11                                          -1.138722                               \n",
      "-6.864480e+11                                          -1.183908                               \n",
      "-6.838560e+11                                          -1.285941                               \n",
      "\n",
      "Shape of the Entire dataset : torch.Size([408, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([123, 1]) and Test dataset :torch.Size([286, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (121, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_MonthlyUSfemale20yearsandoverunemploymentfigures10319481981_1529940391780 in Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1529940391780\n",
      "\n",
      "Input data's shape: (408, 1)\n",
      "Differenced data shape (406, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406,)\n",
      "No of anomalies detected : 4, Fraction of data detected as anomaly : 0.00980392156862745\n",
      "Postgres writer initialised \n",
      "\n",
      "\n",
      "Multivariate writer initialised\n",
      "\n",
      " Successfully written into database\n",
      "\n",
      "{'code': '200', 'status': 'OK'}\n",
      "\n",
      "Detecting anomalies for winter-negative-temperature-sum-.csv\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: 1 \n",
      "            assetno  Winter negative temperature sum (in deg. C), 1781 ? 1988\n",
      "timestamp                                                                   \n",
      "0.0            1.0                                          -0.027888       \n",
      "0.0            1.0                                          -0.507193       \n",
      "0.0            1.0                                           2.003772       \n",
      "0.0            1.0                                           2.091459       \n",
      "0.0            1.0                                          -0.114832       \n",
      "\n",
      "Shape of the Entire dataset : torch.Size([208, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([63, 1]) and Test dataset :torch.Size([146, 1])\n",
      "\n",
      "Network dimensions are 10 x 5 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (62, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_WinternegativetemperaturesumindegC17811988_1529940391937 in Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1529940391937\n",
      "\n",
      "Input data's shape: (208, 1)\n",
      "Differenced data shape (207, 1)\n",
      "(207,)\n",
      "No of anomalies detected : 3, Fraction of data detected as anomaly : 0.014423076923076924\n",
      "Postgres writer initialised \n",
      "\n",
      "\n",
      "Multivariate writer initialised\n",
      "\n",
      " Successfully written into database\n",
      "\n",
      "{'code': '200', 'status': 'OK'}\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'dataset/sample_csv_files/'\n",
    "target_dir = 'dataset/reader_csv_files/'\n",
    "assetno = ['1']\n",
    "difforders = [1,0,0,2,2,1]\n",
    "\n",
    "for i,filename in enumerate(os.listdir(data_dir)):\n",
    "    \n",
    "    name,ext = os.path.splitext(filename)\n",
    "    if ext != '.csv':continue\n",
    "    infile = os.path.join(data_dir,filename)\n",
    "    print(\"\\nDetecting anomalies for {}\\n\".format(filename))\n",
    "    \n",
    "    \n",
    "    kwargs1 = csv_reader.get_csv_kwargs(infile=infile,filename=filename,target_dir=target_dir,assetno=assetno[0])\n",
    "    \n",
    "#     print('params: {}\\n'.format(param))\n",
    "    reader_kwargs1 = kwargs1\n",
    "    model_input_args1 = model_input_args()\n",
    "    model_input_args1['diff_order'] = difforders[i]\n",
    "    training_args1 = training_args()\n",
    "    training_args1['to_plot'] = False\n",
    "    kwargs1 = {**reader_kwargs1,**model_input_args1,**training_args1}\n",
    "    res = train(**kwargs1)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the saved model on sample csv datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: 1 \n",
      "                assetno  \\\n",
      "timestamp                \n",
      "-3.147898e+12      1.0   \n",
      "-3.137357e+12      1.0   \n",
      "-3.126816e+12      1.0   \n",
      "-3.116362e+12      1.0   \n",
      "-3.105821e+12      1.0   \n",
      "\n",
      "               Alcohol demand (log spirits consumption per head), UK, 1870-1938  \n",
      "timestamp                                                                        \n",
      "-3.147898e+12                                           0.026580                 \n",
      "-3.137357e+12                                           0.114869                 \n",
      "-3.126816e+12                                           0.247302                 \n",
      "-3.116362e+12                                           0.379735                 \n",
      "-3.105821e+12                                           0.423880                 \n",
      "\n",
      "Shape of the Entire dataset : torch.Size([207, 1])\n",
      "\n",
      "Input data's shape: (207, 1)\n",
      "Differenced data shape (206, 1)\n",
      "(206,)\n",
      "No of anomalies detected : 4, Fraction of data detected as anomaly : 0.01932367149758454\n",
      "\n",
      " No of Anomalies detected = 4\n",
      "Postgres writer initialised \n",
      "\n",
      "\n",
      "Multivariate writer initialised\n",
      "\n",
      " Successfully written into database\n",
      "\n",
      "{'code': '200', 'status': 'OK'}\n"
     ]
    }
   ],
   "source": [
    "infile = './dataset/sample_csv_files/alcohol-demand-log-spirits-consu.csv'\n",
    "filename = 'alcohol-demand-log-spirits-consu.csv'\n",
    "target_dir = 'dataset/reader_csv_files/'\n",
    "assetno = ['1']\n",
    "kwargs1 = csv_reader.get_csv_kwargs(infile=infile,filename=filename,target_dir=target_dir,assetno=assetno[0])\n",
    "    \n",
    "reader_kwargs1 = kwargs1\n",
    "eval_args1 = eval_args()\n",
    "eval_args1['to_plot'] = False\n",
    "eval_args1['model_path'] = './Anomaly_Detection_Models/Machine_Learning_Models/som_trained_model_AlcoholdemandlogspiritsconsumptionperheadUK18701938_1529931394207'\n",
    "kwargs1 = {**reader_kwargs1,**eval_args1}\n",
    "res = evaluate(**kwargs1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance or Algorithm Tuning Test:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_input_args = lambda :{\n",
    "    'network_shape':(8,8),\n",
    "    'input_feature_size':None,\n",
    "    'time_constant':None,\n",
    "    'minNumPerBmu':2,\n",
    "    'no_of_neighbours':3,\n",
    "    'init_radius':0.4,\n",
    "    'init_learning_rate':0.01,\n",
    "    'N':100,    \n",
    "    'diff_order':1\n",
    "}\n",
    "\n",
    "training_args = lambda:{\n",
    "            'is_train':True,\n",
    "            'epochs':5,\n",
    "            'batch_size':4,\n",
    "            'to_plot':True,\n",
    "            'test_frac':0.7\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_dir = 'dataset/one_csv/'\n",
    "target_dir = 'dataset/reader_csv_files/'\n",
    "assetno = ['1']\n",
    "difforders = [1,0,0,2,2,1]\n",
    "\n",
    "model_input_args1 = model_input_args()\n",
    "training_args1 = training_args()\n",
    "\n",
    "init_radiuses = [0.01,0.025,0.05,0.1,0.25,0.5,1,1.5]\n",
    "no_neighbors = [2,4,6,8,10,12,14,16,20,25]\n",
    "init_learning_rates = [0.0001,0.001,0.01,0.05,0.1,0.25,0.5]\n",
    "minPerBmus = [1,2,4,6,8,10,12]\n",
    "\n",
    "\n",
    "for i,filename in enumerate(os.listdir(data_dir)):\n",
    "    for rad in init_radiuses:\n",
    "        for no_neighbor in no_neighbors:\n",
    "            for minPerBmu in minPerBmus:\n",
    "                for init_learning_rate in init_learning_rates:\n",
    "                    model_input_args1['init_learning_rate'] = init_learning_rate\n",
    "                    model_input_args1['minNumPerBmu'] = minPerBmu\n",
    "                    model_input_args1['no_of_neighbours'] = no_neighbor\n",
    "                    model_input_args1['init_radius'] = rad\n",
    "                    \n",
    "                    print(\"\\nModel args :Learnrate : {},minbmu:{},neighbors:{},init_Rad:{}\\n\".format(init_learning_rate,\n",
    "                                                                                                    minPerBmu,no_neighbor,rad))\n",
    "                    name,ext = os.path.splitext(filename)\n",
    "                    if ext != '.csv':continue\n",
    "                    infile = os.path.join(data_dir,filename)\n",
    "                    print(\"\\nDetecting anomalies for {}\\n\".format(filename))\n",
    "\n",
    "\n",
    "                    kwargs1 = csv_reader.get_csv_kwargs(infile=infile,filename=filename,target_dir=target_dir,assetno=assetno[0])\n",
    "\n",
    "                #     print('params: {}\\n'.format(param))\n",
    "                    reader_kwargs1 = kwargs1\n",
    "                    model_input_args1 = model_input_args()\n",
    "                    model_input_args1['diff_order'] = 0\n",
    "                    training_args1 = training_args()\n",
    "                    kwargs1 = {**reader_kwargs1,**model_input_args1,**training_args1}\n",
    "                    res = main(**kwargs1,anom_thres=2.5)\n",
    "                    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
