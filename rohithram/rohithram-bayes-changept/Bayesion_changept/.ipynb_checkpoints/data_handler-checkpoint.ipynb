{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handler\n",
    "* This python file handles both reading the data and writing data to database using separate classes for both cases.\n",
    "* Structure is : \n",
    "* Class **Data_reader** which contains methods which are used to fetch data using reader api from opentsdb as well as csv file.Which converts json response into list of dataframes per asset with multiple metric being columns from index $1$ onwards with timestamp in epoch being index of dataframe and assetno being the first column i.e column index $0$.\n",
    "* Class **Postgres_Writer** which is used for mapping the outputs of anomaly detector to corresponding ATL's and bulk writes the queries to local db at one shot.It is instantiated after anomaly detected for all metrics in an asset likewise for all such assets from master python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run data_handler.py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "#importing reader and checker for reading data\n",
    "import reader as reader\n",
    "import checker as checker\n",
    "import datetime as dt\n",
    "# error code is python file which contains dictionary of mapped error codes and messages for different errors\n",
    "import error_codes as error_codes\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run data_handler.py -a\n",
    "\n",
    "\n",
    "    \n",
    "class Postgres_Writer():\n",
    "    \n",
    "    \n",
    "    def __init__(self,anomaly_detectors,db_credentials,sql_query_args,table_name,window_size=10):\n",
    "        \n",
    "        '''\n",
    "        Used for mapping the outputs of anomaly detector to corresponding Asset timeline logging and bulk writes the \n",
    "        queries to local db at one shot.It is instantiated after anomaly detected for all metrics in \n",
    "        an asset likewise for all such assets from master python file.\n",
    "        Arguments need while instantiating this class are :\n",
    "        anomaly_detectors : List of anomaly detector objects (Its object of bayesian changept detector class)\n",
    "        db_credentials : dictionary of credentials for connecting to db\n",
    "        sql_query_args : dictionary of args or values used to form the columns in the table for each row\n",
    "        table_name : string - table name\n",
    "        window_size : (int) no of points either side of around anomaly to write into db\n",
    "        '''\n",
    "        \n",
    "#         super(Postgres_Writer,self).__init__(anomaly_detector)\n",
    "        self.anomaly_detectors = anomaly_detectors\n",
    "        self.db_credentials = db_credentials\n",
    "        self.sql_query_args = sql_query_args\n",
    "        self.table_name = table_name\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        #resets the error codes incase anything were over written\n",
    "        error_codes.reset()\n",
    "        print(\"Postgres writer initialised \\n\")\n",
    "\n",
    "        \n",
    "    def write_to_db(self,col_names,col_vals):\n",
    "        \n",
    "        '''\n",
    "        connects to db and executes the query for bulk insert.\n",
    "        Arguments:\n",
    "        col_names : column names (comma separated)\n",
    "        col_vals  : list of column values for each row\n",
    "        '''\n",
    "\n",
    "    #     print(\"\\n Changepoint info : \\n {}\".format(col_vals))\n",
    "        col_vals1 = [[str(val) if(type(val)!=str) else \"'{}'\".format(val) for val in row] for row in col_vals]\n",
    "        joined_col_vals = [\"({})\".format(','.join(map(str,val))) for val in col_vals1]\n",
    "        fmt_col_vals = (','.join(joined_col_vals))\n",
    "        insert_query = \"\"\" INSERT INTO {} ({}) VALUES{};\"\"\".format(self.table_name,col_names,fmt_col_vals)\n",
    "        \n",
    "        status = 0\n",
    "        conn = None\n",
    "        cur = None\n",
    "        try:\n",
    "            conn = psycopg2.connect(**self.db_credentials)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(insert_query)\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print('\\n Successfully written into database\\n')\n",
    "            return error_codes.error_codes['success']\n",
    "        except psycopg2.DatabaseError as error:\n",
    "            status = 1\n",
    "            print(\"Database error : {}\".format(error))\n",
    "            error_codes.error_codes['db']['message']=str(error)\n",
    "            return error_codes.error_codes['db']\n",
    "        finally:\n",
    "                if cur is not None:\n",
    "                    cur.close()\n",
    "                if conn is not None:\n",
    "                    conn.close()\n",
    "        \n",
    "        \n",
    "    def ts_to_unix(self,t):\n",
    "        return int((t - dt.datetime(1970, 1, 1)).total_seconds()*1000)\n",
    "    \n",
    "    def map_outputs_and_write(self):\n",
    "        \n",
    "        '''\n",
    "        maps the values to corresponding columns of the table\n",
    "        Checks whether the algo is univariate or multivariate and proceeds accordingly\n",
    "        '''\n",
    "        \n",
    "        sql_query_args = self.sql_query_args\n",
    "        col_names_list = list(sql_query_args.keys())\n",
    "        col_names = ','.join(col_names_list)\n",
    "        col_vals = []\n",
    "        table_name = self.table_name\n",
    "        \n",
    "        if(self.anomaly_detectors[0].algo_type=='univariate'):\n",
    "            '''\n",
    "            # looping through the list of anomaly detectors which are instances for different metrics and assets\n",
    "            # They have info like anomaly indexes and data , etc\n",
    "            # appends the mapped column values to list for bulk inserting\n",
    "            '''\n",
    "            for anomaly_detector in self.anomaly_detectors:\n",
    "                \n",
    "                queries = self.make_query_args(anomaly_detector,sql_query_args)\n",
    "                [col_vals.append(list(query.values())) for query in queries]\n",
    "\n",
    "        else:\n",
    "            '''\n",
    "            in construction - will be updated shortly for multivariate case\n",
    "            '''\n",
    "            assetlist = [anomaly_detector.assetno for anomaly_detector in self.anomaly_detectors]\n",
    "            df_assets = pd.DataFrame(np.array(assetlist),columns=['assetno'])\n",
    "            asset_groups = df_assets.groupby('assetno')\n",
    "            \n",
    "            for assetno,metrics_per_asset in asset_groups:\n",
    "#                 print(\"Assetno : {}\\n\".format(assetno))\n",
    "                query_per_metric = []\n",
    "                data_per_asset = {\"asset\":assetno,\"readings\":[]}\n",
    "                event_ctxt_info =  {\"body\":[data_per_asset]}\n",
    "                sql_query_args = self.sql_query_args\n",
    "                for index in list(metrics_per_asset.index):\n",
    "                    \n",
    "                    queries = (self.make_query_args(self.anomaly_detectors[index],sql_query_args))\n",
    "                    for query in queries:\n",
    "                        event_ctxt_info_exact = json.loads(query['event_context_info'])\n",
    "                        sql_query_args.update(query)\n",
    "                        event_ctxt_info['body'][0]['readings'].append(event_ctxt_info_exact['body'][0]['readings'][0])\n",
    "                        sql_query_args['parameter_list'] = '{}'.format(list(self.anomaly_detectors[index].data.columns[1:]))\n",
    "                        sql_query_args['event_context_info'] = json.dumps(event_ctxt_info)\n",
    "\n",
    "                col_vals.append(list(sql_query_args.values()))\n",
    "#                 print('Assetno : {} \\n sql_query: \\n{} \\n'.format(assetno,sql_query_args))\n",
    "        \n",
    "        # if there are nothing to write don't connect to db\n",
    "        if(len(col_vals)!=0):\n",
    "            return self.write_to_db(col_names,col_vals)\n",
    "        else:\n",
    "            print(\"\\nNo anomaly detected to write\\n\")\n",
    "            return error_codes.error_codes['success']\n",
    "        \n",
    "        \n",
    "    def make_query_args(self,anomaly_detector,sql_query_args):\n",
    "        \n",
    "        '''\n",
    "        Function to map the details about an anomaly to columns present in log asset timeline table\n",
    "        Arguments :\n",
    "        Takes in single anomaly detector and loops over the anomalies and also sql query arguments.\n",
    "        Returns sql queries -> list of all mapped sql column values for each anomaly\n",
    "        '''\n",
    "        \n",
    "        sql_queries = []\n",
    "        \n",
    "        #Proceed only if no of anomalies detected are not zero\n",
    "        \n",
    "        if(anomaly_detector.anom_indexes is not None and len(anomaly_detector.anom_indexes)!=0):\n",
    "                anom_indexes = anomaly_detector.anom_indexes\n",
    "                original_data = anomaly_detector.data\n",
    "                col_index = anomaly_detector.data_col_index\n",
    "                metric_name = original_data.columns[anomaly_detector.data_col_index]\n",
    "                assetno = anomaly_detector.assetno\n",
    "                window = self.window_size\n",
    "                sql_query_args['event_name'] = '{}_'.format(original_data.columns[col_index])+anomaly_detector.algo_code+'_anomaly'\n",
    "                sql_query_args['event_source'] = anomaly_detector.algo_name\n",
    "                sql_query_args['operating_unit_serial_number'] = int(assetno)\n",
    "                sql_query_args['parameter_list'] = '[{}]'.format(original_data.columns[anomaly_detector.data_col_index])\n",
    "                for i in anom_indexes:\n",
    "                    event_ctxt_info =  {\"body\":[]}\n",
    "                    data_per_asset = {\"asset\": '',\"readings\":[]}\n",
    "                    data_per_metric = {\"name\":'',\"datapoints\":''}\n",
    "\n",
    "                    time_series = (original_data.index[i-window:i+window])\n",
    "                    sql_query_args['event_timestamp'] =  str(pd.to_datetime(original_data.index[i],unit='ms',utc=True))\n",
    "                    sql_query_args['event_timestamp_epoch'] = str(int(original_data.index[i]))\n",
    "                    sql_query_args['created_date'] = str(pd.to_datetime(dt.datetime.now(),utc=True))\n",
    "                    time_around_anoms = [\"''{}''\".format((t)) for t in time_series]                    \n",
    "                    \n",
    "                    data_per_metric['name']=metric_name\n",
    "                    datapoints = (list(zip(time_around_anoms,list(original_data.iloc[i-window:i+window,col_index].values))))\n",
    "                    data_per_metric['datapoints'] = datapoints\n",
    "                    data_per_asset['asset'] = assetno\n",
    "                    data_per_asset['readings'].append(data_per_metric)\n",
    "                    event_ctxt_info['body'].append(data_per_asset)\n",
    "\n",
    "                    sql_query_args['event_context_info'] = json.dumps(event_ctxt_info)\n",
    "#                     sql_query_args['event_context_info'] = '{}'.format(str(event_ctxt_info))\n",
    "                    \n",
    "                    sql_queries.append(sql_query_args)\n",
    "#                     print(\"event_name: \\n {} \\n event context info: \\n {} \\n\".format(sql_query_args['event_name'],sql_query_args['event_context_info']))\n",
    "\n",
    "        return (sql_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run data_handler.py -a\n",
    "\n",
    "class Data_reader():\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Data_reader is a class which contains methods which are used to fetch data using reader api from opentsdb as \n",
    "    well as csv file.Which converts json response into list of dataframes per asset with multiple \n",
    "    metric being columns from index 1 onwards with timestamp in epoch being index \n",
    "    of dataframe and assetno being the first column i.e column index 0.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,reader_kwargs):\n",
    "        \n",
    "        #takes reader arguments \n",
    "        self.reader_kwargs = reader_kwargs\n",
    "        print(\"Data reader initialised \\n\")\n",
    "        error_codes.reset()\n",
    "\n",
    "    def read(self):\n",
    "        '''\n",
    "        Function to read the data using reader api, and parses the json to list of dataframes per asset\n",
    "        '''\n",
    "        \n",
    "\n",
    "        '''\n",
    "        To do when new reader works with csv file\n",
    "        '''\n",
    "#         try:\n",
    "#             response_json=reader.reader_api(**self.reader_kwargs)\n",
    "#             response_dict = json.loads(response_json)\n",
    "#         except Exception as e:\n",
    "#             error_codes.error_codes['data_missing']['message'] = response_json\n",
    "#             return error_codes.error_codes['data_missing']\n",
    "        \n",
    "    \n",
    "        '''\n",
    "        To read from old reader file\n",
    "        '''\n",
    "        response_dict=reader.reader_api(**self.reader_kwargs)\n",
    "\n",
    "        if(type(response_dict)==str):\n",
    "            error_codes.error_codes['data_missing']['message']=response_dict\n",
    "            return error_codes.error_codes['data_missing']\n",
    "        else:\n",
    "            print(\"Getting the dataset from the reader....\\n\")\n",
    "            entire_data = self.parse_dict_to_dataframe(response_dict)\n",
    "\n",
    "        '''\n",
    "        To do when new reader works\n",
    "        '''\n",
    "#         print(\"Getting the dataset from the reader....\\n\")\n",
    "#         entire_data = self.parse_dict_to_dataframe(response_dict)\n",
    "#         print(response_dict)\n",
    "      \n",
    "        try:\n",
    "            entire_data.index = entire_data['timestamp'].astype(np.int64)\n",
    "            del entire_data['timestamp']\n",
    "        except:\n",
    "            pass\n",
    "        return entire_data\n",
    "    \n",
    "    \n",
    "    def parse_dict_to_dataframe(self,response_dict):\n",
    "        \n",
    "        '''\n",
    "        parses the json response from reader api to list of dataframes per asset and metrics being columns of\n",
    "        each of the dataframe with timestamps being the index and first column is assetno\n",
    "        Arguments: response json\n",
    "        Returns -> List of dataframes\n",
    "        '''\n",
    "        \n",
    "        entire_data_set = []\n",
    "        for data_per_asset in response_dict['body']:\n",
    "            dataframe_per_asset = []\n",
    "            assetno = data_per_asset['assetno']\n",
    "            for data_per_metric in data_per_asset['readings']:\n",
    "                data = pd.DataFrame(data_per_metric['datapoints'],columns=['timestamp',data_per_metric['name']])\n",
    "                # making index of dataframe as timestamp and deleting that column\n",
    "                data.index = data['timestamp']\n",
    "                del data['timestamp']\n",
    "                data['assetno']=assetno\n",
    "                dataframe_per_asset.append(data)\n",
    "            dataframe = pd.concat(dataframe_per_asset,axis=1)\n",
    "            dataframe = dataframe.T.drop_duplicates().T\n",
    "            cols = list(dataframe.columns)\n",
    "            cols.insert(0, cols.pop(cols.index('assetno')))\n",
    "            dataframe = dataframe[cols]\n",
    "#             print('Asset no : {} \\n {} \\n'.format(assetno,dataframe.head()))\n",
    "            entire_data_set.append(dataframe)        \n",
    "\n",
    "        return entire_data_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
