{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessors\n",
    "\n",
    "#### Contains :\n",
    "* converting string or any kind of time objects to pandas datetime timestamp\n",
    "* Converting pandas datetime to epoch timestamps\n",
    "* normalising and standardising\n",
    "* Stationarising the timeseries\n",
    "* Differencing the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run preprocessors.py \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run preprocessors.py -a\n",
    "\n",
    "\n",
    "def to_timestamp(dataframe,date_col_index,time_format='%Y-%m',isweek=False):\n",
    "    '''\n",
    "    Converts any string datetime object to pandas datetime\n",
    "    Gets dataframe and date_column index as required args\n",
    "    timeformat is required for rare timeformats like weekly data\n",
    "    isweek is bool type which is False for non weekly data\n",
    "    '''\n",
    "    if(isweek!=True):\n",
    "            dateparse = lambda dates: pd.to_datetime(dates,infer_datetime_format=True)\n",
    "    else:\n",
    "        dateparse = lambda dates: dt.datetime.strptime(dates+'-0', time_format)\n",
    "    dataframe[date_col_index].apply(dateparse)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run preprocessors.py -a\n",
    "\n",
    "\n",
    "def ts_to_unix(t):\n",
    "    '''\n",
    "    Converts datetime to epoch timestamps\n",
    "    Arguments:\n",
    "    single datetime object\n",
    "    '''\n",
    "    return int((t - dt.datetime(1970, 1, 1)).total_seconds()*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run preprocessors.py -a\n",
    "\n",
    "\n",
    "def normalise_standardise(data):    \n",
    "    # Create a minimum and maximum processor object\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    # Create an object to transform the data to fit minmax processor\n",
    "    data_norm = pd.DataFrame(min_max_scaler.fit_transform(data.values),\n",
    "                             columns=data.columns,index=data.index)\n",
    "    data_standardised = (data_norm - data_norm.mean(axis=0))/(data_norm.std(axis=0))\n",
    "    return data_standardised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run preprocessors.py -a\n",
    "\n",
    "\n",
    "def stationarize(data):\n",
    "    '''\n",
    "    Stationarises the data\n",
    "    '''\n",
    "    s,t = fit_seasons(data)\n",
    "\n",
    "    if(s is not None):\n",
    "        adj_sea = adjust_seasons(data,seasons=s)\n",
    "        res_data = adj_sea-(data-detrend(data))\n",
    "    else:\n",
    "        res_data = detrend(data)\n",
    "        \n",
    "    return res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run preprocessors.py -a\n",
    "\n",
    "\n",
    "def differencing(data,n=1,axis=-1):\n",
    "    '''\n",
    "    Does differencing on the data and order of differentiation as parameter\n",
    "    By default n=1 and axis =-1\n",
    "    '''\n",
    "    return np.diff(data,n=n,axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.zeros((len(data) + 1, len(data) + 1))\n",
    "R[0, 0] = 1\n",
    "\n",
    "predprobs = observation_likelihood.pdf(data)\n",
    "indexes = np.arange(len(data))\n",
    "H = hazard_func(np.array(range(indexes+1)))\n",
    "print(len(H))\n",
    "\n",
    "[R[1:index+2,index+1] for index  in indexes] = [R[0:index+1,index]*predprobs[index]*(1-H)[index] for index in indexes]\n",
    "\n",
    "\n",
    "for t, x in enumerate(data):\n",
    "    # Evaluate the predictive distribution for the new datum under each of\n",
    "    # the parameters.  This is the standard thing from Bayesian inference.\n",
    "    predprobs = observation_likelihood.pdf(x)\n",
    "\n",
    "    # Evaluate the hazard function for this interval\n",
    "    H = hazard_func(np.array(range(t+1)))\n",
    "\n",
    "    # Evaluate the growth probabilities - shift the probabilities down and to\n",
    "    # the right, scaled by the hazard function and the predictive\n",
    "    # probabilities.\n",
    "    R[1:t+2, t+1] = R[0:t+1, t] * predprobs * (1-H)\n",
    "\n",
    "    # Evaluate the probability that there *was* a changepoint and we're\n",
    "    # accumulating the mass back down at r = 0.\n",
    "    R[0, t+1] = np.sum( R[0:t+1, t] * predprobs * H)\n",
    "\n",
    "    # Renormalize the run length probabilities for improved numerical\n",
    "    # stability.\n",
    "    R[:, t+1] = R[:, t+1] / np.sum(R[:, t+1])\n",
    "\n",
    "    # Update the parameter sets for each possible run length.\n",
    "    observation_likelihood.update_theta(x)\n",
    "\n",
    "    maxes[t] = R[:, t].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ce1de4057d60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# single-core code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msqroots_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# single-core code\n",
    "sqroots_1 = [sqrt(i ** 2) for i in range(10)]\n",
    "\n",
    "# parallel code\n",
    "sqroots_2 = Parallel(n_jobs=2)(delayed(sqrt)(i ** 2) for i in range(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
