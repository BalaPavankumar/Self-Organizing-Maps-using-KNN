{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run preprocessors.py \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run preprocessors.py -a\n",
    "\n",
    "\n",
    "def to_timestamp(dataframe,date_col_index,time_format='%Y-%m',isweek=False):\n",
    "    if(isweek!=True):\n",
    "            dateparse = lambda dates: pd.to_datetime(dates,infer_datetime_format=True)\n",
    "    else:\n",
    "        dateparse = lambda dates: dt.datetime.strptime(dates+'-0', time_format)\n",
    "    dataframe[date_col_index].apply(dateparse)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run preprocessors.py -a\n",
    "\n",
    "\n",
    "def ts_to_unix(t):\n",
    "    return int((t - dt.datetime(1970, 1, 1)).total_seconds()*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run preprocessors.py -a\n",
    "\n",
    "\n",
    "def normalise_standardise(data):    \n",
    "    # Create a minimum and maximum processor object\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    # Create an object to transform the data to fit minmax processor\n",
    "    data_norm = pd.DataFrame(min_max_scaler.fit_transform(data.values),\n",
    "                             columns=data.columns,index=data.index)\n",
    "    data_standardised = (data_norm - data_norm.mean(axis=0))/(data_norm.std(axis=0))\n",
    "    return data_standardised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run preprocessors.py -a\n",
    "\n",
    "\n",
    "def split_the_data(data,test_frac=0.1):\n",
    "    train_data = data[0:int(np.ceil((1-test_frac)*data[:,].shape[0])),:]\n",
    "    test_data = data[-int(np.ceil(test_frac*data[:,].shape[0])):]\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run preprocessors.py -a\n",
    "\n",
    "\n",
    "def stationarize(data):\n",
    "    s,t = fit_seasons(data)\n",
    "\n",
    "    if(s is not None):\n",
    "        adj_sea = adjust_seasons(data,seasons=s)\n",
    "        res_data = adj_sea-(data-detrend(data))\n",
    "    else:\n",
    "        res_data = detrend(data)\n",
    "        \n",
    "    return res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run preprocessors.py -a\n",
    "\n",
    "\n",
    "def differencing(data,n=1,axis=-1):\n",
    "    return np.diff(data,n=n,axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R = np.zeros((len(data) + 1, len(data) + 1))\n",
    "# R[0, 0] = 1\n",
    "\n",
    "# predprobs = observation_likelihood.pdf(data)\n",
    "# indexes = np.arange(len(data))\n",
    "# H = hazard_func(np.array(range(indexes+1)))\n",
    "# print(len(H))\n",
    "\n",
    "# [R[1:index+2,index+1] for index  in indexes] = [R[0:index+1,index]*predprobs[index]*(1-H)[index] for index in indexes]\n",
    "\n",
    "\n",
    "# for t, x in enumerate(data):\n",
    "#     # Evaluate the predictive distribution for the new datum under each of\n",
    "#     # the parameters.  This is the standard thing from Bayesian inference.\n",
    "#     predprobs = observation_likelihood.pdf(x)\n",
    "\n",
    "#     # Evaluate the hazard function for this interval\n",
    "#     H = hazard_func(np.array(range(t+1)))\n",
    "\n",
    "#     # Evaluate the growth probabilities - shift the probabilities down and to\n",
    "#     # the right, scaled by the hazard function and the predictive\n",
    "#     # probabilities.\n",
    "#     R[1:t+2, t+1] = R[0:t+1, t] * predprobs * (1-H)\n",
    "\n",
    "#     # Evaluate the probability that there *was* a changepoint and we're\n",
    "#     # accumulating the mass back down at r = 0.\n",
    "#     R[0, t+1] = np.sum( R[0:t+1, t] * predprobs * H)\n",
    "\n",
    "#     # Renormalize the run length probabilities for improved numerical\n",
    "#     # stability.\n",
    "#     R[:, t+1] = R[:, t+1] / np.sum(R[:, t+1])\n",
    "\n",
    "#     # Update the parameter sets for each possible run length.\n",
    "#     observation_likelihood.update_theta(x)\n",
    "\n",
    "#     maxes[t] = R[:, t].argmax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
