{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run data_handler.py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import reader_writer.reader as reader\n",
    "import reader_writer.checker as checker\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run data_handler.py -a\n",
    "\n",
    "\n",
    "class Data_Writer():\n",
    "    def __init__(self,anomaly_detector):\n",
    "        self.anomaly_detector = anomaly_detector\n",
    "               \n",
    "        \n",
    "    def write(self):\n",
    "        raise NotImplementedError    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run data_handler.py -a\n",
    "\n",
    "\n",
    "class Anomaly_Detector():\n",
    "    def __init__(self,algo_name,data,anom_indexes,is_train=False,):\n",
    "        self.algo_name = algo_name\n",
    "        self.istrainable = is_train\n",
    "        self.data = data\n",
    "        self.anom_indexes = anom_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run data_handler.py -a\n",
    "\n",
    "\n",
    "    \n",
    "class Postgres_Writer():\n",
    "    def __init__(self,anomaly_detector,db_credentials,sql_query_args,table_name,window_size=10):\n",
    "        \n",
    "#         super(Postgres_Writer,self).__init__(anomaly_detector)\n",
    "        self.anomaly_detector = anomaly_detector\n",
    "        self.db_credentials = db_credentials\n",
    "        self.sql_query_args = sql_query_args\n",
    "        self.table_name = table_name\n",
    "        self.window_size = window_size\n",
    "        print(\"Postgres writer initialised \\n\")\n",
    "\n",
    "        \n",
    "    def write_to_db(self,col_names,col_vals):\n",
    "\n",
    "    #     print(\"\\n Changepoint info : \\n {}\".format(col_vals))\n",
    "        col_vals1 = [[str(val) if(type(val)!=str) else \"'{}'\".format(val) for val in row] for row in col_vals]\n",
    "        joined_col_vals = [\"({})\".format(','.join(map(str,val))) for val in col_vals1]\n",
    "        fmt_col_vals = (','.join(joined_col_vals))\n",
    "        insert_query = \"\"\" INSERT INTO {} ({}) VALUES{};\"\"\".format(self.table_name,col_names,fmt_col_vals)\n",
    "        \n",
    "        status = 0\n",
    "        try:\n",
    "            conn = psycopg2.connect(**self.db_credentials)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(insert_query)\n",
    "            conn.commit()\n",
    "            print('\\n Successfully written into database\\n')\n",
    "\n",
    "        except psycopg2.DatabaseError as error:\n",
    "            status = 1\n",
    "            print(\"Database error : {}\".format(error))\n",
    "        finally:\n",
    "            if cur is not None:\n",
    "                cur.close()\n",
    "            if conn is not None:\n",
    "                conn.close()\n",
    "\n",
    "        return status\n",
    "        \n",
    "    def ts_to_unix(self,t):\n",
    "        return int((t - dt.datetime(1970, 1, 1)).total_seconds()*1000)\n",
    "    \n",
    "    def map_outputs_and_write(self):\n",
    "         \n",
    "        original_data = self.anomaly_detector.data\n",
    "        sql_query_args = self.sql_query_args\n",
    "        window = self.window_size\n",
    "        col_names_list = list(sql_query_args.keys())\n",
    "        col_names = ','.join(col_names_list)\n",
    "        col_vals = []\n",
    "        table_name = self.table_name\n",
    "        \n",
    "        if(self.anomaly_detector.anom_indexes is not None):\n",
    "            anom_indexes = self.anomaly_detector.anom_indexes\n",
    "\n",
    "            for i in anom_indexes:\n",
    "\n",
    "                sql_query_args['parameter_list'] = '[{}]'.format(original_data.columns[self.anomaly_detector.data_col_index])\n",
    "                sql_query_args['event_name'] = original_data.columns[0]+self.anomaly_detector.algo_code+'_anomaly'\n",
    "                sql_query_args['event_source'] = self.anomaly_detector.algo_name\n",
    "                \n",
    "                if(original_data.index.values.dtype!='int64'):\n",
    "                    time_series = original_data.index[i-window:i+window]\n",
    "                    print(time_series)\n",
    "                    sql_query_args['event_timestamp'] =  str(original_data.index[i])\n",
    "                    sql_query_args['event_timestamp_epoch'] = str(self.ts_to_unix(original_data.index[i]))\n",
    "                else:\n",
    "                    time_series = (pd.to_datetime(original_data.index[i-window:i+window],unit='ms',utc=True))\n",
    "                    sql_query_args['event_timestamp'] =  str(pd.to_datetime(original_data.index[i],unit='ms',utc=True))\n",
    "                    sql_query_args['event_timestamp_epoch'] = str((original_data.index[i]))\n",
    "\n",
    "                time_around_anoms = [\"''{}''\".format((t)) for t in time_series]\n",
    "                data_around_anoms = {'timestamp':time_around_anoms,\n",
    "                                    'value':(list(original_data.iloc[i-window:i+window,0].values))}\n",
    "\n",
    "                pts_around_anoms = ''\n",
    "\n",
    "                for key,val in data_around_anoms.items():\n",
    "                    pts_around_anoms += \"{}:{},\".format(key,val)\n",
    "\n",
    "                sql_query_args['event_context_info'] = \"{}\".format(\"{\"+pts_around_anoms.strip(',')+\"}\")\n",
    "\n",
    "                col_vals.append(list(sql_query_args.values()))\n",
    "\n",
    "            self.write_to_db(col_names,col_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run data_handler.py -a\n",
    "\n",
    "class Data_reader():\n",
    "    \n",
    "    def __init__(self,reader_kwargs):\n",
    "        self.reader_kwargs = reader_kwargs\n",
    "        print(\"Data reader initialised \\n\")\n",
    "        \n",
    "    def read(self):\n",
    "        \n",
    "        response_dict=reader.reader_api(**self.reader_kwargs)\n",
    "        print(response_dict)\n",
    "        print(\"Getting the dataset from the reader....\\n\")\n",
    "        entire_data = self.parse_dict_to_dataframe(response_dict)\n",
    "        print(entire_data.head())\n",
    "\n",
    "        print(entire_data.dtypes)\n",
    "        print(entire_data.shape)\n",
    "    #     entire_data = entire_data[np.isfinite(entire_data[entire_data.columns].values)]\n",
    "        return entire_data\n",
    "    \n",
    "    def parse_dict_to_dataframe(self,response_dict):\n",
    "        entire_data_set = []\n",
    "\n",
    "        for data_per_asset in response_dict['body']:\n",
    "            assetno = data_per_asset['assetno']\n",
    "            for data_per_metric in data_per_asset['readings']:\n",
    "                data = pd.DataFrame(data_per_metric['datapoints'],columns=['timestamp',data_per_metric['name']])\n",
    "                data.index = data['timestamp']\n",
    "                del data['timestamp']\n",
    "                data['assetno']=assetno\n",
    "                entire_data_set.append(data)\n",
    "                print(data)\n",
    "        return pd.concat(entire_data_set,ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
