{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing Self organising maps using KNN anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run as writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# importing the bayesian changepoint main python file to detect changepoints\n",
    "from anomaly_detectors.som_knn_detector import som_knn_wrapper as som_wrapper\n",
    "from anomaly_detectors.utils import reader_helper\n",
    "from anomaly_detectors.utils import csv_prep_for_reader as csv_helper\n",
    "from anomaly_detectors.reader_writer import db_properties as db_properties\n",
    "from anomaly_detectors.reader_writer import writer_configs as writer_configs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile_run bayeschangept_sprint1.py -a\n",
    "'''\n",
    "Arguments for reader module to get data from opentsdb\n",
    "This is included for now just for testing, later the main function will take json as direct input\n",
    "'''\n",
    "\n",
    "assetno = ['TSFAD_A1']\n",
    "con = '52.224.236.31:4242'\n",
    "src_type =  'opentsdb'\n",
    "# param = ['ec2_cpu_utilization_5f5533']\n",
    "# param = ['ec2_cpu_utilization_ac20cd']\n",
    "\n",
    "# from_timestamp = 1392388020\n",
    "# to_timestamp = 1393597320\n",
    "param=['ec2_cpu_utilization_5f5533', 'rds_cpu_utilization_cc0c53']\n",
    "from_timestamp=1392388200\n",
    "to_timestamp=1393597320\n",
    "# from_timestamp =1396448940\n",
    "# to_timestamp = 1397659740"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs= lambda:{\n",
    "            'assetno':['TSFAD_A1'],\n",
    "            'from_timestamp':from_timestamp,\n",
    "            'to_timestamp':to_timestamp,\n",
    "            'con':con,\n",
    "            'para_list':param,\n",
    "            'source_type':src_type,\n",
    "            'table_name':'',\n",
    "            'qry_str':'',\n",
    "            'impute_fill_method':'forward',\n",
    "            'down_sampling_method':None,\n",
    "            'down_sampling_window':None,\n",
    "            'freq':None,\n",
    "            'resample_fill_method':None,\n",
    "            'to_resample':None,\n",
    "            'to_impute':True,\n",
    "}\n",
    "\n",
    "model_input_args = lambda :{\n",
    "    'network_shape':(8,8),\n",
    "    'input_feature_size':None,\n",
    "    'time_constant':None,\n",
    "    'minNumPerBmu':2,\n",
    "    'no_of_neighbours':3,\n",
    "    'init_radius':0.4,\n",
    "    'init_learning_rate':0.01,\n",
    "    'N':100,    \n",
    "    'diff_order':1\n",
    "}\n",
    "\n",
    "training_args = lambda:{\n",
    "            'is_train':True,\n",
    "            'epochs':5,\n",
    "            'batch_size':4,\n",
    "            'to_plot':True,\n",
    "            'test_frac':0.2\n",
    "        }\n",
    "\n",
    "\n",
    "        \n",
    "eval_args = lambda: {\n",
    "    'model_path':'',\n",
    "    'to_plot':True,\n",
    "    'anom_thres':3.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(model_input_args().keys())+list(training_args().keys())+list(eval_args().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['network_shape',\n",
       " 'input_feature_size',\n",
       " 'time_constant',\n",
       " 'minNumPerBmu',\n",
       " 'no_of_neighbours',\n",
       " 'init_radius',\n",
       " 'init_learning_rate',\n",
       " 'N',\n",
       " 'diff_order',\n",
       " 'is_train',\n",
       " 'epochs',\n",
       " 'batch_size',\n",
       " 'to_plot',\n",
       " 'test_frac',\n",
       " 'model_path',\n",
       " 'to_plot',\n",
       " 'anom_thres']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for modes :\n",
    "#### Testing three different modes of the program\n",
    "* First training and for testing three different modes are tested and the output is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kwargs():\n",
    "    return model_input_args(),training_args(),eval_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://52.224.236.31:4242/api/query?start=1392388200&end=1393597320&ms=true&m=max:none:ec2_cpu_utilization_5f5533{AssetNo=TSFAD_A1}\n",
      "http://52.224.236.31:4242/api/query?start=1392388200&end=1393597320&ms=true&m=max:none:rds_cpu_utilization_cc0c53{AssetNo=TSFAD_A1}\n",
      "\n",
      "Testing mode option : detect only\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533  \\\n",
      "timestamp                                             \n",
      "1392388200000  TSFAD_A1                    0.325137   \n",
      "1392388320000  TSFAD_A1                    0.325137   \n",
      "1392388500000  TSFAD_A1                    0.325137   \n",
      "1392388620000  TSFAD_A1                   -0.433697   \n",
      "1392388800000  TSFAD_A1                   -0.433697   \n",
      "\n",
      "               rds_cpu_utilization_cc0c53  \n",
      "timestamp                                  \n",
      "1392388200000                   -0.452781  \n",
      "1392388320000                   -0.452781  \n",
      "1392388500000                   -0.628123  \n",
      "1392388620000                   -0.628123  \n",
      "1392388800000                   -0.504288  \n",
      "\n",
      "Shape of the Entire dataset : torch.Size([8061, 2])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([6449, 2]) and Test dataset :torch.Size([1613, 2])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (6448, 2)\n",
      "\n",
      "Epoch : 0 completed\n",
      "\n",
      "Epoch : 1 completed\n",
      "\n",
      "Epoch : 2 completed\n",
      "\n",
      "Epoch : 3 completed\n",
      "\n",
      "Epoch : 4 completed\n",
      "\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_ec2cpuutilization5f5533_rdscpuutilizationcc0c53_TSFAD_A1_1530882791062 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530882791062\n",
      "\n",
      "{\"code\": \"400\", \"status\": \"Bad Request\", \"message\": \"should be of type <class 'float'>\", \"data\": {\"argument\": \"anom_thres\", \"value\": 3}}\n",
      "\n",
      "Testing mode option : detect and log\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533  \\\n",
      "timestamp                                             \n",
      "1392388200000  TSFAD_A1                    0.325137   \n",
      "1392388320000  TSFAD_A1                    0.325137   \n",
      "1392388500000  TSFAD_A1                    0.325137   \n",
      "1392388620000  TSFAD_A1                   -0.433697   \n",
      "1392388800000  TSFAD_A1                   -0.433697   \n",
      "\n",
      "               rds_cpu_utilization_cc0c53  \n",
      "timestamp                                  \n",
      "1392388200000                   -0.452781  \n",
      "1392388320000                   -0.452781  \n",
      "1392388500000                   -0.628123  \n",
      "1392388620000                   -0.628123  \n",
      "1392388800000                   -0.504288  \n",
      "\n",
      "Shape of the Entire dataset : torch.Size([8061, 2])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([6449, 2]) and Test dataset :torch.Size([1613, 2])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (6448, 2)\n",
      "\n",
      "Epoch : 0 completed\n",
      "\n",
      "Epoch : 1 completed\n",
      "\n",
      "Epoch : 2 completed\n",
      "\n",
      "Epoch : 3 completed\n",
      "\n",
      "Epoch : 4 completed\n",
      "\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_ec2cpuutilization5f5533_rdscpuutilizationcc0c53_TSFAD_A1_1530882791973 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530882791973\n",
      "\n",
      "{\"code\": \"400\", \"status\": \"Bad Request\", \"message\": \"should be of type <class 'float'>\", \"data\": {\"argument\": \"anom_thres\", \"value\": 3}}\n",
      "\n",
      "Testing mode option : log only\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533  \\\n",
      "timestamp                                             \n",
      "1392388200000  TSFAD_A1                    0.325137   \n",
      "1392388320000  TSFAD_A1                    0.325137   \n",
      "1392388500000  TSFAD_A1                    0.325137   \n",
      "1392388620000  TSFAD_A1                   -0.433697   \n",
      "1392388800000  TSFAD_A1                   -0.433697   \n",
      "\n",
      "               rds_cpu_utilization_cc0c53  \n",
      "timestamp                                  \n",
      "1392388200000                   -0.452781  \n",
      "1392388320000                   -0.452781  \n",
      "1392388500000                   -0.628123  \n",
      "1392388620000                   -0.628123  \n",
      "1392388800000                   -0.504288  \n",
      "\n",
      "Shape of the Entire dataset : torch.Size([8061, 2])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([6449, 2]) and Test dataset :torch.Size([1613, 2])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (6448, 2)\n",
      "\n",
      "Epoch : 0 completed\n",
      "\n",
      "Epoch : 1 completed\n",
      "\n",
      "Epoch : 2 completed\n",
      "\n",
      "Epoch : 3 completed\n",
      "\n",
      "Epoch : 4 completed\n",
      "\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_ec2cpuutilization5f5533_rdscpuutilizationcc0c53_TSFAD_A1_1530882792762 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530882792762\n",
      "\n",
      "{\"code\": \"400\", \"status\": \"Bad Request\", \"message\": \"should be of type <class 'float'>\", \"data\": {\"argument\": \"anom_thres\", \"value\": 3}}\n"
     ]
    }
   ],
   "source": [
    "reader_kwargs1= reader_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs1)\n",
    "model_input_args1,training_args1,eval_args1 = get_kwargs()\n",
    "training_args1['to_plot'] = False\n",
    "\n",
    "for i in range(3):\n",
    "    mode = som_wrapper.mode_options[i]\n",
    "    print(\"\\nTesting mode option : {}\\n\".format(mode))\n",
    "    training_args1['to_plot']=False\n",
    "    train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "    eval_args1['anom_thres'] = 3\n",
    "    eval_args1['model_path'] = train_res['models'][0][assetno[0]]\n",
    "#     json_data = reader_helper.read(reader_kwargs1)\n",
    "    eval_args1['to_plot']=False\n",
    "    test_res = som_wrapper.evaluate(**eval_args1,json_data=json_data,mode=som_wrapper.mode_options[i])\n",
    "    print(test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 : \n",
    "#### Testing with parameters being empty quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://52.224.236.31:4242/api/query?start=1392388200&end=1393597320&ms=true&m=max:none:ec2_cpu_utilization_5f5533{AssetNo=TSFAD_A1}\n",
      "http://52.224.236.31:4242/api/query?start=1392388200&end=1393597320&ms=true&m=max:none:rds_cpu_utilization_cc0c53{AssetNo=TSFAD_A1}\n"
     ]
    }
   ],
   "source": [
    "reader_kwargs1= reader_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs1)\n",
    "model_input_args1,training_args1,eval_args1 = get_kwargs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "from anomaly_detectors.utils import data_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "\n",
    "df = json_normalize(data=json.loads(json_data), record_path=['body', 'readings', 'datapoints'], meta=[['body', 'assetno'],\n",
    "   ['body', 'readings', 'name']])\n",
    "df.columns = ['timestamp', 'values', 'assetno', 'parameters']\n",
    "df = pd.pivot_table(df, values='values', index=['assetno','timestamp'], columns=['parameters'], aggfunc=np.mean)\n",
    "data = df.reset_index(drop=False).rename_axis(None,axis=1)\n",
    "data.index = data['timestamp']\n",
    "del data['timestamp']\n",
    "data_per_assets = data.groupby('assetno')\n",
    "entire_data = []\n",
    "for name,group in data_per_assets:\n",
    "    entire_data.append(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "data_reader = data_handler.Data_reader(json_data)\n",
    "dfs = data_reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on model_input_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(model_input_args1.keys()):\n",
    "    print(\"\\nGiving {} parameter : ''\\n\".format(key))\n",
    "    model_input_args1 = model_input_args()\n",
    "    model_input_args1[key] = ''\n",
    "    training_args1['to_plot']=False\n",
    "    train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "    print(train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_args1 = model_input_args()\n",
    "\n",
    "for key in list(training_args().keys()):\n",
    "    training_args1 = training_args()\n",
    "    training_args1[key] = ''\n",
    "#     eval_args1['to_plot']=False\n",
    "    training_args1['to_plot']=False\n",
    "    train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "    print(train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on evaluation args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%timeit \n",
    "\n",
    "training_args1 = training_args()\n",
    "model_input_args1 = model_input_args()\n",
    "training_args1['to_plot']=False\n",
    "train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "print(train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in list(eval_args().keys()):\n",
    "    eval_args1 = eval_args()\n",
    "    eval_args1['model_path'] = train_res['models'][0][assetno[0]]\n",
    "    eval_args1[key] = ''\n",
    "#     json_data = reader_helper.read(reader_kwargs1)\n",
    "    eval_args1['to_plot']=False\n",
    "    test_res = som_wrapper.evaluate(**eval_args1,json_data=json_data)\n",
    "    print(test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 :\n",
    "#### Testing missing parameters : \n",
    "* Since only model path is required arg, it doesn't throw any error when we don't pass other evalution args since they are optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model input args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(model_input_args1.keys()):\n",
    "    print(\"\\nGiving {} parameter : ''\\n\".format(key))\n",
    "    model_input_args1 = model_input_args()\n",
    "    del model_input_args1[key]\n",
    "    training_args1['to_plot']=False\n",
    "    train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "    print(train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(training_args1.keys()):\n",
    "    print(\"\\nGiving {} parameter : ''\\n\".format(key))\n",
    "    model_input_args1 = model_input_args()\n",
    "    del training_args1[key]\n",
    "#     training_args1['to_plot']=False\n",
    "    train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "    print(train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(eval_args().keys()):\n",
    "    eval_args1 = eval_args()\n",
    "    del eval_args1[key]\n",
    "    eval_args1['model_path'] = train_res['models'][0][assetno[0]]\n",
    "\n",
    "#     json_data = reader_helper.read(reader_kwargs1)\n",
    "    eval_args1['to_plot']=False\n",
    "    test_res = som_wrapper.evaluate(**eval_args1,json_data=json_data)\n",
    "    print(test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3:\n",
    "#### Testing parameter type mismatch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mismatched params arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vals = {'batch_size': 4.5,\n",
    "             'epochs': 5.5,\n",
    "             'is_train': 'True',\n",
    "             'test_frac': 3.0,\n",
    "             'to_plot': 'True'}\n",
    "\n",
    "model_vals = {'N': '100',\n",
    "             'diff_order': 1.4,\n",
    "             'init_learning_rate': 1,\n",
    "             'init_radius': 'f',\n",
    "             'input_feature_size': 'ff',\n",
    "             'minNumPerBmu': 2,\n",
    "             'network_shape': ('8', 8),\n",
    "             'no_of_neighbours': 34.4,\n",
    "             'time_constant': 'None'}\n",
    "\n",
    "eval_vals = {\n",
    "            'model_path':34,\n",
    "            'to_plot':'True',\n",
    "            'anom_thres':4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Testing training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,key in enumerate(list(training_args().keys())):\n",
    "    print(\"\\nGiving {} parameter : {}\\n\".format(key,train_vals[key]))\n",
    "    model_input_args1 = model_input_args()\n",
    "    training_args1 = training_args()\n",
    "    training_args1[key] = train_vals[key]\n",
    "    train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "    print(train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Testing model input args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,key in enumerate(list(model_input_args().keys())):\n",
    "    print(\"\\nGiving {} parameter : {}\\n\".format(key,model_vals[key]))\n",
    "    training_args1 = training_args()\n",
    "    model_input_args1 = model_input_args()\n",
    "    model_input_args1[key] = model_vals[key]\n",
    "    train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "    print(train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing eval args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args1 = training_args()\n",
    "model_input_args1 = model_input_args()\n",
    "training_args1['to_plot'] = False\n",
    "\n",
    "train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(eval_args().keys()):\n",
    "    eval_args1 = eval_args()\n",
    "    eval_args1[key] = eval_vals[key]\n",
    "\n",
    "    test_res = som_wrapper.evaluate(**eval_args1,json_data=json_data)\n",
    "    print(test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Test Case:\n",
    "#### Testing the data missing case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = reader_kwargs()\n",
    "reader_kwargs1['from_timestamp'] = int(2**40)\n",
    "\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs1)\n",
    "\n",
    "model_input_args1 = model_input_args()\n",
    "training_args1 = training_args()\n",
    "training_args1['to_plot']=False\n",
    "train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "print(train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4:\n",
    "#### Testing Algorithm Logic :\n",
    "* Running the algorithm for different values of probability thresholds\n",
    "* so expected o/p behaviour expected is as we increase the probability threshold the no of anomaly detected reduces and reaches zero when set to $1.0$\n",
    "* Default threshold is $0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "\n",
    "pthreses = [0.5,0.01,0.99]\n",
    "\n",
    "for i,pthres in enumerate(pthreses):\n",
    "    algo_kwargs1['thres_prob']=pthres\n",
    "    res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the effect of expected_run_length on the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "algo_kwargs1['thres_prob'] = 0.5\n",
    "mean_run_lens = [0,100,10000]\n",
    "for i,mean_run_len in enumerate(mean_run_lens):\n",
    "    print('\\n Anomaly detection for expected run length  = {}\\n'.format(mean_run_len))\n",
    "    algo_kwargs1['expected_run_length']=mean_run_len\n",
    "    res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results:\n",
    "* As we observed that when expected run length given zero. the algo stops and throws an zero division error as expected/\n",
    "* Then as we increase it from zero, we observed that interval between changepoints increased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5:\n",
    "#### Testing Algorithm tuning :\n",
    "* Trying to change the parameters of algorithm and observe the results obtained\n",
    "* Here we try to tune the algo to run faster and as we observe that on increasing expected run length, the sensitivity of detecting changepoints decreases which we saw above case too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "\n",
    "    \n",
    "reader_kwargs1 = csv_helper.get_csv_kwargs(infile='../../dataset/bearings_1.csv',\n",
    "                                           filename='bearings_1.csv',n_rows=5000,has_time=False)\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs1)\n",
    "\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6:\n",
    "#### Testing Asset Timeline Logging :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Asset timeline logging 1'](./atl_test2_bayes.png)\n",
    "!['Asset timeline logging 2](./atl_test3_bayes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7:\n",
    "#### Testing the response from program to follow agreed upon template :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8:\n",
    "#### Testing No Data exception :\n",
    "* To get empty dataframe we set from and to timestamp to be not in range of the timestamps in dataset analysed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "reader_kwargs1['from_timestamp'] = int(2**60)\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 9:\n",
    "#### Testing Database connectivity  exception :\n",
    "* To test this we edit the db properties and run the algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_configs.table_name = 'f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_properties.db_connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We change the db name and we expect a database error as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties.db_connection['dbname'] = 'eg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "algo_kwargs1['to_plot'] = False\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data,mode=bayeschangept.mode_options[2])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we try to change the password of db properties and observe the exception handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties.db_connection['password']='fef'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "algo_kwargs1['to_plot'] = False\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data,mode=bayeschangept.mode_options[2])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run db_properties.py\n",
    "\n",
    "db_connection = {'dbname': 'Cerebra',\n",
    " 'host': '127.0.0.1',\n",
    " 'password': 'givemeachance',\n",
    " 'port': '5432',\n",
    " 'user': 'postgres'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we change the table name in which we are writing, and we observe that relation doesn't exist as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_configs.table_name = 'ffee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "# reader_kwargs1['from_timestamp'] = int(2**60)\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "algo_kwargs1['to_plot'] = False\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data,mode=bayeschangept.mode_options[2])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_configs.table_name = 'public.log_asset_timeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 10:\n",
    "#### Testing random exceptions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "algo_kwargs1['to_plot'] = False\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 11:\n",
    "#### Testing High Performance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reader_kwargs1= csv_helper.get_csv_kwargs(infile='../../dataset/bearings.csv',filename='bearings.csv',has_time=False)\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs1)\n",
    "model_input_args1,training_args1,eval_args1 = get_kwargs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "training_args1['to_plot']=False\n",
    "training_args1['epochs']= 10\n",
    "training_args1['batch_size'] = 32\n",
    "train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "\n",
    "eval_args1['anom_thres'] = 1\n",
    "eval_args1['model_path'] = train_res['models'][0][assetno[0]]\n",
    "eval_args1['to_plot']=False\n",
    "test_res = som_wrapper.evaluate(**eval_args1,json_data=json_data,mode=som_wrapper.mode_options[1])\n",
    "print(test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We observe that for running oddly $15000$ datapoints for $1$ metric name it takes around $58.7$ seconds to run.\n",
    "* I believe that reason for this timing is the algo computes the matrix of order of $NxN$ where $N$ is the size of dataset.\n",
    "* So it takes lot of time to compute that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "* Hence we observe that **Bayesian Changepoint Detection** works well only on level shifts or variational shift datasets over outlier or surge,sag datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
