{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing Self organising maps using KNN anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run as writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "# importing the bayesian changepoint main python file to detect changepoints\n",
    "from anomaly_detectors.som_knn_detector import som_knn_wrapper as som_wrapper\n",
    "from anomaly_detectors.utils import reader_helper\n",
    "from anomaly_detectors.utils import csv_prep_for_reader as csv_helper\n",
    "from anomaly_detectors.reader_writer import db_properties as db_properties\n",
    "from anomaly_detectors.reader_writer import writer_configs as writer_configs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile_run bayeschangept_sprint1.py -a\n",
    "'''\n",
    "Arguments for reader module to get data from opentsdb\n",
    "This is included for now just for testing, later the main function will take json as direct input\n",
    "'''\n",
    "\n",
    "assetno = ['TSFAD_A1']\n",
    "con = '192.168.2.5:4242'\n",
    "src_type =  'opentsdb'\n",
    "param = ['ec2_cpu_utilization_5f5533']\n",
    "# param = ['ec2_cpu_utilization_ac20cd']\n",
    "\n",
    "# from_timestamp = 1392388020\n",
    "# to_timestamp = 1393597320\n",
    "# param=['ec2_cpu_utilization_5f5533', 'rds_cpu_utilization_cc0c53']\n",
    "from_timestamp=1392388200\n",
    "to_timestamp=1393597320\n",
    "# from_timestamp =1396448940\n",
    "# to_timestamp = 1397659740"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs= lambda:{\n",
    "            'assetno':['TSFAD_A1'],\n",
    "            'from_timestamp':from_timestamp,\n",
    "            'to_timestamp':to_timestamp,\n",
    "            'con':con,\n",
    "            'para_list':param,\n",
    "            'source_type':src_type,\n",
    "            'table_name':'',\n",
    "            'qry_str':'',\n",
    "            'impute_fill_method':'forward',\n",
    "            'down_sampling_method':None,\n",
    "            'down_sampling_window':None,\n",
    "            'freq':None,\n",
    "            'resample_fill_method':None,\n",
    "            'to_resample':None,\n",
    "            'to_impute':True,\n",
    "}\n",
    "\n",
    "model_input_args = lambda :{\n",
    "    'network_shape':(8,8),\n",
    "    'input_feature_size':None,\n",
    "    'time_constant':None,\n",
    "    'minNumPerBmu':2,\n",
    "    'no_of_neighbours':3,\n",
    "    'init_radius':0.4,\n",
    "    'init_learning_rate':0.01,\n",
    "    'N':100,    \n",
    "    'diff_order':1\n",
    "}\n",
    "\n",
    "training_args = lambda:{\n",
    "            'is_train':True,\n",
    "            'epochs':5,\n",
    "            'batch_size':4,\n",
    "            'to_plot':True,\n",
    "            'test_frac':0.2\n",
    "        }\n",
    "\n",
    "\n",
    "        \n",
    "eval_args = lambda: {\n",
    "    'model_path':'',\n",
    "    'to_plot':True,\n",
    "    'anom_thres':3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(model_input_args().keys())+list(training_args().keys())+list(eval_args().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['network_shape',\n",
       " 'input_feature_size',\n",
       " 'time_constant',\n",
       " 'minNumPerBmu',\n",
       " 'no_of_neighbours',\n",
       " 'init_radius',\n",
       " 'init_learning_rate',\n",
       " 'N',\n",
       " 'diff_order',\n",
       " 'is_train',\n",
       " 'epochs',\n",
       " 'batch_size',\n",
       " 'to_plot',\n",
       " 'test_frac',\n",
       " 'model_path',\n",
       " 'to_plot',\n",
       " 'anom_thres']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for modes :\n",
    "#### Testing three different modes of the program\n",
    "* First training and for testing three different modes are tested and the output is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kwargs():\n",
    "    return model_input_args(),training_args(),eval_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://192.168.2.5:4242/api/query?start=1392388200&end=1393597320&ms=true&m=max:none:ec2_cpu_utilization_5f5533{AssetNo=TSFAD_A1}\n",
      "\n",
      "Testing mode option : detect only\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533\n",
      "timestamp                                          \n",
      "1392388320000  TSFAD_A1                    0.325390\n",
      "1392388620000  TSFAD_A1                   -0.433345\n",
      "1392388920000  TSFAD_A1                    1.269160\n",
      "1392389220000  TSFAD_A1                    0.838187\n",
      "1392389520000  TSFAD_A1                    0.436504\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([4031, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([3225, 1]) and Test dataset :torch.Size([807, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (3224, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_ec2cpuutilization5f5533_1530286580445 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530286580445\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533\n",
      "timestamp                                          \n",
      "1392388320000  TSFAD_A1                    0.325390\n",
      "1392388620000  TSFAD_A1                   -0.433345\n",
      "1392388920000  TSFAD_A1                    1.269160\n",
      "1392389220000  TSFAD_A1                    0.838187\n",
      "1392389520000  TSFAD_A1                    0.436504\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([4031, 1])\n",
      "\n",
      "Input data's shape: (4031, 1)\n",
      "Differenced data shape (4030, 1)\n",
      "(4030,)\n",
      "No of anomalies detected : 20, Fraction of data detected as anomaly : 0.004961548002976929\n",
      "\n",
      " No of Anomalies detected = 20\n",
      "{\"detect_status\": {\"header\": {\"code\": \"200\", \"status\": \"OK\"}, \"body\": [{\"asset\": \"TSFAD_A1\", \"anomalies\": [{\"name\": \"ec2_cpu_utilization_5f5533\", \"datapoints\": [{\"from_timestamp\": 1392472920000, \"to_timestamp\": 1392472920000, \"anomaly_timestamp\": [1392472920000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392473220000, \"to_timestamp\": 1392473220000, \"anomaly_timestamp\": [1392473220000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392501720000, \"to_timestamp\": 1392501720000, \"anomaly_timestamp\": [1392501720000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392517620000, \"to_timestamp\": 1392517620000, \"anomaly_timestamp\": [1392517620000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392520920000, \"to_timestamp\": 1392520920000, \"anomaly_timestamp\": [1392520920000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392521220000, \"to_timestamp\": 1392521220000, \"anomaly_timestamp\": [1392521220000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392537120000, \"to_timestamp\": 1392537120000, \"anomaly_timestamp\": [1392537120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392558720000, \"to_timestamp\": 1392558720000, \"anomaly_timestamp\": [1392558720000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392605520000, \"to_timestamp\": 1392605520000, \"anomaly_timestamp\": [1392605520000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392658320000, \"to_timestamp\": 1392658320000, \"anomaly_timestamp\": [1392658320000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392660420000, \"to_timestamp\": 1392660420000, \"anomaly_timestamp\": [1392660420000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392663120000, \"to_timestamp\": 1392663120000, \"anomaly_timestamp\": [1392663120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392663420000, \"to_timestamp\": 1392663420000, \"anomaly_timestamp\": [1392663420000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392692520000, \"to_timestamp\": 1392692520000, \"anomaly_timestamp\": [1392692520000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392753120000, \"to_timestamp\": 1392753120000, \"anomaly_timestamp\": [1392753120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392769320000, \"to_timestamp\": 1392769320000, \"anomaly_timestamp\": [1392769320000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392771120000, \"to_timestamp\": 1392771120000, \"anomaly_timestamp\": [1392771120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1393278120000, \"to_timestamp\": 1393278120000, \"anomaly_timestamp\": [1393278120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1393278720000, \"to_timestamp\": 1393278720000, \"anomaly_timestamp\": [1393278720000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1393279020000, \"to_timestamp\": 1393279020000, \"anomaly_timestamp\": [1393279020000], \"anomaly_code\": \"som\"}]}]}]}}\n",
      "\n",
      "Testing mode option : detect and log\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533\n",
      "timestamp                                          \n",
      "1392388320000  TSFAD_A1                    0.325390\n",
      "1392388620000  TSFAD_A1                   -0.433345\n",
      "1392388920000  TSFAD_A1                    1.269160\n",
      "1392389220000  TSFAD_A1                    0.838187\n",
      "1392389520000  TSFAD_A1                    0.436504\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([4031, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([3225, 1]) and Test dataset :torch.Size([807, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (3224, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_ec2cpuutilization5f5533_1530286581762 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530286581762\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533\n",
      "timestamp                                          \n",
      "1392388320000  TSFAD_A1                    0.325390\n",
      "1392388620000  TSFAD_A1                   -0.433345\n",
      "1392388920000  TSFAD_A1                    1.269160\n",
      "1392389220000  TSFAD_A1                    0.838187\n",
      "1392389520000  TSFAD_A1                    0.436504\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([4031, 1])\n",
      "\n",
      "Input data's shape: (4031, 1)\n",
      "Differenced data shape (4030, 1)\n",
      "(4030,)\n",
      "No of anomalies detected : 23, Fraction of data detected as anomaly : 0.005705780203423468\n",
      "\n",
      " No of Anomalies detected = 23\n",
      "Postgres writer initialised \n",
      "\n",
      "\n",
      "Multivariate writer initialised\n",
      "\n",
      " Successfully written into database\n",
      "\n",
      "{\"detect_status\": {\"header\": {\"code\": \"200\", \"status\": \"OK\"}, \"body\": [{\"asset\": \"TSFAD_A1\", \"anomalies\": [{\"name\": \"ec2_cpu_utilization_5f5533\", \"datapoints\": [{\"from_timestamp\": 1392437220000, \"to_timestamp\": 1392437220000, \"anomaly_timestamp\": [1392437220000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392472920000, \"to_timestamp\": 1392472920000, \"anomaly_timestamp\": [1392472920000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392473220000, \"to_timestamp\": 1392473220000, \"anomaly_timestamp\": [1392473220000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392501720000, \"to_timestamp\": 1392501720000, \"anomaly_timestamp\": [1392501720000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392517620000, \"to_timestamp\": 1392517620000, \"anomaly_timestamp\": [1392517620000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392520920000, \"to_timestamp\": 1392520920000, \"anomaly_timestamp\": [1392520920000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392521220000, \"to_timestamp\": 1392521220000, \"anomaly_timestamp\": [1392521220000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392537120000, \"to_timestamp\": 1392537120000, \"anomaly_timestamp\": [1392537120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392558720000, \"to_timestamp\": 1392558720000, \"anomaly_timestamp\": [1392558720000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392605520000, \"to_timestamp\": 1392605520000, \"anomaly_timestamp\": [1392605520000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392634320000, \"to_timestamp\": 1392634320000, \"anomaly_timestamp\": [1392634320000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392658320000, \"to_timestamp\": 1392658320000, \"anomaly_timestamp\": [1392658320000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392660420000, \"to_timestamp\": 1392660420000, \"anomaly_timestamp\": [1392660420000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392663120000, \"to_timestamp\": 1392663120000, \"anomaly_timestamp\": [1392663120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392663420000, \"to_timestamp\": 1392663420000, \"anomaly_timestamp\": [1392663420000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392692520000, \"to_timestamp\": 1392692520000, \"anomaly_timestamp\": [1392692520000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392753120000, \"to_timestamp\": 1392753120000, \"anomaly_timestamp\": [1392753120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392753420000, \"to_timestamp\": 1392753420000, \"anomaly_timestamp\": [1392753420000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392769320000, \"to_timestamp\": 1392769320000, \"anomaly_timestamp\": [1392769320000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392771120000, \"to_timestamp\": 1392771120000, \"anomaly_timestamp\": [1392771120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1393278120000, \"to_timestamp\": 1393278120000, \"anomaly_timestamp\": [1393278120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1393278720000, \"to_timestamp\": 1393278720000, \"anomaly_timestamp\": [1393278720000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1393279020000, \"to_timestamp\": 1393279020000, \"anomaly_timestamp\": [1393279020000], \"anomaly_code\": \"som\"}]}]}]}, \"log_status\": {\"code\": \"200\", \"status\": \"OK\"}}\n",
      "\n",
      "Testing mode option : log only\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533\n",
      "timestamp                                          \n",
      "1392388320000  TSFAD_A1                    0.325390\n",
      "1392388620000  TSFAD_A1                   -0.433345\n",
      "1392388920000  TSFAD_A1                    1.269160\n",
      "1392389220000  TSFAD_A1                    0.838187\n",
      "1392389520000  TSFAD_A1                    0.436504\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([4031, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([3225, 1]) and Test dataset :torch.Size([807, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (3224, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_ec2cpuutilization5f5533_1530286583245 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530286583245\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533\n",
      "timestamp                                          \n",
      "1392388320000  TSFAD_A1                    0.325390\n",
      "1392388620000  TSFAD_A1                   -0.433345\n",
      "1392388920000  TSFAD_A1                    1.269160\n",
      "1392389220000  TSFAD_A1                    0.838187\n",
      "1392389520000  TSFAD_A1                    0.436504\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([4031, 1])\n",
      "\n",
      "Input data's shape: (4031, 1)\n",
      "Differenced data shape (4030, 1)\n",
      "(4030,)\n",
      "No of anomalies detected : 22, Fraction of data detected as anomaly : 0.005457702803274621\n",
      "\n",
      " No of Anomalies detected = 22\n",
      "Postgres writer initialised \n",
      "\n",
      "\n",
      "Multivariate writer initialised\n",
      "\n",
      " Successfully written into database\n",
      "\n",
      "{\"log_status\": {\"code\": \"200\", \"status\": \"OK\"}}\n"
     ]
    }
   ],
   "source": [
    "reader_kwargs1= reader_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs1)\n",
    "model_input_args1,training_args1,eval_args1 = get_kwargs()\n",
    "training_args1['to_plot'] = True\n",
    "\n",
    "for i in range(3):\n",
    "    mode = som_wrapper.mode_options[i]\n",
    "    print(\"\\nTesting mode option : {}\\n\".format(mode))\n",
    "    training_args1['to_plot']=False\n",
    "    train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "    eval_args1['anom_thres'] = 3\n",
    "    eval_args1['model_path'] = train_res['models'][0][assetno[0]]\n",
    "#     json_data = reader_helper.read(reader_kwargs1)\n",
    "    eval_args1['to_plot']=False\n",
    "    test_res = som_wrapper.evaluate(**eval_args1,json_data=json_data,mode=som_wrapper.mode_options[i])\n",
    "    print(test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 : \n",
    "#### Testing with parameters being empty quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://192.168.2.5:4242/api/query?start=1392388200&end=1393597320&ms=true&m=max:none:ec2_cpu_utilization_5f5533{AssetNo=TSFAD_A1}\n"
     ]
    }
   ],
   "source": [
    "reader_kwargs1= reader_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs1)\n",
    "model_input_args1,training_args1,eval_args1 = get_kwargs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on model_input_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Giving network_shape parameter : ''\n",
      "\n",
      "{'code': '400', 'status': 'Bad Request', 'message': \"should be of type <class 'tuple'>\", 'data': {'argument': 'som_shape', 'value': ''}}\n",
      "\n",
      "Giving input_feature_size parameter : ''\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533\n",
      "timestamp                                          \n",
      "1392388320000  TSFAD_A1                    0.325390\n",
      "1392388620000  TSFAD_A1                   -0.433345\n",
      "1392388920000  TSFAD_A1                    1.269160\n",
      "1392389220000  TSFAD_A1                    0.838187\n",
      "1392389520000  TSFAD_A1                    0.436504\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([4031, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([3225, 1]) and Test dataset :torch.Size([807, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (3224, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_ec2cpuutilization5f5533_1530286960137 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530286960137\n",
      "\n",
      "{'header': {'code': '200', 'status': 'OK'}, 'models': [{'TSFAD_A1': '../../Anomaly_Detection_Models/Machine_Learning_Models\\\\som_trained_model_ec2cpuutilization5f5533_1530286960137'}]}\n",
      "\n",
      "Giving time_constant parameter : ''\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533\n",
      "timestamp                                          \n",
      "1392388320000  TSFAD_A1                    0.325390\n",
      "1392388620000  TSFAD_A1                   -0.433345\n",
      "1392388920000  TSFAD_A1                    1.269160\n",
      "1392389220000  TSFAD_A1                    0.838187\n",
      "1392389520000  TSFAD_A1                    0.436504\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([4031, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([3225, 1]) and Test dataset :torch.Size([807, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (3224, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_ec2cpuutilization5f5533_1530286960936 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530286960936\n",
      "\n",
      "{'header': {'code': '200', 'status': 'OK'}, 'models': [{'TSFAD_A1': '../../Anomaly_Detection_Models/Machine_Learning_Models\\\\som_trained_model_ec2cpuutilization5f5533_1530286960936'}]}\n",
      "\n",
      "Giving minNumPerBmu parameter : ''\n",
      "\n",
      "{'code': '400', 'status': 'Bad Request', 'message': \"should be of type <class 'int'>\", 'data': {'argument': 'minNumPerBmu', 'value': ''}}\n",
      "\n",
      "Giving no_of_neighbours parameter : ''\n",
      "\n",
      "{'code': '400', 'status': 'Bad Request', 'message': \"should be of type <class 'int'>\", 'data': {'argument': 'no_of_neighbors', 'value': ''}}\n",
      "\n",
      "Giving init_radius parameter : ''\n",
      "\n",
      "{'code': '400', 'status': 'Bad Request', 'message': \"should be of type <class 'float'>\", 'data': {'argument': 'initial_radius', 'value': ''}}\n",
      "\n",
      "Giving init_learning_rate parameter : ''\n",
      "\n",
      "{'code': '400', 'status': 'Bad Request', 'message': \"should be of type <class 'float'>\", 'data': {'argument': 'initial_learning_rate', 'value': ''}}\n",
      "\n",
      "Giving N parameter : ''\n",
      "\n",
      "{'code': '400', 'status': 'Bad Request', 'message': \"should be of type <class 'int'>\", 'data': {'argument': 'N', 'value': ''}}\n",
      "\n",
      "Giving diff_order parameter : ''\n",
      "\n",
      "{'code': '400', 'status': 'Bad Request', 'message': \"should be of type <class 'int'>\", 'data': {'argument': 'diff_order', 'value': ''}}\n"
     ]
    }
   ],
   "source": [
    "for key in list(model_input_args1.keys()):\n",
    "    print(\"\\nGiving {} parameter : ''\\n\".format(key))\n",
    "    model_input_args1 = model_input_args()\n",
    "    model_input_args1[key] = ''\n",
    "    training_args1['to_plot']=False\n",
    "    train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "    print(train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533\n",
      "timestamp                                          \n",
      "1392388320000  TSFAD_A1                    0.325390\n",
      "1392388620000  TSFAD_A1                   -0.433345\n",
      "1392388920000  TSFAD_A1                    1.269160\n",
      "1392389220000  TSFAD_A1                    0.838187\n",
      "1392389520000  TSFAD_A1                    0.436504\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([4031, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([3225, 1]) and Test dataset :torch.Size([807, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (3224, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_ec2cpuutilization5f5533_1530287077068 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530287077068\n",
      "\n",
      "{'header': {'code': '200', 'status': 'OK'}, 'models': [{'TSFAD_A1': '../../Anomaly_Detection_Models/Machine_Learning_Models\\\\som_trained_model_ec2cpuutilization5f5533_1530287077068'}]}\n",
      "{'code': '400', 'status': 'Bad Request', 'message': \"should be of type <class 'int'>\", 'data': {'argument': 'epochs', 'value': ''}}\n",
      "{'code': '400', 'status': 'Bad Request', 'message': \"should be of type <class 'int'>\", 'data': {'argument': 'batch_size', 'value': ''}}\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533\n",
      "timestamp                                          \n",
      "1392388320000  TSFAD_A1                    0.325390\n",
      "1392388620000  TSFAD_A1                   -0.433345\n",
      "1392388920000  TSFAD_A1                    1.269160\n",
      "1392389220000  TSFAD_A1                    0.838187\n",
      "1392389520000  TSFAD_A1                    0.436504\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([4031, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([3225, 1]) and Test dataset :torch.Size([807, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (3224, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_ec2cpuutilization5f5533_1530287077881 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530287077881\n",
      "\n",
      "{'header': {'code': '200', 'status': 'OK'}, 'models': [{'TSFAD_A1': '../../Anomaly_Detection_Models/Machine_Learning_Models\\\\som_trained_model_ec2cpuutilization5f5533_1530287077881'}]}\n",
      "{'code': '400', 'status': 'Bad Request', 'message': \"should be of type <class 'float'>\", 'data': {'argument': 'test_frac', 'value': ''}}\n"
     ]
    }
   ],
   "source": [
    "model_input_args1 = model_input_args()\n",
    "\n",
    "for key in list(training_args().keys()):\n",
    "    training_args1 = training_args()\n",
    "    training_args1[key] = ''\n",
    "#     eval_args1['to_plot']=False\n",
    "    training_args1['to_plot']=False\n",
    "    train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "    print(train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on evaluation args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533\n",
      "timestamp                                          \n",
      "1392388320000  TSFAD_A1                    0.325390\n",
      "1392388620000  TSFAD_A1                   -0.433345\n",
      "1392388920000  TSFAD_A1                    1.269160\n",
      "1392389220000  TSFAD_A1                    0.838187\n",
      "1392389520000  TSFAD_A1                    0.436504\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([4031, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([3225, 1]) and Test dataset :torch.Size([807, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (3224, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_ec2cpuutilization5f5533_1530287302249 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530287302249\n",
      "\n",
      "{'header': {'code': '200', 'status': 'OK'}, 'models': [{'TSFAD_A1': '../../Anomaly_Detection_Models/Machine_Learning_Models\\\\som_trained_model_ec2cpuutilization5f5533_1530287302249'}]}\n"
     ]
    }
   ],
   "source": [
    "training_args1 = training_args()\n",
    "model_input_args1 = model_input_args()\n",
    "training_args1['to_plot']=False\n",
    "train_res = json.loads(som_wrapper.train(**{**model_input_args1,**training_args1},json_data=json_data))\n",
    "print(train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533\n",
      "timestamp                                          \n",
      "1392388320000  TSFAD_A1                    0.325390\n",
      "1392388620000  TSFAD_A1                   -0.433345\n",
      "1392388920000  TSFAD_A1                    1.269160\n",
      "1392389220000  TSFAD_A1                    0.838187\n",
      "1392389520000  TSFAD_A1                    0.436504\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([4031, 1])\n",
      "\n",
      "{\"code\": \"500\", \"status\": \"Unknown Exception\", \"message\": \"[Errno 2] No such file or directory: ''\"}\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\tr1109\\intrototimeseries\\tsfad\\rohithram\\anomaly_detectors\\som_knn_detector\\som_knn_wrapper.py\", line 307, in evaluate\n",
      "    anom_indexes = anomaly_detector.detect_anomalies()\n",
      "  File \"c:\\users\\tr1109\\intrototimeseries\\tsfad\\rohithram\\anomaly_detectors\\som_knn_detector\\som_knn_detector.py\", line 138, in detect_anomalies\n",
      "    eval_net = load_model(model_path)\n",
      "  File \"c:\\users\\tr1109\\intrototimeseries\\tsfad\\rohithram\\anomaly_detectors\\som_knn_detector\\som_knn_detector.py\", line 64, in load_model\n",
      "    filehandler = open(filepath, 'rb')\n",
      "FileNotFoundError: [Errno 2] No such file or directory: ''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  ec2_cpu_utilization_5f5533\n",
      "timestamp                                          \n",
      "1392388320000  TSFAD_A1                    0.325390\n",
      "1392388620000  TSFAD_A1                   -0.433345\n",
      "1392388920000  TSFAD_A1                    1.269160\n",
      "1392389220000  TSFAD_A1                    0.838187\n",
      "1392389520000  TSFAD_A1                    0.436504\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([4031, 1])\n",
      "\n",
      "Input data's shape: (4031, 1)\n",
      "Differenced data shape (4030, 1)\n",
      "(4030,)\n",
      "No of anomalies detected : 20, Fraction of data detected as anomaly : 0.004961548002976929\n",
      "\n",
      " No of Anomalies detected = 20\n",
      "{\"detect_status\": {\"header\": {\"code\": \"200\", \"status\": \"OK\"}, \"body\": [{\"asset\": \"TSFAD_A1\", \"anomalies\": [{\"name\": \"ec2_cpu_utilization_5f5533\", \"datapoints\": [{\"from_timestamp\": 1392472920000, \"to_timestamp\": 1392472920000, \"anomaly_timestamp\": [1392472920000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392473220000, \"to_timestamp\": 1392473220000, \"anomaly_timestamp\": [1392473220000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392501720000, \"to_timestamp\": 1392501720000, \"anomaly_timestamp\": [1392501720000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392517620000, \"to_timestamp\": 1392517620000, \"anomaly_timestamp\": [1392517620000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392520920000, \"to_timestamp\": 1392520920000, \"anomaly_timestamp\": [1392520920000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392521220000, \"to_timestamp\": 1392521220000, \"anomaly_timestamp\": [1392521220000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392537120000, \"to_timestamp\": 1392537120000, \"anomaly_timestamp\": [1392537120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392558720000, \"to_timestamp\": 1392558720000, \"anomaly_timestamp\": [1392558720000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392605520000, \"to_timestamp\": 1392605520000, \"anomaly_timestamp\": [1392605520000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392658320000, \"to_timestamp\": 1392658320000, \"anomaly_timestamp\": [1392658320000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392660420000, \"to_timestamp\": 1392660420000, \"anomaly_timestamp\": [1392660420000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392663120000, \"to_timestamp\": 1392663120000, \"anomaly_timestamp\": [1392663120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392663420000, \"to_timestamp\": 1392663420000, \"anomaly_timestamp\": [1392663420000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392692520000, \"to_timestamp\": 1392692520000, \"anomaly_timestamp\": [1392692520000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392753120000, \"to_timestamp\": 1392753120000, \"anomaly_timestamp\": [1392753120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392769320000, \"to_timestamp\": 1392769320000, \"anomaly_timestamp\": [1392769320000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1392771120000, \"to_timestamp\": 1392771120000, \"anomaly_timestamp\": [1392771120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1393278120000, \"to_timestamp\": 1393278120000, \"anomaly_timestamp\": [1393278120000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1393278720000, \"to_timestamp\": 1393278720000, \"anomaly_timestamp\": [1393278720000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": 1393279020000, \"to_timestamp\": 1393279020000, \"anomaly_timestamp\": [1393279020000], \"anomaly_code\": \"som\"}]}]}]}}\n",
      "{\"code\": \"400\", \"status\": \"Bad Request\", \"message\": \"should be of type <class 'int'>\", \"data\": {\"argument\": \"anom_thres\", \"value\": \"\"}}\n"
     ]
    }
   ],
   "source": [
    "for key in list(eval_args().keys()):\n",
    "    eval_args1 = eval_args()\n",
    "    eval_args1['model_path'] = train_res['models'][0][assetno[0]]\n",
    "    eval_args1[key] = ''\n",
    "#     json_data = reader_helper.read(reader_kwargs1)\n",
    "    eval_args1['to_plot']=False\n",
    "    test_res = som_wrapper.evaluate(**eval_args1,json_data=json_data)\n",
    "    print(test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 :\n",
    "#### Testing missing parameters : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs())\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "for key in keys:\n",
    "    del algo_kwargs1[key]\n",
    "    res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3:\n",
    "#### Testing parameter type mismatch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs())\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "val = ['2',4.5,'def']\n",
    "for i,key in enumerate(keys):\n",
    "    algo_kwargs1[key]=val[i]\n",
    "    res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Test Case:\n",
    "#### Testing the fact that probability threshold must be between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs())\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "val = [5,4,100]\n",
    "for i,key in enumerate(keys):\n",
    "    algo_kwargs1[key]=val[i]\n",
    "    res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4:\n",
    "#### Testing Algorithm Logic :\n",
    "* Running the algorithm for different values of probability thresholds\n",
    "* so expected o/p behaviour expected is as we increase the probability threshold the no of anomaly detected reduces and reaches zero when set to $1.0$\n",
    "* Default threshold is $0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "\n",
    "pthreses = [0.5,0.01,0.99]\n",
    "\n",
    "for i,pthres in enumerate(pthreses):\n",
    "    algo_kwargs1['thres_prob']=pthres\n",
    "    res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observe the effect of expected_run_length on the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "algo_kwargs1['thres_prob'] = 0.5\n",
    "mean_run_lens = [0,100,10000]\n",
    "for i,mean_run_len in enumerate(mean_run_lens):\n",
    "    print('\\n Anomaly detection for expected run length  = {}\\n'.format(mean_run_len))\n",
    "    algo_kwargs1['expected_run_length']=mean_run_len\n",
    "    res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results:\n",
    "* As we observed that when expected run length given zero. the algo stops and throws an zero division error as expected/\n",
    "* Then as we increase it from zero, we observed that interval between changepoints increased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5:\n",
    "#### Testing Algorithm tuning :\n",
    "* Trying to change the parameters of algorithm and observe the results obtained\n",
    "* Here we try to tune the algo to run faster and as we observe that on increasing expected run length, the sensitivity of detecting changepoints decreases which we saw above case too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "\n",
    "    \n",
    "reader_kwargs1 = csv_helper.get_csv_kwargs(infile='../../dataset/bearings_1.csv',\n",
    "                                           filename='bearings_1.csv',n_rows=5000,has_time=False)\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs1)\n",
    "\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6:\n",
    "#### Testing Asset Timeline Logging :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Asset timeline logging 1'](./atl_test2_bayes.png)\n",
    "!['Asset timeline logging 2](./atl_test3_bayes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7:\n",
    "#### Testing the response from program to follow agreed upon template :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8:\n",
    "#### Testing No Data exception :\n",
    "* To get empty dataframe we set from and to timestamp to be not in range of the timestamps in dataset analysed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "reader_kwargs1['from_timestamp'] = int(2**60)\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 9:\n",
    "#### Testing Database connectivity  exception :\n",
    "* To test this we edit the db properties and run the algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_configs.table_name = 'f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_properties.db_connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We change the db name and we expect a database error as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties.db_connection['dbname'] = 'eg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "algo_kwargs1['to_plot'] = False\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data,mode=bayeschangept.mode_options[2])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we try to change the password of db properties and observe the exception handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties.db_connection['password']='fef'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "algo_kwargs1['to_plot'] = False\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data,mode=bayeschangept.mode_options[2])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run db_properties.py\n",
    "\n",
    "db_connection = {'dbname': 'Cerebra',\n",
    " 'host': '127.0.0.1',\n",
    " 'password': 'givemeachance',\n",
    " 'port': '5432',\n",
    " 'user': 'postgres'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we change the table name in which we are writing, and we observe that relation doesn't exist as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_configs.table_name = 'ffee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "# reader_kwargs1['from_timestamp'] = int(2**60)\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "algo_kwargs1['to_plot'] = False\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data,mode=bayeschangept.mode_options[2])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_configs.table_name = 'public.log_asset_timeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 10:\n",
    "#### Testing random exceptions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs1 = csv_helper.get_csv_kwargs()\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "algo_kwargs1['to_plot'] = False\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 11:\n",
    "#### Testing High Performance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "reader_kwargs1 = csv_helper.get_csv_kwargs(infile='../../dataset/bearings_1.csv',filename='bearings_1.csv',n_rows=15000,has_time=False)\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "algo_kwargs1['to_plot']=False\n",
    "res = bayeschangept.main(**algo_kwargs1,json_data=json_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We observe that for running oddly $15000$ datapoints for $1$ metric name it takes around $58.7$ seconds to run.\n",
    "* I believe that reason for this timing is the algo computes the matrix of order of $NxN$ where $N$ is the size of dataset.\n",
    "* So it takes lot of time to compute that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "* Hence we observe that **Bayesian Changepoint Detection** works well only on level shifts or variational shift datasets over outlier or surge,sag datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
