{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../../anomaly_detectors/som_knn_detector/som_knn_detector.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import scipy as sp\n",
    "\n",
    "#torch libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import pickle \n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#importing dependencies\n",
    "from anomaly_detectors.utils.error_codes import error_codes\n",
    "from anomaly_detectors.som_knn_detector import som_knn_module\n",
    "from anomaly_detectors.utils.preprocessors import *\n",
    "\n",
    "import traceback\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 9\n",
    "rcParams[ 'axes.grid']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model,metric_names,assetno,filename='som_trained_model',\n",
    "               target_dir=\"../../Anomaly_Detection_Models/Machine_Learning_Models\"):\n",
    "    '''\n",
    "    Function to save the model in the given relative path using pickle\n",
    "    Arguments:\n",
    "    Required Params:\n",
    "        model : MODEL's class object which contains all model related info like weights and architecture\n",
    "        metric_names :  list of metric names to form the filename for saving the model\n",
    "        filename : Default -> 'som_trained_model' (It's main part of the filename)\n",
    "        target_dir: Give relative path to the target directory\n",
    "    '''\n",
    "    \n",
    "    error_codes1  = error_codes()\n",
    "    try:\n",
    "        time_now = ts_to_unix(pd.to_datetime(dt.datetime.now()))\n",
    "        metric_names = [''.join(e for e in metric if e.isalnum()) for metric in metric_names]\n",
    "        \n",
    "        # Creating the filename with metricnames and assetno and current time\n",
    "        filename = filename+'_{}_{}'.format('_'.join(metric_names),str(assetno),str(time_now))\n",
    "        \n",
    "        filepath = os.path.join(target_dir,filename)\n",
    "        \n",
    "        if(len(filepath)>100):\n",
    "            filepath = filepath[:100]\n",
    "\n",
    "        filehandler = open(filepath, 'wb')\n",
    "        pickle.dump(model, filehandler)\n",
    "        print(\"\\nSaved model : {} in {},\\nLast Checkpointed at: {}\\n\".format(filename,target_dir,time_now))\n",
    "        return filepath\n",
    "    \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(\"Error occured while saving model\\n\")\n",
    "        error_codes1['unknown']['message']=e\n",
    "        return error_codes1['unknown']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "def load_model(filepath):\n",
    "    '''\n",
    "    Load the model from the given relative filepath\n",
    "    '''\n",
    "    filehandler = open(filepath, 'rb')\n",
    "    return pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "class Som_Detector():\n",
    "    def __init__(self,data,model_input_args,training_args,eval_args):\n",
    "        \n",
    "        '''\n",
    "        Class which is used to find Changepoints in the dataset with given algorithm parameters.\n",
    "        It has all methods related to finding anomalies to plotting those anomalies and returns the\n",
    "        data being analysed and anomaly indexes.\n",
    "        Arguments :\n",
    "        data -> dataframe which has one or two more metric columnwise per asset\n",
    "        model_input_args: dictionary of model related arguments \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        self.algo_name = 'Self Organizing Map AD'\n",
    "        self.algo_code = 'som'\n",
    "        self.algo_type = 'multivariate'\n",
    "        \n",
    "        #training args is set to None incase of evaluation mode\n",
    "        if(training_args is not None):\n",
    "            self.istrain = training_args['is_train']\n",
    "        else:\n",
    "            self.istrain = False\n",
    "            \n",
    "        self.data = data\n",
    "        self.metric_name = list(data[data.columns[1:]])\n",
    "        self.assetno = list(pd.unique(data['assetno']))[0]\n",
    "        self.anom_indexes = None\n",
    "        self.data_col_index = None\n",
    "        self.model_input_args = model_input_args\n",
    "        self.training_args = training_args\n",
    "        self.eval_args = eval_args\n",
    "\n",
    "    def detect_anomalies(self):\n",
    "        \n",
    "        '''\n",
    "        Detects anomalies and returns data and anomaly indexes\n",
    "        '''\n",
    "        \n",
    "        data = torch.from_numpy(self.data[self.data.columns[1:]].values)\n",
    "        \n",
    "        \n",
    "        print(\"Shape of the Entire dataset : {}\\n\".format(data.shape))\n",
    "\n",
    "        if(self.istrain):\n",
    "            data_set,train_data,test_data = process_data(data=data,test_frac=self.training_args['test_frac'],\n",
    "                                                        to_plot=self.training_args['to_plot'])\n",
    "            entire_data = data_set[:,].numpy()\n",
    "\n",
    "            diff_order = self.model_input_args['diff_order']\n",
    "            net = create_cum_train_som(train_data,self.model_input_args,self.training_args)\n",
    "            model_path = save_model(net,metric_names = self.metric_name,assetno=self.assetno)\n",
    "            \n",
    "            return model_path\n",
    "        else:\n",
    "            model_path = self.eval_args['model_path']\n",
    "            anom_thres = self.eval_args['anom_thres']\n",
    "            eval_net = load_model(model_path)\n",
    "            anom_indexes = test(eval_net,data.numpy(),anom_thres=anom_thres,to_plot=self.eval_args['to_plot'])\n",
    "            self.anom_indexes = anom_indexes\n",
    "\n",
    "            print(\"\\n No of Anomalies detected = %g\"%(len(anom_indexes)))\n",
    "            return anom_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "\n",
    "class TimeSeries_Dataset(Dataset):\n",
    "    \"\"\"Time series dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,data,data_col_index=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: input data after all preprocessing done\n",
    "            Trains the model on this Tensor object data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = data\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]        \n",
    "        \n",
    "        return (sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def split_the_data(data,test_frac=0.0):\n",
    "    '''\n",
    "    Splitting the data into train and test with default ratio = 0.1\n",
    "    Splits the data in orderly manner not random\n",
    "    '''\n",
    "    if(test_frac is not None):\n",
    "        train_data = data[0:int(np.ceil((1-test_frac)*data[:,].shape[0])),:]\n",
    "        test_data = data[-int(np.ceil(test_frac*data[:,].shape[0])):]\n",
    "    else:\n",
    "        train_data = data\n",
    "        test_data = torch.empty()\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def plot_dataset(data):\n",
    "    '''\n",
    "    To visualise the multivariate data\n",
    "    '''\n",
    "    fig = plt.figure()\n",
    "    print(\"Dataset has {} rows {} columns\".format(data[:,].shape[0],data[:,].shape[1]))\n",
    "\n",
    "    for i in range(data[:,].shape[-1]):\n",
    "        plt.plot(data[:,i])\n",
    "        plt.title(\"Plot of the dataset column wise\")\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def process_data(data,test_frac,to_plot=True):\n",
    "    '''\n",
    "    Function to convert the raw dataset into Timeseries class object\n",
    "    Then splits the data into train and test with test_frac arg\n",
    "    Returns : timeseries dataset, train_data and test_data\n",
    "    '''\n",
    "    data_set = TimeSeries_Dataset(data)\n",
    "    train_data,test_data = split_the_data(data_set,test_frac=test_frac)\n",
    "    print(\"Shape of Training dataset :{} and Test dataset :{}\\n\".format(train_data.shape,test_data.shape))\n",
    "    if(to_plot):\n",
    "        plot_dataset(train_data.numpy())\n",
    "    return data_set,train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def network_dimensions(train_data,N=100):\n",
    "    '''\n",
    "    Calculates the 2D network shape\n",
    "    The neurons are organized in a 2-dimensional map. The\n",
    "    ratio of the side lengths of the map is approximately the\n",
    "    ratio of the two largest eigenvalues of the training data\n",
    "    covariance matrix. \n",
    "    Arguments: \n",
    "    training data - Tensor object\n",
    "    N  - no of observations it's little abstract to decide this value. So its default set to 100\n",
    "    Returns: \n",
    "    Dimensions of the 2D network\n",
    "    '''\n",
    "    \n",
    "    approx_network_size = 5*np.sqrt(N)\n",
    "    train_df = pd.DataFrame(train_data.numpy())\n",
    "    cov_train_df = train_df.cov()\n",
    "    cov_data = cov_train_df.values\n",
    "    \n",
    "    if(train_data.shape[-1]<=1):\n",
    "        x = sp.linalg.eigh(cov_data,eigvals_only=True)[-1:]\n",
    "        ratio = np.ceil(x)    \n",
    "    else:\n",
    "        x,y = sp.linalg.eigh(cov_data,eigvals_only=True)[-2:]\n",
    "        ratio = np.ceil(x)/np.ceil(y)\n",
    "        \n",
    "    row_dim = int(np.ceil(np.sqrt(approx_network_size*ratio)))\n",
    "    col_dim = int(np.ceil(approx_network_size/row_dim))\n",
    "    return row_dim,col_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def test(model,evaluateData,anom_thres=3,to_plot=True):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Function to detect anomalies (Evaluating from  the saved model)\n",
    "        Arguments: \n",
    "        model -> Its instance of SOM_MODULE class imported from som_knn_module.py which is typically saved model loaded\n",
    "                from user given model path\n",
    "        evaluateData -> Data to detect anomalies\n",
    "        anom_thres   -> Default:3, Takes only integer\n",
    "        to_plot      -> Default: True, Give False (bool) to not plot the plots\n",
    "        \n",
    "        Returns : anomaly indexes\n",
    "        '''\n",
    "        \n",
    "        original_data = evaluateData\n",
    "        no_cols = original_data.shape[-1]\n",
    "        diff_order = model.diff_order\n",
    "        print(\"Input data's shape: {}\".format(original_data.shape))\n",
    "        \n",
    "        # Differencing done on the data if diff_order is non zero, default diff_order=0\n",
    "        res_evaluateData = np.diff(evaluateData,n=diff_order,axis=0).reshape(-1,no_cols)\n",
    "        print(\"Differenced data shape {}\".format(res_evaluateData.shape))\n",
    "        \n",
    "        #evaluate and get anomaly scores\n",
    "        anomaly_metrics = model.evaluate(res_evaluateData)\n",
    "        print(anomaly_metrics.shape)\n",
    "        anomaly_metrics = anomaly_metrics/np.linalg.norm(anomaly_metrics)\n",
    "        \n",
    "        #Normalising the anomaly scores by l2 norm\n",
    "        thres = anom_thres*(1/np.sqrt(len(anomaly_metrics)))\n",
    "        \n",
    "        #finding indexes where anomaly scores are greater than threshold\n",
    "        selector = anomaly_metrics > thres\n",
    "        \n",
    "        # getting the anomaly indexes\n",
    "        anom_indexes = np.arange(len(res_evaluateData))[selector]\n",
    "        \n",
    "        \n",
    "        if(to_plot):\n",
    "            '''\n",
    "            # We make a density plot and a histogram showing the distrbution\n",
    "            # of the number of points mapped to a BMU\n",
    "            '''\n",
    "            figa = plt.figure(figsize=(20,10))\n",
    "            plt.subplot(121)\n",
    "            density = gaussian_kde(anomaly_metrics)\n",
    "            xs = np.linspace(0,5,200)\n",
    "            plt.plot(xs,density(xs))\n",
    "            plt.title(\"Distribution of dataset\")\n",
    "\n",
    "            plt.subplot(122)\n",
    "            plt.hist(model.bmu_counts)\n",
    "            plt.title(\"Histogram of Bmu counts\")\n",
    "            plt.show();\n",
    "            \n",
    "            fig = plt.figure(figsize=(20,10))\n",
    "            plt.plot(anomaly_metrics)\n",
    "            plt.title(\"Anomaly score\")\n",
    "            plt.axhline(y=thres,color='r',label=\"Threshold\")\n",
    "            plt.legend()\n",
    "            plt.show();\n",
    "\n",
    "            if(diff_order!=0):\n",
    "                fig2 = plt.figure(figsize=(20,10))\n",
    "                plt.plot(res_evaluateData[:,])\n",
    "                plt.title(\"Dataset after differencing marked with anomalies\")\n",
    "                [plt.axvline(x=ind,color='r') for ind in anom_indexes]\n",
    "                plt.show();\n",
    "\n",
    "                fig3 = plt.figure(figsize=(20,10))\n",
    "                plt.plot(original_data)\n",
    "                plt.title(\"Exact Dataset with detectedanomalies\")\n",
    "#                 plt.scatter(x=anom_indexes,y=original_data[anom_indexes,0],color='r')\n",
    "                [plt.axvline(x=ind,color='r') for ind in anom_indexes]\n",
    "\n",
    "                plt.show();\n",
    "\n",
    "            else:\n",
    "                fig3 = plt.figure(figsize=(20,10))\n",
    "                plt.plot(original_data)\n",
    "                plt.title(\"Exact Dataset with detectedanomalies\")\n",
    "#                 plt.scatter(x=anom_indexes,y=original_data[anom_indexes,0],color='r')\n",
    "                [plt.axvline(x=ind,color='r') for ind in anom_indexes]\n",
    "\n",
    "                plt.show();\n",
    "        \n",
    "    \n",
    "        no_anoms_detected = (list(selector).count(True))\n",
    "        print(\"\\nNo of anomalies detected : {}, Fraction of data detected as anomaly : {}\".\n",
    "              format(no_anoms_detected,no_anoms_detected/(evaluateData.shape[0])))\n",
    "        return anom_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "\n",
    "def train_loop(model,train_loader,epochs):\n",
    "    '''\n",
    "    Function to train the model.\n",
    "    Arguments:\n",
    "    model: instance of the som_module class imported from som_knn_module.py\n",
    "    train_loader : Its data loader using pytorch which gives out batch of inputs\n",
    "    epochs: No of epochs to train the model\n",
    "    \n",
    "    Returns : Trained model\n",
    "    '''\n",
    "    curr_batch_iter = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch : {} completed\\n\".format(epoch))\n",
    "\n",
    "    for i,x_batch in enumerate(train_loader):\n",
    "            curr_batch_iter += 1\n",
    "            model = model.fit(x_batch,curr_batch_iter)\n",
    "    \n",
    "    print(\"\\n Training successfully completed \\n\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "\n",
    "def check_default_args(inp_kwargs,def_kwargs):\n",
    "    '''\n",
    "    Checking Default arguments\n",
    "    Arguments :\n",
    "    inp_kwargs : input kwargs given by the user\n",
    "    def_kwargs : default kwargs calculated from logic\n",
    "    \n",
    "    So it overwrites the input args if its not none \n",
    "    Returns: \n",
    "    Updated inp_kwargs\n",
    "    '''\n",
    "    for key in inp_kwargs:\n",
    "        for def_key in def_kwargs:\n",
    "            if(inp_kwargs[def_key]==None):\n",
    "                inp_kwargs[def_key]=def_kwargs[def_key]\n",
    "    return inp_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def create_cum_train_som(train_data,model_input_args,training_args):\n",
    "    \n",
    "    '''\n",
    "    Function to create the model and train the som_knn model\n",
    "    Arguments :\n",
    "    \n",
    "    train_data : training data\n",
    "    model_input_args : dictionary of model_input_args\n",
    "    training_args : dictionary of training_args\n",
    "    \n",
    "    Returns:\n",
    "    Created and trained model \n",
    "    '''\n",
    "    def_kwargs = {}\n",
    "    \n",
    "    epochs = training_args['epochs']\n",
    "    batch_size = training_args['batch_size']\n",
    "    n_iterations = int(epochs*(len(train_data)/batch_size))\n",
    "\n",
    "    def_kwargs['som_shape'] = network_dimensions(train_data,model_input_args['N'])\n",
    "    row_dim,col_dim = def_kwargs['som_shape']\n",
    "\n",
    "    def_kwargs['initial_radius'] = max(row_dim,col_dim)/2\n",
    "    def_kwargs['time_constant'] = n_iterations/np.log(def_kwargs['initial_radius'])\n",
    "    \n",
    "    model_kwargs = check_default_args(model_input_args,def_kwargs)\n",
    "    model_input_args.update(model_kwargs)\n",
    "    \n",
    "    model_input_args['input_feature_size'] = train_data.shape[-1]\n",
    "    model_input_args['n_iterations'] = n_iterations\n",
    "    \n",
    "    print(\"Network dimensions are {} x {} \\n\".format(row_dim,col_dim))\n",
    "    diff_order = model_input_args['diff_order']\n",
    "    del model_input_args['N']\n",
    "    model = som_knn_module.Som_model(**model_input_args)\n",
    "\n",
    "    res_train_data = (np.diff(train_data.numpy(),n=diff_order,axis=0).reshape(-1,train_data.numpy().shape[-1]))\n",
    "    print(\"\\nShape of differenced Training data : {}\\n\".format(res_train_data.shape))\n",
    "    train_data_diff = torch.from_numpy(res_train_data)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data_diff, batch_size=batch_size,shuffle=True)\n",
    "    \n",
    "    model = train_loop(model=model,epochs=epochs,train_loader=train_loader)\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
