{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import writefile_run as writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '../../anomaly_detectors/som_knn_detector/som_knn_wrapper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohithram/anaconda3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "%%writefile_run $filename\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import pickle\n",
    "\n",
    "#torch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "#importing sklearn libraries\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# Importing reader and checker python files as modules\n",
    "from anomaly_detectors.reader_writer import db_properties as db_props\n",
    "from anomaly_detectors.reader_writer import writer_configs as write_args\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "from anomaly_detectors.utils.preprocessors import *\n",
    "from anomaly_detectors.utils.data_handler import *\n",
    "# from anomaly_detectors.bayesian_detectorbayesian_changept_detector import *\n",
    "\n",
    "from anomaly_detectors.utils import error_codes as error_codes\n",
    "from anomaly_detectors.utils import type_checker as type_checker\n",
    "from anomaly_detectors.utils import csv_prep_for_reader as csv_helper\n",
    "from anomaly_detectors.utils import reader_helper\n",
    "from anomaly_detectors.utils import make_ackg_json\n",
    "\n",
    "\n",
    "from anomaly_detectors.som_knn_detector import som_knn_detector as som_detector\n",
    "from anomaly_detectors.som_knn_detector import som_knn_module as som_model\n",
    "\n",
    "\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 9\n",
    "rcParams[ 'axes.grid']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "from IPython import display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "\n",
    "ideal_train_kwargs_type  = {\n",
    "            'som_shape':tuple,\n",
    "            'input_feature_size':int,\n",
    "            'time_constant':float,\n",
    "            'minNumPerBmu':int,\n",
    "            'no_of_neighbors':int,\n",
    "            'initial_radius':float,\n",
    "            'initial_learning_rate':float,\n",
    "            'n_iterations':int,\n",
    "            'N':int,    \n",
    "            'diff_order':int,\n",
    "            'is_train':bool,\n",
    "            'epochs':int,\n",
    "            'batch_size':int,\n",
    "            'to_plot':bool,\n",
    "            'test_frac':float\n",
    "        }\n",
    "\n",
    "ideal_eval_kwargs_type = {\n",
    "            'model_path':str,\n",
    "            'to_plot':bool,\n",
    "            'anom_thres':int\n",
    "        }\n",
    "\n",
    "mode_options = ['detect only','detect and log','log only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def train(json_data,network_shape=None,input_feature_size=None,time_constant=None,minNumPerBmu=2,\n",
    "          no_of_neighbours=10,init_radius=None,init_learning_rate=0.01,N=100,diff_order=1,is_train=True\n",
    "          ,epochs=4,batch_size=4,to_plot=True,test_frac=0.5):\n",
    "\n",
    "        '''\n",
    "        Wrapper function which should be called inorder to run the anomaly detection, it has four parts :\n",
    "        *reader           - Class Data_reader defined in data_handler.py which takes in reader args and parses json \n",
    "                            and gives dataframes\n",
    "        *preprocessor     - preprocessors are defined in preprocessors.py, which takes in data and gives out processed \n",
    "                            data\n",
    "        *anomaly detector - Class Bayesian_Changept_Detector defined in bayesian_changept_detector.py, which takes in\n",
    "                            data and algorithm parameters as argument and returns anomaly indexes and data.        \n",
    "        *writer           - Class Postgres_Writer defined in data_handler.py which takes in anomaly detector object and\n",
    "                            and sql_queries , db_properties and table name as args and gives out response code.\n",
    "        \n",
    "        Arguments :\n",
    "        It takes reader args as of now to get the dataset and algo related arguments\n",
    "        Note:\n",
    "        To run this, import this python file as module and call this function with required args and it will detect\n",
    "        anomalies and writes to the local database.\n",
    "        This algorithm is univariate, so each metric per asset is processed individually\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        #algorithm arguments\n",
    "\n",
    "        model_input_args = {\n",
    "            'som_shape':network_shape,\n",
    "            'input_feature_size':None,\n",
    "            'time_constant':None,\n",
    "            'minNumPerBmu':minNumPerBmu,\n",
    "            'no_of_neighbors':no_of_neighbours,\n",
    "            'initial_radius':init_radius,\n",
    "            'initial_learning_rate':init_learning_rate,\n",
    "            'n_iterations':None,\n",
    "            'N':N,    \n",
    "            'diff_order':diff_order\n",
    "        }\n",
    "        \n",
    "        #Training arguments\n",
    "        training_args = {\n",
    "            'is_train':True,\n",
    "            'epochs':epochs,\n",
    "            'batch_size':batch_size,\n",
    "            'to_plot':to_plot,\n",
    "            'test_frac':test_frac\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "        #merging all algo arguments for params checking\n",
    "        algo_kwargs = {**model_input_args,**training_args}\n",
    "        \n",
    "                    \n",
    "        try: \n",
    "            '''\n",
    "            #reseting the error_codes to avoid overwritting\n",
    "            #error_codes is a python file imported as error_codes which has error_codes dictionary mapping \n",
    "            #for different kinds errors and reset function to reset them.\n",
    "            '''\n",
    "            \n",
    "            error_codes.reset()\n",
    "            # type_checker is python file which has Type_checker class which checks given parameter types\n",
    "            checker = type_checker.Type_checker(kwargs=algo_kwargs,ideal_args_type=ideal_train_kwargs_type)\n",
    "            # res is None when no error raised, otherwise it stores the appropriate error message\n",
    "            res = checker.params_checker()\n",
    "            if(res!=None):\n",
    "                return json.dumps(res)\n",
    "            \n",
    "            # instanstiating the reader class with reader arguments\n",
    "            data_reader = Data_reader(json_data=json_data)\n",
    "            #getting list of dataframes per asset if not empty\n",
    "            #otherwise gives string 'Empty Dataframe'\n",
    "            entire_data = data_reader.read()\n",
    "            \n",
    "            writer_data = []\n",
    "            anomaly_detectors = []\n",
    "            \n",
    "            if((len(entire_data)!=0 and entire_data is not None and type(entire_data)!=dict)):\n",
    "            \n",
    "                '''\n",
    "                looping over the data per assets and inside that looping over metrics per asset\n",
    "                * Instantiates anomaly detector class with algo args and metric index to detect on\n",
    "                * Stores the anomaly indexes and anomaly detector object to bulk write to db at once\n",
    "                '''\n",
    "                \n",
    "#                 model_paths = []\n",
    "                out_json = {'header':'','models':[]}\n",
    "\n",
    "                for i,data_per_asset in enumerate(entire_data):\n",
    "                    assetno = pd.unique(data_per_asset['assetno'])[0]\n",
    "#                     print(assetno)\n",
    "                    data_per_asset[data_per_asset.columns[1:]] = normalise_standardise(data_per_asset[data_per_asset.columns[1:]]\n",
    "                                                                 )\n",
    "                    \n",
    "                    \n",
    "                    print(\"Data of Asset no: {} \\n {}\\n\".format(assetno,data_per_asset.head()))\n",
    "                    cols = list(data_per_asset.columns[1:])\n",
    "                    \n",
    "                    anomaly_detector = som_detector.Som_Detector(data = data_per_asset,                                                            assetno=assetno,model_input_args=model_input_args,\n",
    "                                                                 training_args=training_args,metric_names=cols,\n",
    "                                                                eval_args=None)\n",
    "                    \n",
    "                    model_path = (anomaly_detector.detect_anomalies())\n",
    "                    \n",
    "                    model = {anomaly_detector.assetno:model_path[0]}\n",
    "#                     table_name = write_args.table_name\n",
    "#                     window_size = 10\n",
    "#                     anomaly_detectors.append(anomaly_detector)\n",
    "#                     sql_query_args = write_args.writer_kwargs\n",
    "                    \n",
    "\n",
    "                    out_json['models'].append(model)\n",
    "        \n",
    "                out_json['header'] = error_codes.error_codes['success']\n",
    "                \n",
    "#                 if(mode==mode_options[0] or mode==mode_options[1]):\n",
    "#                     ack_json = make_ackg_json.make_ack_json(anomaly_detectors)\n",
    "#                     out_json['detect_status'] = ack_json\n",
    "#                 if(mode==mode_options[1] or mode==mode_options[2]):\n",
    "#                     '''\n",
    "#                     Instantiates writer class to write into local database with arguments given below\n",
    "#                     Used for Bulk writing\n",
    "#                     '''\n",
    "#                     writer = Postgres_Writer(anomaly_detectors,db_credentials=db_props.db_connection,\n",
    "#                                              sql_query_args=sql_query_args,\n",
    "#                                             table_name=table_name,window_size=window_size)\n",
    "\n",
    "#                     #called for mapping args before writing into db\n",
    "#                     res = writer.map_outputs_and_write()\n",
    "#                     out_json['log_status']=res\n",
    "               \n",
    "                return json.dumps(out_json)\n",
    "            elif(type(entire_data)==dict):\n",
    "               return json.dumps(entire_data)\n",
    "            else:\n",
    "                '''\n",
    "                Data empty error\n",
    "                '''\n",
    "                \n",
    "                return json.dumps(error_codes.error_codes['data_missing'])\n",
    "        except Exception as e:\n",
    "            '''\n",
    "            unknown exceptions are caught here and traceback used to know the source of the error\n",
    "            '''\n",
    "            traceback.print_exc()\n",
    "            error_codes.error_codes['unknown']['message']=str(e)\n",
    "            return json.dumps(error_codes.error_codes['unknown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def evaluate(json_data,model_path,mode=mode_options[0],to_plot=True,anom_thres=3):\n",
    "\n",
    "    \n",
    "        '''\n",
    "        Wrapper function which should be called inorder to run the anomaly detection, it has four parts :\n",
    "        *reader           - Class Data_reader defined in data_handler.py which takes in reader args and parses json \n",
    "                            and gives dataframes\n",
    "        *preprocessor     - preprocessors are defined in preprocessors.py, which takes in data and gives out processed \n",
    "                            data\n",
    "        *anomaly detector - Class Bayesian_Changept_Detector defined in bayesian_changept_detector.py, which takes in\n",
    "                            data and algorithm parameters as argument and returns anomaly indexes and data.        \n",
    "        *writer           - Class Postgres_Writer defined in data_handler.py which takes in anomaly detector object and\n",
    "                            and sql_queries , db_properties and table name as args and gives out response code.\n",
    "        \n",
    "        Arguments :\n",
    "        It takes reader args as of now to get the dataset and algo related arguments\n",
    "        Note:\n",
    "        To run this, import this python file as module and call this function with required args and it will detect\n",
    "        anomalies and writes to the local database.\n",
    "        This algorithm is univariate, so each metric per asset is processed individually\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        eval_args = {\n",
    "            'model_path':model_path,\n",
    "            'to_plot':to_plot,\n",
    "            'anom_thres':anom_thres\n",
    "        }\n",
    "                \n",
    "                    \n",
    "        try: \n",
    "            '''\n",
    "            #reseting the error_codes to avoid overwritting\n",
    "            #error_codes is a python file imported as error_codes which has error_codes dictionary mapping \n",
    "            #for different kinds errors and reset function to reset them.\n",
    "            '''\n",
    "            \n",
    "            error_codes.reset()\n",
    "            # type_checker is python file which has Type_checker class which checks given parameter types\n",
    "            checker = type_checker.Type_checker(kwargs=eval_args,ideal_args_type=ideal_eval_kwargs_type)\n",
    "            # res is None when no error raised, otherwise it stores the appropriate error message\n",
    "            res = checker.params_checker()\n",
    "            if(res!=None):\n",
    "                return json.dumps(res)\n",
    "            \n",
    "            # instanstiating the reader class with reader arguments\n",
    "            data_reader = Data_reader(json_data=json_data)\n",
    "            #getting list of dataframes per asset if not empty\n",
    "            #otherwise gives string 'Empty Dataframe'\n",
    "            entire_data = data_reader.read()\n",
    "            \n",
    "            writer_data = []\n",
    "            anomaly_detectors = []\n",
    "            \n",
    "            if((len(entire_data)!=0 and entire_data!=None and type(entire_data)!=dict)):\n",
    "\n",
    "                '''\n",
    "                looping over the data per assets and inside that looping over metrics per asset\n",
    "                * Instantiates anomaly detector class with algo args and metric index to detect on\n",
    "                * Stores the anomaly indexes and anomaly detector object to bulk write to db at once\n",
    "                '''\n",
    "\n",
    "                for i,data_per_asset in enumerate(entire_data):\n",
    "                    assetno = pd.unique(data_per_asset['assetno'])[0]\n",
    "                    data_per_asset[data_per_asset.columns[1:]] = normalise_standardise(data_per_asset[data_per_asset.columns[1:]]\n",
    "                                                                 )\n",
    "                    \n",
    "                    print(\"Data of Asset no: {} \\n {}\\n\".format(assetno,data_per_asset.head()))\n",
    "                    cols = list(data_per_asset.columns[1:])\n",
    "                    \n",
    "                    anomaly_detector = som_detector.Som_Detector(data = data_per_asset,                                                            assetno=assetno,model_input_args=model_input_args,\n",
    "                                                                 training_args=None,metric_names=cols,eval_args=eval_args)\n",
    "                    \n",
    "                    anom_indexes = anomaly_detector.detect_anomalies()\n",
    "                    anomaly_detectors.append(anomaly_detector)\n",
    "                    \n",
    "                out_json = {}\n",
    "                \n",
    "                if(mode==mode_options[0] or mode==mode_options[1]):\n",
    "                    ack_json = make_ackg_json.make_ack_json(anomaly_detectors)\n",
    "                    out_json['detect_status'] = ack_json\n",
    "                if(mode==mode_options[1] or mode==mode_options[2]):\n",
    "                    \n",
    "                    '''\n",
    "                    Instantiates writer class to write into local database with arguments given below\n",
    "                    Used for Bulk writing\n",
    "                    '''\n",
    "                    sql_query_args = write_args.writer_kwargs\n",
    "                    table_name = write_args.table_name\n",
    "                    window_size = 10\n",
    "\n",
    "                    writer = Postgres_Writer(anomaly_detectors,db_credentials=db_props.db_connection,sql_query_args=sql_query_args,\n",
    "                                            table_name=table_name,window_size=window_size)\n",
    "\n",
    "                    #called for mapping args before writing into db\n",
    "                    res = writer.map_outputs_and_write()\n",
    "                    out_json['log_status'] = res\n",
    "                    \n",
    "                return json.dumps(out_json)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                '''\n",
    "                Data empty error\n",
    "                '''\n",
    "                return json.dumps(error_codes.error_codes['data_missing'])\n",
    "        except Exception as e:\n",
    "            '''\n",
    "            unknown exceptions are caught here and traceback used to know the source of the error\n",
    "            '''\n",
    "            traceback.print_exc()\n",
    "            error_codes.error_codes['unknown']['message']=str(e)\n",
    "            return json.dumps(error_codes.error_codes['unknown'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "reader_kwargs= lambda:{\n",
    "            'assetno':['TSFAD_A1'],\n",
    "            'from_timestamp':'',\n",
    "            'to_timestamp':'',\n",
    "            'con':'',\n",
    "            'para_list':'',\n",
    "            'source_type':'',\n",
    "            'table_name':'',\n",
    "            'qry_str':'',\n",
    "            'impute_fill_method':'forward',\n",
    "            'down_sampling_method':None,\n",
    "            'down_sampling_window':None,\n",
    "            'freq':None,\n",
    "            'resample_fill_method':None,\n",
    "            'to_resample':None,\n",
    "            'to_impute':True,\n",
    "}\n",
    "\n",
    "model_input_args = lambda :{\n",
    "    'network_shape':(8,8),\n",
    "    'input_feature_size':None,\n",
    "    'time_constant':None,\n",
    "    'minNumPerBmu':2,\n",
    "    'no_of_neighbours':3,\n",
    "    'init_radius':0.4,\n",
    "    'init_learning_rate':0.01,\n",
    "    'N':100,    \n",
    "    'diff_order':1\n",
    "}\n",
    "\n",
    "training_args = lambda:{\n",
    "            'is_train':True,\n",
    "            'epochs':5,\n",
    "            'batch_size':4,\n",
    "            'to_plot':True,\n",
    "            'test_frac':0.7\n",
    "        }\n",
    "\n",
    "\n",
    "        \n",
    "eval_args = lambda: {\n",
    "    'model_path':'',\n",
    "    'to_plot':True,\n",
    "    'anom_thres':3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%writefile_run bayeschangept_sprint1.py -a\n",
    "\n",
    "'''\n",
    "Arguments for reader module to get data from opentsdb\n",
    "This is included for now just for testing, later the main function will take json as direct input\n",
    "'''\n",
    "\n",
    "assetno = ['TSFAD_A1']\n",
    "con = '192.168.2.5:4242'\n",
    "src_type =  'opentsdb'\n",
    "param = ['ec2_cpu_utilization_5f5533']\n",
    "from_timestamp = 1392388020\n",
    "to_timestamp = 1393597320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on data from opentsdb and saving it in a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# %%writefile_run som_knn_wrapper_sprint1.py -a\n",
    "\n",
    "\n",
    "'''\n",
    "Dictionary of arguments given to wrapper function which executes this whole program for detecting changepoints and writing\n",
    "to database\n",
    "'''\n",
    "\n",
    "reader_kwargs1 = reader_kwargs()\n",
    "reader_kwargs1['assetno'] = assetno\n",
    "reader_kwargs1['source_type']=src_type\n",
    "reader_kwargs1['con'] = con\n",
    "reader_kwargs1['from_timestamp'] = from_timestamp\n",
    "reader_kwargs1['to_timestamp'] = to_timestamp\n",
    "reader_kwargs1['para_list'] = param\n",
    "model_input_args1 = model_input_args()\n",
    "training_args1 = training_args()\n",
    "training_args1['test_frac'] = 0.2\n",
    "json_data = reader_helper.read(reader_kwargs=reader_kwargs1)\n",
    "model_input_args1['diff_order'] = 0\n",
    "kwargs1 = {**model_input_args1,**training_args1}\n",
    "train_res = json.loads(train(**kwargs1,json_data=json_data))\n",
    "# model_path1 = res['models'][0]\n",
    "print(train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing saved model on data from opentsdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "'''\n",
    "Dictionary of arguments given to wrapper function which executes this whole program for detecting changepoints with \n",
    "a trained model and writing anomalies to database\n",
    "'''\n",
    "\n",
    "reader_kwargs1 = reader_kwargs()\n",
    "reader_kwargs1['assetno'] = assetno\n",
    "reader_kwargs1['con'] = con\n",
    "reader_kwargs1['source_type']=src_type\n",
    "reader_kwargs1['from_timestamp'] = from_timestamp\n",
    "reader_kwargs1['to_timestamp'] = to_timestamp\n",
    "reader_kwargs1['para_list'] = param\n",
    "eval_args1 = eval_args()\n",
    "eval_args1['anom_thres'] = 3\n",
    "# print(res)\n",
    "eval_args1['model_path'] = train_res['models'][0][assetno[0]]\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "test_res = evaluate(**eval_args1,json_data=json_data,mode=mode_options[0])\n",
    "print(test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on data from a list of sample csv datasets and saving them in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detecting anomalies for methane-input-into-gas-furnace-c.csv\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno       CO2   Methane\n",
      "timestamp                                  \n",
      "7808400000000  TSFAD_A1  0.090839 -1.170356\n",
      "7808940000000  TSFAD_A1  0.028381 -1.340206\n",
      "7809480000000  TSFAD_A1 -0.002849 -1.062837\n",
      "7810020000000  TSFAD_A1 -0.002849 -0.811959\n",
      "7810560000000  TSFAD_A1 -0.034078 -0.758978\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([296, 2])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([89, 2]) and Test dataset :torch.Size([208, 2])\n",
      "\n",
      "Network dimensions are 5 x 10 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (88, 2)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_CO2_Methane_1530375811256 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530375811256\n",
      "\n",
      "{'header': {'code': '200', 'status': 'OK'}, 'models': [{'TSFAD_A1': '../../Anomaly_Detection_Models/Machine_Learning_Models/som_trained_model_CO2_Methane_1530375811256'}]}\n",
      "\n",
      "Detecting anomalies for average-annual-temperature-centr.csv\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "             assetno  Average annual temperature, central England, 1723 ? 1970\n",
      "timestamp                                                                    \n",
      "0          TSFAD_A1                                           0.920853       \n",
      "0          TSFAD_A1                                           0.089020       \n",
      "0          TSFAD_A1                                          -0.925817       \n",
      "0          TSFAD_A1                                           0.205476       \n",
      "0          TSFAD_A1                                           1.203676       \n",
      "\n",
      "Shape of the Entire dataset : torch.Size([248, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([75, 1]) and Test dataset :torch.Size([174, 1])\n",
      "\n",
      "Network dimensions are 10 x 5 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (75, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_AverageannualtemperaturecentralEngland17231970_1530375811471 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530375811471\n",
      "\n",
      "{'header': {'code': '200', 'status': 'OK'}, 'models': [{'TSFAD_A1': '../../Anomaly_Detection_Models/Machine_Learning_Models/som_trained_model_AverageannualtemperaturecentralEngland17231970_1530375811471'}]}\n",
      "\n",
      "Detecting anomalies for mean-monthly-temperature-1907-19.csv\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                  assetno  Mean monthly temperature, 1907 ? 1972\n",
      "timestamp                                                      \n",
      "-1988150400000  TSFAD_A1                              -1.280625\n",
      "-1985472000000  TSFAD_A1                              -0.477613\n",
      "-1983052800000  TSFAD_A1                              -0.667301\n",
      "-1980374400000  TSFAD_A1                               0.091451\n",
      "-1977782400000  TSFAD_A1                              -0.110883\n",
      "\n",
      "Shape of the Entire dataset : torch.Size([792, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([238, 1]) and Test dataset :torch.Size([555, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (238, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_Meanmonthlytemperature19071972_1530375811903 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530375811903\n",
      "\n",
      "{'header': {'code': '200', 'status': 'OK'}, 'models': [{'TSFAD_A1': '../../Anomaly_Detection_Models/Machine_Learning_Models/som_trained_model_Meanmonthlytemperature19071972_1530375811903'}]}\n",
      "\n",
      "Detecting anomalies for monthly-us-female-20-years-and-o.csv\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                 assetno  \\\n",
      "timestamp                 \n",
      "-694310400000  TSFAD_A1   \n",
      "-691632000000  TSFAD_A1   \n",
      "-689126400000  TSFAD_A1   \n",
      "-686448000000  TSFAD_A1   \n",
      "-683856000000  TSFAD_A1   \n",
      "\n",
      "               Monthly U.S. female (20 years and over) unemployment figures (10**3) 1948-1981  \n",
      "timestamp                                                                                      \n",
      "-694310400000                                          -1.351533                               \n",
      "-691632000000                                          -1.054180                               \n",
      "-689126400000                                          -1.138722                               \n",
      "-686448000000                                          -1.183908                               \n",
      "-683856000000                                          -1.285941                               \n",
      "\n",
      "Shape of the Entire dataset : torch.Size([408, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([123, 1]) and Test dataset :torch.Size([286, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (121, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_MonthlyUSfemale20yearsandoverunemploymentfigures10319481981_1530375812192 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530375812192\n",
      "\n",
      "{'header': {'code': '200', 'status': 'OK'}, 'models': [{'TSFAD_A1': '../../Anomaly_Detection_Models/Machine_Learning_Models/som_trained_model_MonthlyUSfemale20yearsandoverunemploymentfigures10319481981_1530375812192'}]}\n",
      "\n",
      "Detecting anomalies for winter-negative-temperature-sum-.csv\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "             assetno  Winter negative temperature sum (in deg. C), 1781 ? 1988\n",
      "timestamp                                                                    \n",
      "0          TSFAD_A1                                          -0.027888       \n",
      "0          TSFAD_A1                                          -0.507193       \n",
      "0          TSFAD_A1                                           2.003772       \n",
      "0          TSFAD_A1                                           2.091459       \n",
      "0          TSFAD_A1                                          -0.114832       \n",
      "\n",
      "Shape of the Entire dataset : torch.Size([208, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([63, 1]) and Test dataset :torch.Size([146, 1])\n",
      "\n",
      "Network dimensions are 10 x 5 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (61, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_WinternegativetemperaturesumindegC17811988_1530375812314 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530375812314\n",
      "\n",
      "{'header': {'code': '200', 'status': 'OK'}, 'models': [{'TSFAD_A1': '../../Anomaly_Detection_Models/Machine_Learning_Models/som_trained_model_WinternegativetemperaturesumindegC17811988_1530375812314'}]}\n",
      "\n",
      "Detecting anomalies for alcohol-demand-log-spirits-consu.csv\n",
      "\n",
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                  assetno  \\\n",
      "timestamp                  \n",
      "-3147897600000  TSFAD_A1   \n",
      "-3137356800000  TSFAD_A1   \n",
      "-3126816000000  TSFAD_A1   \n",
      "-3116361600000  TSFAD_A1   \n",
      "-3105820800000  TSFAD_A1   \n",
      "\n",
      "                Alcohol demand (log spirits consumption per head), UK, 1870-1938  \n",
      "timestamp                                                                         \n",
      "-3147897600000                                           0.026580                 \n",
      "-3137356800000                                           0.114869                 \n",
      "-3126816000000                                           0.247302                 \n",
      "-3116361600000                                           0.379735                 \n",
      "-3105820800000                                           0.423880                 \n",
      "\n",
      "Shape of the Entire dataset : torch.Size([207, 1])\n",
      "\n",
      "torch.float64\n",
      "Shape of Training dataset :torch.Size([63, 1]) and Test dataset :torch.Size([145, 1])\n",
      "\n",
      "Network dimensions are 8 x 7 \n",
      "\n",
      "\n",
      "Shape of differenced Training data : (62, 1)\n",
      "\n",
      "Epoch : 0 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 1 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 2 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 3 completed \n",
      " Max Bmu index : (0, 0)\n",
      "Epoch : 4 completed \n",
      " Max Bmu index : (0, 0)\n",
      "\n",
      " Training successfully completed \n",
      "\n",
      "\n",
      "Saved model : som_trained_model_AlcoholdemandlogspiritsconsumptionperheadUK18701938_1530375812477 in ../../Anomaly_Detection_Models/Machine_Learning_Models,\n",
      "Last Checkpointed at: 1530375812477\n",
      "\n",
      "{'header': {'code': '200', 'status': 'OK'}, 'models': [{'TSFAD_A1': '../../Anomaly_Detection_Models/Machine_Learning_Models/som_trained_model_AlcoholdemandlogspiritsconsumptionperheadUK18701938_1530375812477'}]}\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../../dataset/sample_csv_files/'\n",
    "target_dir = '../../dataset/reader_csv_files/'\n",
    "assetno = ['TSFAD_A1']\n",
    "difforders = [1,0,0,2,2,1]\n",
    "model_paths = []\n",
    "for i,filename in enumerate(os.listdir(data_dir)):\n",
    "    \n",
    "    name,ext = os.path.splitext(filename)\n",
    "    if ext != '.csv':continue\n",
    "    infile = os.path.join(data_dir,filename)\n",
    "    print(\"\\nDetecting anomalies for {}\\n\".format(filename))\n",
    "    \n",
    "    \n",
    "    kwargs1 = csv_helper.get_csv_kwargs(infile=infile,filename=filename,target_dir=target_dir,assetno=assetno[0])\n",
    "    \n",
    "#     print('params: {}\\n'.format(param))\n",
    "    reader_kwargs1 = kwargs1\n",
    "    model_input_args1 = model_input_args()\n",
    "    model_input_args1['diff_order'] = difforders[i]\n",
    "    training_args1 = training_args()\n",
    "    training_args1['to_plot'] = False\n",
    "    json_data = reader_helper.read(reader_kwargs1)\n",
    "    \n",
    "    kwargs1 = {**model_input_args1,**training_args1}\n",
    "    res = json.loads(train(**kwargs1,json_data=json_data))\n",
    "    model_paths.append(res['models'][0][assetno[0]])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the saved model on sample csv datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reader initialised \n",
      "\n",
      "Getting the dataset from the reader....\n",
      "\n",
      "Data of Asset no: TSFAD_A1 \n",
      "                  assetno  \\\n",
      "timestamp                  \n",
      "-3147897600000  TSFAD_A1   \n",
      "-3137356800000  TSFAD_A1   \n",
      "-3126816000000  TSFAD_A1   \n",
      "-3116361600000  TSFAD_A1   \n",
      "-3105820800000  TSFAD_A1   \n",
      "\n",
      "                Alcohol demand (log spirits consumption per head), UK, 1870-1938  \n",
      "timestamp                                                                         \n",
      "-3147897600000                                           0.026580                 \n",
      "-3137356800000                                           0.114869                 \n",
      "-3126816000000                                           0.247302                 \n",
      "-3116361600000                                           0.379735                 \n",
      "-3105820800000                                           0.423880                 \n",
      "\n",
      "Shape of the Entire dataset : torch.Size([207, 1])\n",
      "\n",
      "Input data's shape: (207, 1)\n",
      "Differenced data shape (206, 1)\n",
      "(206,)\n",
      "No of anomalies detected : 4, Fraction of data detected as anomaly : 0.01932367149758454\n",
      "\n",
      " No of Anomalies detected = 4\n",
      "{\"detect_status\": {\"header\": {\"code\": \"200\", \"status\": \"OK\"}, \"body\": [{\"asset\": \"TSFAD_A1\", \"anomalies\": [{\"name\": \"Alcohol demand (log spirits consumption per head), UK, 1870-1938\", \"datapoints\": [{\"from_timestamp\": -2663971200000, \"to_timestamp\": -2663971200000, \"anomaly_timestamp\": [-2663971200000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": -2432505600000, \"to_timestamp\": -2432505600000, \"anomaly_timestamp\": [-2432505600000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": -1706832000000, \"to_timestamp\": -1706832000000, \"anomaly_timestamp\": [-1706832000000], \"anomaly_code\": \"som\"}, {\"from_timestamp\": -1212451200000, \"to_timestamp\": -1212451200000, \"anomaly_timestamp\": [-1212451200000], \"anomaly_code\": \"som\"}]}]}]}}\n"
     ]
    }
   ],
   "source": [
    "infile = '../../dataset/sample_csv_files/alcohol-demand-log-spirits-consu.csv'\n",
    "filename = 'alcohol-demand-log-spirits-consu.csv'\n",
    "target_dir = '../../dataset/reader_csv_files/'\n",
    "assetno = ['TSFAD_A1']\n",
    "kwargs1 = csv_helper.get_csv_kwargs(infile=infile,filename=filename,target_dir=target_dir,assetno=assetno[0])\n",
    "    \n",
    "reader_kwargs1 = kwargs1\n",
    "eval_args1 = eval_args()\n",
    "eval_args1['to_plot'] = False\n",
    "eval_args1['model_path'] = model_paths[-1]\n",
    "kwargs1 = {**eval_args1}\n",
    "json_data = reader_helper.read(reader_kwargs1)\n",
    "res = evaluate(**kwargs1,json_data=json_data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance or Algorithm Tuning Test:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_input_args = lambda :{\n",
    "    'network_shape':(8,8),\n",
    "    'input_feature_size':None,\n",
    "    'time_constant':None,\n",
    "    'minNumPerBmu':2,\n",
    "    'no_of_neighbours':3,\n",
    "    'init_radius':0.4,\n",
    "    'init_learning_rate':0.01,\n",
    "    'N':100,    \n",
    "    'diff_order':1\n",
    "}\n",
    "\n",
    "training_args = lambda:{\n",
    "            'is_train':True,\n",
    "            'epochs':5,\n",
    "            'batch_size':4,\n",
    "            'to_plot':True,\n",
    "            'test_frac':0.7\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_dir = 'dataset/one_csv/'\n",
    "target_dir = 'dataset/reader_csv_files/'\n",
    "assetno = ['1']\n",
    "difforders = [1,0,0,2,2,1]\n",
    "\n",
    "model_input_args1 = model_input_args()\n",
    "training_args1 = training_args()\n",
    "\n",
    "init_radiuses = [0.01,0.025,0.05,0.1,0.25,0.5,1,1.5]\n",
    "no_neighbors = [2,4,6,8,10,12,14,16,20,25]\n",
    "init_learning_rates = [0.0001,0.001,0.01,0.05,0.1,0.25,0.5]\n",
    "minPerBmus = [1,2,4,6,8,10,12]\n",
    "\n",
    "\n",
    "for i,filename in enumerate(os.listdir(data_dir)):\n",
    "    for rad in init_radiuses:\n",
    "        for no_neighbor in no_neighbors:\n",
    "            for minPerBmu in minPerBmus:\n",
    "                for init_learning_rate in init_learning_rates:\n",
    "                    model_input_args1['init_learning_rate'] = init_learning_rate\n",
    "                    model_input_args1['minNumPerBmu'] = minPerBmu\n",
    "                    model_input_args1['no_of_neighbours'] = no_neighbor\n",
    "                    model_input_args1['init_radius'] = rad\n",
    "                    \n",
    "                    print(\"\\nModel args :Learnrate : {},minbmu:{},neighbors:{},init_Rad:{}\\n\".format(init_learning_rate,\n",
    "                                                                                                    minPerBmu,no_neighbor,rad))\n",
    "                    name,ext = os.path.splitext(filename)\n",
    "                    if ext != '.csv':continue\n",
    "                    infile = os.path.join(data_dir,filename)\n",
    "                    print(\"\\nDetecting anomalies for {}\\n\".format(filename))\n",
    "\n",
    "\n",
    "                    kwargs1 = csv_reader.get_csv_kwargs(infile=infile,filename=filename,target_dir=target_dir,assetno=assetno[0])\n",
    "\n",
    "                #     print('params: {}\\n'.format(param))\n",
    "                    reader_kwargs1 = kwargs1\n",
    "                    model_input_args1 = model_input_args()\n",
    "                    model_input_args1['diff_order'] = 0\n",
    "                    training_args1 = training_args()\n",
    "                    kwargs1 = {**reader_kwargs1,**model_input_args1,**training_args1}\n",
    "                    res = main(**kwargs1,anom_thres=2.5)\n",
    "                    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
